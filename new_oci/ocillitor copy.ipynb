{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "from IPython.display import clear_output  # 引入 clear_output\n",
    "# torch.manual_seed(seed=42)  \n",
    "\n",
    "os.makedirs(\"fun_images\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num=3\n",
    "hidden_num=64\n",
    "sym='no_symetry'\n",
    "\n",
    "class Mynetwork(nn.Module):\n",
    "    def __init__(self,input_num=1 , out_num=1,hidden_num=64):\n",
    "        super().__init__()\n",
    "        self.MLP=nn.Sequential(\n",
    "            nn.Linear(input_num, hidden_num),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_num,hidden_num),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_num,hidden_num),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_num,out_num),\n",
    "        )\n",
    "        pass\n",
    "    def forward(self,x):\n",
    "        return self.MLP(x)+self.MLP(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential(x,k):\n",
    "    poten=1/2*k*x**2\n",
    "    return poten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_test(x,k):\n",
    "    mask=(x>0).float()\n",
    "    mask2=(x<=0).float()\n",
    "    poten=x**2*mask-30*x*mask2\n",
    "    return poten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(en_level,k,m,h_bar,device,dtype):\n",
    "    omega=np.sqrt(k/m)\n",
    "    en_list=[h_bar*omega*(1/2+i) for i in range(en_level)]\n",
    "    en_list=torch.tensor(en_list,device=device,dtype=dtype)\n",
    "    return en_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial eigenvalues check:\n",
      "tensor([ 1.0003,  2.9988,  5.0004,  6.9991,  9.0011, 10.9993, 13.0005, 14.9992,\n",
      "        17.0003, 18.9990, 20.9998, 22.9993, 25.0000, 26.9966, 28.9989, 30.9973,\n",
      "        32.9980, 34.9966, 36.9968, 38.9953, 40.9961, 42.9949, 44.9947, 46.9930,\n",
      "        48.9938, 50.9925, 52.9926, 54.9914, 56.9916, 58.9902], device='cuda:2',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#需要适当减小,x_M=sqrt(h_bar/(m*w)*(2n+1)),考虑要>10x_M,700sqrt(h_bar/\\omega*m)\n",
    "h_bar=1\n",
    "m=1\n",
    "b_lap:float=-h_bar**2/(2*m)\n",
    "\n",
    "# 同时对于库伦势函数, 取e=1, 4\\pi\\epsilon_0=1, E_n=-1/(2n^2)\n",
    "dtype=torch.float32\n",
    "device=torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(f'./model_para_{sym}_{layer_num}_{hidden_num}_{dtype}', exist_ok=True)\n",
    "La=-10\n",
    "Lb =10\n",
    "L=Lb-La  # domain length\n",
    "N = 3000   # number of interior points # 对时间成本来说几乎是平方量级\n",
    "h :float= L / (N+1)\n",
    "grid=torch.linspace(La,Lb,N+2,dtype=dtype,device=device,requires_grad=True)\n",
    "grid=grid[1:-1].unsqueeze(-1)\n",
    "# 角量子数\n",
    "l=0\n",
    "\n",
    "k=4\n",
    "en_num=30\n",
    "epoch=2500\n",
    "lr=0.01\n",
    "l2_reg=0\n",
    "d2_reg=0\n",
    "\n",
    "real_en=energy(en_num,k,m,h_bar,device,dtype)\n",
    "\n",
    "model=Mynetwork().to(device=device,dtype=dtype)\n",
    "loss_fn=nn.MSELoss()\n",
    "# Construct the tridiagonal matrix A\n",
    "diag = -2.0 / h**2 * torch.ones(N,device=device) * b_lap\n",
    "off_diag = 1.0 / h**2 * torch.ones(N - 1,device=device) * b_lap\n",
    "\n",
    "V_real_poten=potential(grid,k)\n",
    "A = torch.diag(diag) + torch.diag(off_diag,diagonal=1) + torch.diag(off_diag, diagonal=-1)+torch.diag(V_real_poten.flatten())\n",
    "eigenvalues= torch.linalg.eigvalsh(A)\n",
    "print('initial eigenvalues check:')\n",
    "print(eigenvalues[:en_num])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_train epoch 0,loss 8037.1171875,lr 0.001,grad 122.52733612060547\n",
      "pre_train epoch 20,loss 6077.16455078125,lr 0.001,grad 349.02490234375\n",
      "pre_train epoch 40,loss 1944.3099365234375,lr 0.001,grad 711.8723754882812\n",
      "pre_train epoch 60,loss 754.8318481445312,lr 0.001,grad 518.2570190429688\n",
      "pre_train epoch 80,loss 435.9332580566406,lr 0.001,grad 190.7085723876953\n",
      "pre_train epoch 100,loss 257.72882080078125,lr 0.001,grad 53.18207550048828\n",
      "pre_train epoch 120,loss 171.9376983642578,lr 0.001,grad 47.40472412109375\n",
      "pre_train epoch 140,loss 136.35630798339844,lr 0.001,grad 27.808706283569336\n",
      "pre_train epoch 160,loss 117.73270416259766,lr 0.001,grad 21.216461181640625\n",
      "pre_train epoch 180,loss 102.44654083251953,lr 0.001,grad 24.018827438354492\n",
      "pre_train epoch 200,loss 90.2016830444336,lr 0.001,grad 20.679006576538086\n",
      "pre_train epoch 220,loss 80.01850128173828,lr 0.001,grad 19.43442153930664\n",
      "pre_train epoch 240,loss 70.75192260742188,lr 0.001,grad 17.996047973632812\n",
      "pre_train epoch 260,loss 62.0633430480957,lr 0.001,grad 17.1806697845459\n",
      "pre_train epoch 280,loss 53.69796371459961,lr 0.001,grad 16.66602897644043\n",
      "pre_train epoch 300,loss 45.68326950073242,lr 0.001,grad 16.289255142211914\n",
      "pre_train epoch 320,loss 37.943790435791016,lr 0.001,grad 15.702719688415527\n",
      "pre_train epoch 340,loss 30.555620193481445,lr 0.001,grad 14.289556503295898\n",
      "pre_train epoch 360,loss 23.775733947753906,lr 0.001,grad 12.153873443603516\n",
      "pre_train epoch 380,loss 17.95931625366211,lr 0.001,grad 9.779831886291504\n",
      "pre_train epoch 400,loss 13.343825340270996,lr 0.001,grad 7.92050313949585\n",
      "pre_train epoch 420,loss 10.069375991821289,lr 0.001,grad 6.514005184173584\n",
      "pre_train epoch 440,loss 7.844824314117432,lr 0.001,grad 5.621424198150635\n",
      "pre_train epoch 460,loss 6.336616516113281,lr 0.001,grad 4.959306240081787\n",
      "pre_train epoch 480,loss 5.275365352630615,lr 0.001,grad 4.2322797775268555\n",
      "pre_train epoch 500,loss 4.498233795166016,lr 0.001,grad 3.6511833667755127\n",
      "pre_train epoch 520,loss 3.902411460876465,lr 0.001,grad 3.2652995586395264\n",
      "pre_train epoch 540,loss 3.427910327911377,lr 0.001,grad 2.9815776348114014\n",
      "pre_train epoch 560,loss 3.038416624069214,lr 0.001,grad 2.769723892211914\n",
      "pre_train epoch 580,loss 2.711958885192871,lr 0.001,grad 2.4160501956939697\n",
      "pre_train epoch 600,loss 2.4371001720428467,lr 0.001,grad 2.092721939086914\n",
      "pre_train epoch 620,loss 2.200629711151123,lr 0.001,grad 1.8868380784988403\n",
      "pre_train epoch 640,loss 1.9932464361190796,lr 0.001,grad 1.7289690971374512\n",
      "pre_train epoch 660,loss 1.8092049360275269,lr 0.001,grad 1.5943347215652466\n",
      "pre_train epoch 680,loss 1.644504189491272,lr 0.001,grad 1.474980354309082\n",
      "pre_train epoch 700,loss 1.4962457418441772,lr 0.001,grad 1.3673179149627686\n",
      "pre_train epoch 720,loss 1.362385869026184,lr 0.001,grad 1.2717372179031372\n",
      "pre_train epoch 740,loss 1.2410545349121094,lr 0.001,grad 1.182164192199707\n",
      "pre_train epoch 760,loss 1.130746841430664,lr 0.001,grad 1.1000854969024658\n",
      "pre_train epoch 780,loss 1.0302761793136597,lr 0.001,grad 1.0251479148864746\n",
      "pre_train epoch 800,loss 0.938676118850708,lr 0.001,grad 0.9564800262451172\n",
      "pre_train epoch 820,loss 0.8551164269447327,lr 0.001,grad 0.8936101794242859\n",
      "pre_train epoch 840,loss 0.7789230346679688,lr 0.001,grad 0.8361997604370117\n",
      "pre_train epoch 860,loss 0.7095025777816772,lr 0.001,grad 0.7836341857910156\n",
      "pre_train epoch 880,loss 0.6463295817375183,lr 0.001,grad 0.7353941798210144\n",
      "pre_train epoch 900,loss 0.5889260172843933,lr 0.001,grad 0.6911349296569824\n",
      "pre_train epoch 920,loss 0.5368642807006836,lr 0.001,grad 0.6501407623291016\n",
      "pre_train epoch 940,loss 0.48972395062446594,lr 0.001,grad 0.6122434139251709\n",
      "pre_train epoch 960,loss 0.44711872935295105,lr 0.001,grad 0.5770277380943298\n",
      "pre_train epoch 980,loss 0.4086686968803406,lr 0.001,grad 0.5442132353782654\n",
      "pre_train epoch 1000,loss 0.3740087151527405,lr 0.001,grad 0.5136457681655884\n",
      "pre_train epoch 1020,loss 0.3427935242652893,lr 0.001,grad 0.485042929649353\n",
      "pre_train epoch 1040,loss 0.31469306349754333,lr 0.001,grad 0.45810791850090027\n",
      "pre_train epoch 1060,loss 0.28940024971961975,lr 0.001,grad 0.43324220180511475\n",
      "pre_train epoch 1080,loss 0.26662421226501465,lr 0.001,grad 0.4098374545574188\n",
      "pre_train epoch 1100,loss 0.2461039125919342,lr 0.001,grad 0.387919157743454\n",
      "pre_train epoch 1120,loss 0.22759760916233063,lr 0.001,grad 0.3676591217517853\n",
      "pre_train epoch 1140,loss 0.21088199317455292,lr 0.001,grad 0.34849733114242554\n",
      "pre_train epoch 1160,loss 0.19576159119606018,lr 0.001,grad 0.33078789710998535\n",
      "pre_train epoch 1180,loss 0.18206056952476501,lr 0.001,grad 0.31412699818611145\n",
      "pre_train epoch 1200,loss 0.16961811482906342,lr 0.001,grad 0.2987558841705322\n",
      "pre_train epoch 1220,loss 0.15828877687454224,lr 0.001,grad 0.2841683328151703\n",
      "pre_train epoch 1240,loss 0.1479540467262268,lr 0.001,grad 0.27044883370399475\n",
      "pre_train epoch 1260,loss 0.1385004222393036,lr 0.001,grad 0.2577141225337982\n",
      "pre_train epoch 1280,loss 0.12983551621437073,lr 0.001,grad 0.24554891884326935\n",
      "pre_train epoch 1300,loss 0.12187418341636658,lr 0.001,grad 0.23425757884979248\n",
      "pre_train epoch 1320,loss 0.11454252153635025,lr 0.001,grad 0.22375918924808502\n",
      "pre_train epoch 1340,loss 0.10777447372674942,lr 0.001,grad 0.2139820009469986\n",
      "pre_train epoch 1360,loss 0.1015145555138588,lr 0.001,grad 0.20471778512001038\n",
      "pre_train epoch 1380,loss 0.09571270644664764,lr 0.001,grad 0.19589807093143463\n",
      "pre_train epoch 1400,loss 0.09032560884952545,lr 0.001,grad 0.18768469989299774\n",
      "pre_train epoch 1420,loss 0.08531469106674194,lr 0.001,grad 0.17987576127052307\n",
      "pre_train epoch 1440,loss 0.08064685016870499,lr 0.001,grad 0.17260129749774933\n",
      "pre_train epoch 1460,loss 0.0762934759259224,lr 0.001,grad 0.16554586589336395\n",
      "pre_train epoch 1480,loss 0.07222697883844376,lr 0.001,grad 0.158982515335083\n",
      "pre_train epoch 1500,loss 0.06842448562383652,lr 0.001,grad 0.15289832651615143\n",
      "pre_train epoch 1520,loss 0.06486643850803375,lr 0.001,grad 0.14704470336437225\n",
      "pre_train epoch 1540,loss 0.06153278425335884,lr 0.001,grad 0.14117486774921417\n",
      "pre_train epoch 1560,loss 0.05840785801410675,lr 0.001,grad 0.13576647639274597\n",
      "pre_train epoch 1580,loss 0.05547533184289932,lr 0.001,grad 0.13063429296016693\n",
      "pre_train epoch 1600,loss 0.0527227409183979,lr 0.001,grad 0.12593844532966614\n",
      "pre_train epoch 1620,loss 0.05013655498623848,lr 0.001,grad 0.12120375037193298\n",
      "pre_train epoch 1640,loss 0.04770582169294357,lr 0.001,grad 0.11679323017597198\n",
      "pre_train epoch 1660,loss 0.04542015492916107,lr 0.001,grad 0.11243795603513718\n",
      "pre_train epoch 1680,loss 0.043269142508506775,lr 0.001,grad 0.10850092768669128\n",
      "pre_train epoch 1700,loss 0.04124438390135765,lr 0.001,grad 0.10459543019533157\n",
      "pre_train epoch 1720,loss 0.0393369123339653,lr 0.001,grad 0.10117305815219879\n",
      "pre_train epoch 1740,loss 0.037540048360824585,lr 0.001,grad 0.09750115126371384\n",
      "pre_train epoch 1760,loss 0.03584607318043709,lr 0.001,grad 0.09421930462121964\n",
      "pre_train epoch 1780,loss 0.034248147159814835,lr 0.001,grad 0.09109453111886978\n",
      "pre_train epoch 1800,loss 0.032740313559770584,lr 0.001,grad 0.08798980712890625\n",
      "pre_train epoch 1820,loss 0.031316936016082764,lr 0.001,grad 0.08524885773658752\n",
      "pre_train epoch 1840,loss 0.029972761869430542,lr 0.001,grad 0.08251550048589706\n",
      "pre_train epoch 1860,loss 0.028702493757009506,lr 0.001,grad 0.07969002425670624\n",
      "pre_train epoch 1880,loss 0.027501635253429413,lr 0.001,grad 0.07698749750852585\n",
      "pre_train epoch 1900,loss 0.02636615000665188,lr 0.001,grad 0.07463356852531433\n",
      "pre_train epoch 1920,loss 0.02529178000986576,lr 0.001,grad 0.07221528887748718\n",
      "pre_train epoch 1940,loss 0.024274827912449837,lr 0.001,grad 0.06979590654373169\n",
      "pre_train epoch 1960,loss 0.023311840370297432,lr 0.001,grad 0.06774846464395523\n",
      "pre_train epoch 1980,loss 0.0223992969840765,lr 0.001,grad 0.06550607085227966\n",
      "pre_train epoch 2000,loss 0.021534066647291183,lr 0.001,grad 0.06366708129644394\n",
      "pre_train epoch 2020,loss 0.02071334607899189,lr 0.001,grad 0.06153695657849312\n",
      "pre_train epoch 2040,loss 0.019934266805648804,lr 0.001,grad 0.05942436680197716\n",
      "pre_train epoch 2060,loss 0.019194072112441063,lr 0.001,grad 0.05773914232850075\n",
      "pre_train epoch 2080,loss 0.018490582704544067,lr 0.001,grad 0.055938635021448135\n",
      "pre_train epoch 2100,loss 0.01782122813165188,lr 0.001,grad 0.054389096796512604\n",
      "pre_train epoch 2120,loss 0.017184317111968994,lr 0.001,grad 0.05278627946972847\n",
      "pre_train epoch 2140,loss 0.016577420756220818,lr 0.001,grad 0.05130453407764435\n",
      "pre_train epoch 2160,loss 0.015998970717191696,lr 0.001,grad 0.049668893218040466\n",
      "pre_train epoch 2180,loss 0.0154473427683115,lr 0.001,grad 0.048289697617292404\n",
      "pre_train epoch 2200,loss 0.014920832589268684,lr 0.001,grad 0.046957921236753464\n",
      "pre_train epoch 2220,loss 0.014417804777622223,lr 0.001,grad 0.045492395758628845\n",
      "pre_train epoch 2240,loss 0.013937147334218025,lr 0.001,grad 0.044410862028598785\n",
      "pre_train epoch 2260,loss 0.013477624393999577,lr 0.001,grad 0.04332010820508003\n",
      "pre_train epoch 2280,loss 0.013037622906267643,lr 0.001,grad 0.0416865348815918\n",
      "pre_train epoch 2300,loss 0.012616482563316822,lr 0.001,grad 0.04093238338828087\n",
      "pre_train epoch 2320,loss 0.012212997302412987,lr 0.001,grad 0.03966936469078064\n",
      "pre_train epoch 2340,loss 0.01182607002556324,lr 0.001,grad 0.03847397491335869\n",
      "pre_train epoch 2360,loss 0.011454915627837181,lr 0.001,grad 0.03805256262421608\n",
      "pre_train epoch 2380,loss 0.011098823510110378,lr 0.001,grad 0.03644150123000145\n",
      "pre_train epoch 2400,loss 0.010756826028227806,lr 0.001,grad 0.036010418087244034\n",
      "pre_train epoch 2420,loss 0.010428124107420444,lr 0.001,grad 0.03511149808764458\n",
      "pre_train epoch 2440,loss 0.010111951269209385,lr 0.001,grad 0.033753350377082825\n",
      "pre_train epoch 2460,loss 0.009808017872273922,lr 0.001,grad 0.03288045525550842\n",
      "pre_train epoch 2480,loss 0.009515545330941677,lr 0.001,grad 0.03239227831363678\n",
      "pre_train epoch 2500,loss 0.00923385564237833,lr 0.001,grad 0.031301360577344894\n",
      "pre_train epoch 2520,loss 0.00896258745342493,lr 0.001,grad 0.029956383630633354\n",
      "pre_train epoch 2540,loss 0.008700830861926079,lr 0.001,grad 0.029865851625800133\n",
      "pre_train epoch 2560,loss 0.008448651991784573,lr 0.001,grad 0.0284766536206007\n",
      "pre_train epoch 2580,loss 0.008205333724617958,lr 0.001,grad 0.028153549879789352\n",
      "pre_train epoch 2600,loss 0.007970466278493404,lr 0.001,grad 0.027331337332725525\n",
      "pre_train epoch 2620,loss 0.007743785623461008,lr 0.001,grad 0.026872046291828156\n",
      "pre_train epoch 2640,loss 0.007524742744863033,lr 0.001,grad 0.02649994008243084\n",
      "pre_train epoch 2660,loss 0.007313005160540342,lr 0.001,grad 0.02595258504152298\n",
      "pre_train epoch 2680,loss 0.007108152844011784,lr 0.001,grad 0.024881770834326744\n",
      "pre_train epoch 2700,loss 0.0069102635607123375,lr 0.001,grad 0.024413131177425385\n",
      "pre_train epoch 2720,loss 0.006718724500387907,lr 0.001,grad 0.024171581491827965\n",
      "pre_train epoch 2740,loss 0.00653335265815258,lr 0.001,grad 0.023252977058291435\n",
      "pre_train epoch 2760,loss 0.006353956647217274,lr 0.001,grad 0.022750133648514748\n",
      "pre_train epoch 2780,loss 0.006180271040648222,lr 0.001,grad 0.02220182679593563\n",
      "pre_train epoch 2800,loss 0.006011794321238995,lr 0.001,grad 0.02152533084154129\n",
      "pre_train epoch 2820,loss 0.005848656874150038,lr 0.001,grad 0.02131047286093235\n",
      "pre_train epoch 2840,loss 0.005690538790076971,lr 0.001,grad 0.020295796915888786\n",
      "pre_train epoch 2860,loss 0.005537305027246475,lr 0.001,grad 0.020463809370994568\n",
      "pre_train epoch 2880,loss 0.005388622172176838,lr 0.001,grad 0.020247256383299828\n",
      "pre_train epoch 2900,loss 0.005244477652013302,lr 0.001,grad 0.019197961315512657\n",
      "pre_train epoch 2920,loss 0.005104522220790386,lr 0.001,grad 0.018554309383034706\n",
      "pre_train epoch 2940,loss 0.004968937952071428,lr 0.001,grad 0.01802176609635353\n",
      "pre_train epoch 2960,loss 0.004837215878069401,lr 0.001,grad 0.017463169991970062\n",
      "pre_train epoch 2980,loss 0.004709357395768166,lr 0.001,grad 0.01757187582552433\n",
      "pre_train epoch 3000,loss 0.004585355520248413,lr 0.001,grad 0.01769447885453701\n",
      "pre_train epoch 3020,loss 0.00446490989997983,lr 0.001,grad 0.017628774046897888\n",
      "pre_train epoch 3040,loss 0.004347959998995066,lr 0.001,grad 0.016866689547896385\n",
      "pre_train epoch 3060,loss 0.004234433639794588,lr 0.001,grad 0.016874011605978012\n",
      "pre_train epoch 3080,loss 0.004124206956475973,lr 0.001,grad 0.017025364562869072\n",
      "pre_train epoch 3100,loss 0.0040170298889279366,lr 0.001,grad 0.015465785749256611\n",
      "pre_train epoch 3120,loss 0.003913068678230047,lr 0.001,grad 0.015411189757287502\n",
      "pre_train epoch 3140,loss 0.0038119659293442965,lr 0.001,grad 0.015250474214553833\n",
      "pre_train epoch 3160,loss 0.0037137616891413927,lr 0.001,grad 0.013765311799943447\n",
      "pre_train epoch 3180,loss 0.003618698799982667,lr 0.001,grad 0.08450277149677277\n",
      "pre_train epoch 3200,loss 0.006568477489054203,lr 0.001,grad 8.078595161437988\n",
      "pre_train epoch 3220,loss 0.04994725435972214,lr 0.0001,grad 31.58390235900879\n",
      "pre_train epoch 3240,loss 0.00837700255215168,lr 0.0001,grad 10.268170356750488\n",
      "pre_train epoch 3260,loss 0.003930933773517609,lr 1e-05,grad 3.1720361709594727\n",
      "pre_train epoch 3280,loss 0.003479652339592576,lr 1e-05,grad 0.5698415040969849\n",
      "pre_train epoch 3300,loss 0.0034635288175195456,lr 1e-05,grad 0.05995267257094383\n",
      "pre_train epoch 3320,loss 0.003462962107732892,lr 1e-05,grad 0.11439571529626846\n",
      "pre_train epoch 3340,loss 0.0034616696648299694,lr 1e-05,grad 0.04969482868909836\n",
      "pre_train epoch 3360,loss 0.0034606189001351595,lr 1e-05,grad 0.025186123326420784\n",
      "pre_train epoch 3380,loss 0.0034597727935761213,lr 1e-05,grad 0.013610556721687317\n",
      "pre_train epoch 3400,loss 0.003458788152784109,lr 1e-05,grad 0.014109047129750252\n",
      "pre_train epoch 3420,loss 0.003457822138443589,lr 1e-05,grad 0.014312977902591228\n",
      "pre_train epoch 3440,loss 0.003456894541159272,lr 1e-05,grad 0.014037172310054302\n",
      "pre_train epoch 3460,loss 0.0034559560008347034,lr 1e-05,grad 0.014180373400449753\n",
      "pre_train epoch 3480,loss 0.0034549927804619074,lr 1e-05,grad 0.014171411283314228\n",
      "pre_train epoch 3500,loss 0.003454007441177964,lr 1e-05,grad 0.014192174188792706\n",
      "pre_train epoch 3520,loss 0.0034530130214989185,lr 1e-05,grad 0.014234060421586037\n",
      "pre_train epoch 3540,loss 0.0034520297776907682,lr 1e-05,grad 0.014234044589102268\n",
      "pre_train epoch 3560,loss 0.0034510211553424597,lr 1e-05,grad 0.014256918802857399\n",
      "pre_train epoch 3580,loss 0.0034499934408813715,lr 1e-05,grad 0.014226356521248817\n",
      "pre_train epoch 3600,loss 0.0034489487297832966,lr 1e-05,grad 0.014276547357439995\n",
      "pre_train epoch 3620,loss 0.0034479384776204824,lr 1e-05,grad 0.014246789738535881\n",
      "pre_train epoch 3640,loss 0.0034468665253371,lr 1e-05,grad 0.01422469038516283\n",
      "pre_train epoch 3660,loss 0.0034458001609891653,lr 1e-05,grad 0.014322427101433277\n",
      "pre_train epoch 3680,loss 0.003444778500124812,lr 1e-05,grad 0.014294219203293324\n",
      "pre_train epoch 3700,loss 0.0034436092246323824,lr 1e-05,grad 0.014346100389957428\n",
      "pre_train epoch 3720,loss 0.003442595014348626,lr 1e-05,grad 0.014341073110699654\n",
      "pre_train epoch 3740,loss 0.0034415351692587137,lr 1e-05,grad 0.014357655309140682\n",
      "pre_train epoch 3760,loss 0.0034403873141855,lr 1e-05,grad 0.014298428781330585\n",
      "pre_train epoch 3780,loss 0.0034392387606203556,lr 1e-05,grad 0.014195487834513187\n",
      "pre_train epoch 3800,loss 0.0034381337463855743,lr 1e-05,grad 0.01418245304375887\n",
      "pre_train epoch 3820,loss 0.0034370047505944967,lr 1e-05,grad 0.0141449561342597\n",
      "pre_train epoch 3840,loss 0.003435817314311862,lr 1e-05,grad 0.014117490500211716\n",
      "pre_train epoch 3860,loss 0.0034346147440373898,lr 1e-05,grad 0.014116047881543636\n",
      "pre_train epoch 3880,loss 0.003433436620980501,lr 1e-05,grad 0.01408425997942686\n",
      "pre_train epoch 3900,loss 0.003432318102568388,lr 1e-05,grad 0.014111827127635479\n",
      "pre_train epoch 3920,loss 0.003431059420108795,lr 1e-05,grad 0.01409193780273199\n",
      "pre_train epoch 3940,loss 0.0034298140089958906,lr 1e-05,grad 0.014092575758695602\n",
      "pre_train epoch 3960,loss 0.0034285748843103647,lr 1e-05,grad 0.014076384715735912\n",
      "pre_train epoch 3980,loss 0.003427328774705529,lr 1e-05,grad 0.014082854613661766\n",
      "pre_train epoch 4000,loss 0.0034260812681168318,lr 1e-05,grad 0.014107844792306423\n",
      "pre_train epoch 4020,loss 0.0034249145537614822,lr 1e-05,grad 0.014093339443206787\n",
      "pre_train epoch 4040,loss 0.003423565300181508,lr 1e-05,grad 0.014110877178609371\n",
      "pre_train epoch 4060,loss 0.00342232221737504,lr 1e-05,grad 0.01417397242039442\n",
      "pre_train epoch 4080,loss 0.003421025350689888,lr 1e-05,grad 0.014089645817875862\n",
      "pre_train epoch 4100,loss 0.0034197450149804354,lr 1e-05,grad 0.014077768661081791\n",
      "pre_train epoch 4120,loss 0.003418385749682784,lr 1e-05,grad 0.014056169427931309\n",
      "pre_train epoch 4140,loss 0.0034171028528362513,lr 1e-05,grad 0.01400288287550211\n",
      "pre_train epoch 4160,loss 0.0034157291520386934,lr 1e-05,grad 0.014009987935423851\n",
      "pre_train epoch 4180,loss 0.0034143731463700533,lr 1e-05,grad 0.013986687175929546\n",
      "pre_train epoch 4200,loss 0.0034129698760807514,lr 1e-05,grad 0.014026971533894539\n",
      "pre_train epoch 4220,loss 0.0034115733578801155,lr 1e-05,grad 0.01398938987404108\n",
      "pre_train epoch 4240,loss 0.0034101782366633415,lr 1e-05,grad 0.013988780789077282\n",
      "pre_train epoch 4260,loss 0.0034087111707776785,lr 1e-05,grad 0.01397277694195509\n",
      "pre_train epoch 4280,loss 0.003407310228794813,lr 1e-05,grad 0.013985193334519863\n",
      "pre_train epoch 4300,loss 0.003405895084142685,lr 1e-05,grad 0.013956798240542412\n",
      "pre_train epoch 4320,loss 0.003404376097023487,lr 1e-05,grad 0.013856034725904465\n",
      "pre_train epoch 4340,loss 0.0034029120579361916,lr 1e-05,grad 0.01395412813872099\n",
      "pre_train epoch 4360,loss 0.0034014112316071987,lr 1e-05,grad 0.013976505026221275\n",
      "pre_train epoch 4380,loss 0.003399862674996257,lr 1e-05,grad 0.014018969610333443\n",
      "pre_train epoch 4400,loss 0.003398397471755743,lr 1e-05,grad 0.013978670351207256\n",
      "pre_train epoch 4420,loss 0.003396796528249979,lr 1e-05,grad 0.014006317593157291\n",
      "pre_train epoch 4440,loss 0.0033951851073652506,lr 1e-05,grad 0.014018178917467594\n",
      "pre_train epoch 4460,loss 0.003393585095182061,lr 1e-05,grad 0.013974063098430634\n",
      "pre_train epoch 4480,loss 0.0033920102287083864,lr 1e-05,grad 0.014041708782315254\n",
      "pre_train epoch 4500,loss 0.003390402067452669,lr 1e-05,grad 0.014020487666130066\n",
      "pre_train epoch 4520,loss 0.0033887792378664017,lr 1e-05,grad 0.014004756696522236\n",
      "pre_train epoch 4540,loss 0.0033870746847242117,lr 1e-05,grad 0.013967716135084629\n",
      "pre_train epoch 4560,loss 0.0033854031935334206,lr 1e-05,grad 0.014045167714357376\n",
      "pre_train epoch 4580,loss 0.0033837412483990192,lr 1e-05,grad 0.01407292764633894\n",
      "pre_train epoch 4600,loss 0.0033819747623056173,lr 1e-05,grad 0.01405890379101038\n",
      "pre_train epoch 4620,loss 0.0033802709076553583,lr 1e-05,grad 0.014070659875869751\n",
      "pre_train epoch 4640,loss 0.003378537716343999,lr 1e-05,grad 0.014076187275350094\n",
      "pre_train epoch 4660,loss 0.0033767849672585726,lr 1e-05,grad 0.014038651250302792\n",
      "pre_train epoch 4680,loss 0.003375069238245487,lr 1e-05,grad 0.014026165939867496\n",
      "pre_train epoch 4700,loss 0.003373224288225174,lr 1e-05,grad 0.014053188264369965\n",
      "pre_train epoch 4720,loss 0.003371459199115634,lr 1e-05,grad 0.01398886926472187\n",
      "pre_train epoch 4740,loss 0.0033697369508445263,lr 1e-05,grad 0.014056452549993992\n",
      "pre_train epoch 4760,loss 0.0033678384497761726,lr 1e-05,grad 0.014036758802831173\n",
      "pre_train epoch 4780,loss 0.0033659934997558594,lr 1e-05,grad 0.013953662477433681\n",
      "pre_train epoch 4800,loss 0.0033641865011304617,lr 1e-05,grad 0.013996059074997902\n",
      "pre_train epoch 4820,loss 0.003362274030223489,lr 1e-05,grad 0.013946839608252048\n",
      "pre_train epoch 4840,loss 0.0033604518976062536,lr 1e-05,grad 0.013903278857469559\n",
      "pre_train epoch 4860,loss 0.003358545247465372,lr 1e-05,grad 0.013952460139989853\n",
      "pre_train epoch 4880,loss 0.0033565761987119913,lr 1e-05,grad 0.013955164700746536\n",
      "pre_train epoch 4900,loss 0.003354649292305112,lr 1e-05,grad 0.014012004248797894\n",
      "pre_train epoch 4920,loss 0.0033527023624628782,lr 1e-05,grad 0.014016888104379177\n",
      "pre_train epoch 4940,loss 0.003350750310346484,lr 1e-05,grad 0.013986720703542233\n",
      "pre_train epoch 4960,loss 0.0033486911561340094,lr 1e-05,grad 0.01396996807307005\n",
      "pre_train epoch 4980,loss 0.0033467398025095463,lr 1e-05,grad 0.0140282167121768\n",
      "pre_train epoch 5000,loss 0.003344691824167967,lr 1e-05,grad 0.01394596230238676\n",
      "pre_train epoch 5020,loss 0.0033426417503505945,lr 1e-05,grad 0.013993554748594761\n",
      "pre_train epoch 5040,loss 0.0033405525609850883,lr 1e-05,grad 0.013971084728837013\n",
      "pre_train epoch 5060,loss 0.003338478971272707,lr 1e-05,grad 0.013920988887548447\n",
      "pre_train epoch 5080,loss 0.003336388850584626,lr 1e-05,grad 0.013905554078519344\n",
      "pre_train epoch 5100,loss 0.0033343080431222916,lr 1e-05,grad 0.013898561708629131\n",
      "pre_train epoch 5120,loss 0.003332118969410658,lr 1e-05,grad 0.013935335911810398\n",
      "pre_train epoch 5140,loss 0.0033299352508038282,lr 1e-05,grad 0.013854900375008583\n",
      "pre_train epoch 5160,loss 0.0033277219627052546,lr 1e-05,grad 0.013796140439808369\n",
      "pre_train epoch 5180,loss 0.0033255484886467457,lr 1e-05,grad 0.013882036320865154\n",
      "pre_train epoch 5200,loss 0.0033233845606446266,lr 1e-05,grad 0.013832228258252144\n",
      "pre_train epoch 5220,loss 0.0033211102709174156,lr 1e-05,grad 0.013794711790978909\n",
      "pre_train epoch 5240,loss 0.0033188513480126858,lr 1e-05,grad 0.013807002454996109\n",
      "pre_train epoch 5260,loss 0.0033165067434310913,lr 1e-05,grad 0.013706165365874767\n",
      "pre_train epoch 5280,loss 0.0033142324537038803,lr 1e-05,grad 0.013749620877206326\n",
      "pre_train epoch 5300,loss 0.0033118685241788626,lr 1e-05,grad 0.013759296387434006\n",
      "pre_train epoch 5320,loss 0.003309508552774787,lr 1e-05,grad 0.013707050122320652\n",
      "pre_train epoch 5340,loss 0.003307119943201542,lr 1e-05,grad 0.013681354001164436\n",
      "pre_train epoch 5360,loss 0.003304694779217243,lr 1e-05,grad 0.013644937425851822\n",
      "pre_train epoch 5380,loss 0.003302290802821517,lr 1e-05,grad 0.013622441329061985\n",
      "pre_train epoch 5400,loss 0.0032998009119182825,lr 1e-05,grad 0.013590460643172264\n",
      "pre_train epoch 5420,loss 0.003297335933893919,lr 1e-05,grad 0.01360026653856039\n",
      "pre_train epoch 5440,loss 0.0032948977313935757,lr 1e-05,grad 0.013604514300823212\n",
      "pre_train epoch 5460,loss 0.0032923889812082052,lr 1e-05,grad 0.013527746312320232\n",
      "pre_train epoch 5480,loss 0.003289761720225215,lr 1e-05,grad 0.013445050455629826\n",
      "pre_train epoch 5500,loss 0.003287261351943016,lr 1e-05,grad 0.013511821627616882\n",
      "pre_train epoch 5520,loss 0.0032846599351614714,lr 1e-05,grad 0.013524302281439304\n",
      "pre_train epoch 5540,loss 0.0032820883207023144,lr 1e-05,grad 0.013496740721166134\n",
      "pre_train epoch 5560,loss 0.0032794137950986624,lr 1e-05,grad 0.013465477153658867\n",
      "pre_train epoch 5580,loss 0.0032767746597528458,lr 1e-05,grad 0.013427281752228737\n",
      "pre_train epoch 5600,loss 0.0032740484457463026,lr 1e-05,grad 0.01349734142422676\n",
      "pre_train epoch 5620,loss 0.0032713795080780983,lr 1e-05,grad 0.013399177230894566\n",
      "pre_train epoch 5640,loss 0.0032686893828213215,lr 1e-05,grad 0.013393166474997997\n",
      "pre_train epoch 5660,loss 0.0032658881973475218,lr 1e-05,grad 0.013456445187330246\n",
      "pre_train epoch 5680,loss 0.003263125428929925,lr 1e-05,grad 0.013388259336352348\n",
      "pre_train epoch 5700,loss 0.003260331228375435,lr 1e-05,grad 0.013438358902931213\n",
      "pre_train epoch 5720,loss 0.0032574795186519623,lr 1e-05,grad 0.013479206711053848\n",
      "pre_train epoch 5740,loss 0.0032547172158956528,lr 1e-05,grad 0.01347271166741848\n",
      "pre_train epoch 5760,loss 0.00325181195512414,lr 1e-05,grad 0.013460642658174038\n",
      "pre_train epoch 5780,loss 0.003248876892030239,lr 1e-05,grad 0.01346684992313385\n",
      "pre_train epoch 5800,loss 0.0032460098154842854,lr 1e-05,grad 0.013292929157614708\n",
      "pre_train epoch 5820,loss 0.0032430458813905716,lr 1e-05,grad 0.01331329345703125\n",
      "pre_train epoch 5840,loss 0.003240092657506466,lr 1e-05,grad 0.013386380858719349\n",
      "pre_train epoch 5860,loss 0.0032371007837355137,lr 1e-05,grad 0.013376607559621334\n",
      "pre_train epoch 5880,loss 0.0032340430188924074,lr 1e-05,grad 0.013391483575105667\n",
      "pre_train epoch 5900,loss 0.003231023671105504,lr 1e-05,grad 0.01339133270084858\n",
      "pre_train epoch 5920,loss 0.0032279351726174355,lr 1e-05,grad 0.013395611196756363\n",
      "pre_train epoch 5940,loss 0.003224817803129554,lr 1e-05,grad 0.013377866707742214\n",
      "pre_train epoch 5960,loss 0.0032217393163591623,lr 1e-05,grad 0.013356275856494904\n",
      "pre_train epoch 5980,loss 0.003218453610315919,lr 1e-05,grad 0.013310312293469906\n",
      "pre_train epoch 6000,loss 0.0032153327483683825,lr 1e-05,grad 0.01333970669656992\n",
      "pre_train epoch 6020,loss 0.0032121771946549416,lr 1e-05,grad 0.013289354741573334\n",
      "pre_train epoch 6040,loss 0.0032088293228298426,lr 1e-05,grad 0.013297262601554394\n",
      "pre_train epoch 6060,loss 0.0032056309282779694,lr 1e-05,grad 0.013306557200849056\n",
      "pre_train epoch 6080,loss 0.0032023272942751646,lr 1e-05,grad 0.013313094154000282\n",
      "pre_train epoch 6100,loss 0.003198964288458228,lr 1e-05,grad 0.013325492851436138\n",
      "pre_train epoch 6120,loss 0.003195681842043996,lr 1e-05,grad 0.01324207428842783\n",
      "pre_train epoch 6140,loss 0.0031922461930662394,lr 1e-05,grad 0.01322344969958067\n",
      "pre_train epoch 6160,loss 0.0031888445373624563,lr 1e-05,grad 0.013249126262962818\n",
      "pre_train epoch 6180,loss 0.003185408189892769,lr 1e-05,grad 0.013235725462436676\n",
      "pre_train epoch 6200,loss 0.003181896870955825,lr 1e-05,grad 0.013216840103268623\n",
      "pre_train epoch 6220,loss 0.0031783771701157093,lr 1e-05,grad 0.013264290057122707\n",
      "pre_train epoch 6240,loss 0.003174785291776061,lr 1e-05,grad 0.013223456218838692\n",
      "pre_train epoch 6260,loss 0.003171282820403576,lr 1e-05,grad 0.01318287756294012\n",
      "pre_train epoch 6280,loss 0.003167745191603899,lr 1e-05,grad 0.01318806316703558\n",
      "pre_train epoch 6300,loss 0.0031641556415706873,lr 1e-05,grad 0.013098053634166718\n",
      "pre_train epoch 6320,loss 0.0031604869291186333,lr 1e-05,grad 0.013128937222063541\n",
      "pre_train epoch 6340,loss 0.0031567902769893408,lr 1e-05,grad 0.013203997164964676\n",
      "pre_train epoch 6360,loss 0.003153064753860235,lr 1e-05,grad 0.01310896035283804\n",
      "pre_train epoch 6380,loss 0.0031493650749325752,lr 1e-05,grad 0.013104919344186783\n",
      "pre_train epoch 6400,loss 0.003145655384287238,lr 1e-05,grad 0.013065080158412457\n",
      "pre_train epoch 6420,loss 0.003141883760690689,lr 1e-05,grad 0.013157350942492485\n",
      "pre_train epoch 6440,loss 0.0031380271539092064,lr 1e-05,grad 0.01307692937552929\n",
      "pre_train epoch 6460,loss 0.0031341472640633583,lr 1e-05,grad 0.013029566034674644\n",
      "pre_train epoch 6480,loss 0.00313028902746737,lr 1e-05,grad 0.013112896122038364\n",
      "pre_train epoch 6500,loss 0.0031263523269444704,lr 1e-05,grad 0.013055157847702503\n",
      "pre_train epoch 6520,loss 0.0031223774421960115,lr 1e-05,grad 0.013003148138523102\n",
      "pre_train epoch 6540,loss 0.0031184344552457333,lr 1e-05,grad 0.01299359556287527\n",
      "pre_train epoch 6560,loss 0.0031144109088927507,lr 1e-05,grad 0.012954809702932835\n",
      "pre_train epoch 6580,loss 0.003110402962192893,lr 1e-05,grad 0.01295202225446701\n",
      "pre_train epoch 6600,loss 0.003106310497969389,lr 1e-05,grad 0.01288136001676321\n",
      "pre_train epoch 6620,loss 0.003102173563092947,lr 1e-05,grad 0.012856061570346355\n",
      "pre_train epoch 6640,loss 0.0030980422161519527,lr 1e-05,grad 0.012922843918204308\n",
      "pre_train epoch 6660,loss 0.003093911102041602,lr 1e-05,grad 0.012857764028012753\n",
      "pre_train epoch 6680,loss 0.00308970152400434,lr 1e-05,grad 0.012871740385890007\n",
      "pre_train epoch 6700,loss 0.0030854332726448774,lr 1e-05,grad 0.01290108636021614\n",
      "pre_train epoch 6720,loss 0.003081107512116432,lr 1e-05,grad 0.012702232226729393\n",
      "pre_train epoch 6740,loss 0.0030768588185310364,lr 1e-05,grad 0.012799193151295185\n",
      "pre_train epoch 6760,loss 0.0030724662356078625,lr 1e-05,grad 0.012758533470332623\n",
      "pre_train epoch 6780,loss 0.003068051068112254,lr 1e-05,grad 0.012790527194738388\n",
      "pre_train epoch 6800,loss 0.0030636980663985014,lr 1e-05,grad 0.012711497023701668\n",
      "pre_train epoch 6820,loss 0.003059270093217492,lr 1e-05,grad 0.012857596389949322\n",
      "pre_train epoch 6840,loss 0.003054778091609478,lr 1e-05,grad 0.012701216153800488\n",
      "pre_train epoch 6860,loss 0.0030502029694616795,lr 1e-05,grad 0.012719751335680485\n",
      "pre_train epoch 6880,loss 0.0030456706881523132,lr 1e-05,grad 0.012763087637722492\n",
      "pre_train epoch 6900,loss 0.0030410359613597393,lr 1e-05,grad 0.012718799524009228\n",
      "pre_train epoch 6920,loss 0.003036415670067072,lr 1e-05,grad 0.0126756951212883\n",
      "pre_train epoch 6940,loss 0.0030317131895571947,lr 1e-05,grad 0.012741903774440289\n",
      "pre_train epoch 6960,loss 0.0030270388815551996,lr 1e-05,grad 0.012668125331401825\n",
      "pre_train epoch 6980,loss 0.003022274700924754,lr 1e-05,grad 0.012756784446537495\n",
      "pre_train epoch 7000,loss 0.0030174546409398317,lr 1e-05,grad 0.012732758186757565\n",
      "pre_train epoch 7020,loss 0.003012601751834154,lr 1e-05,grad 0.012649551033973694\n",
      "pre_train epoch 7040,loss 0.0030076932162046432,lr 1e-05,grad 0.01265134010463953\n",
      "pre_train epoch 7060,loss 0.003002883167937398,lr 1e-05,grad 0.01262635923922062\n",
      "pre_train epoch 7080,loss 0.002997978823259473,lr 1e-05,grad 0.012565478682518005\n",
      "pre_train epoch 7100,loss 0.002992983441799879,lr 1e-05,grad 0.012629184871912003\n",
      "pre_train epoch 7120,loss 0.002987944521009922,lr 1e-05,grad 0.012589978985488415\n",
      "pre_train epoch 7140,loss 0.00298297218978405,lr 1e-05,grad 0.012586572207510471\n",
      "pre_train epoch 7160,loss 0.002977851778268814,lr 1e-05,grad 0.012549913488328457\n",
      "pre_train epoch 7180,loss 0.0029727360233664513,lr 1e-05,grad 0.012558646500110626\n",
      "pre_train epoch 7200,loss 0.002967581618577242,lr 1e-05,grad 0.012448960915207863\n",
      "pre_train epoch 7220,loss 0.0029624144081026316,lr 1e-05,grad 0.012520033866167068\n",
      "pre_train epoch 7240,loss 0.0029571736231446266,lr 1e-05,grad 0.012496680952608585\n",
      "pre_train epoch 7260,loss 0.0029518925584852695,lr 1e-05,grad 0.01241188682615757\n",
      "pre_train epoch 7280,loss 0.002946544671431184,lr 1e-05,grad 0.012469529174268246\n",
      "pre_train epoch 7300,loss 0.0029411818832159042,lr 1e-05,grad 0.012472362257540226\n",
      "pre_train epoch 7320,loss 0.0029358984902501106,lr 1e-05,grad 0.012378765270113945\n",
      "pre_train epoch 7340,loss 0.0029303913470357656,lr 1e-05,grad 0.012273735366761684\n",
      "pre_train epoch 7360,loss 0.0029249503277242184,lr 1e-05,grad 0.012319379486143589\n",
      "pre_train epoch 7380,loss 0.0029194511007517576,lr 1e-05,grad 0.012303794734179974\n",
      "pre_train epoch 7400,loss 0.002913936274126172,lr 1e-05,grad 0.012227854691445827\n",
      "pre_train epoch 7420,loss 0.002908355323597789,lr 1e-05,grad 0.012257235124707222\n",
      "pre_train epoch 7440,loss 0.0029026654083281755,lr 1e-05,grad 0.012329473160207272\n",
      "pre_train epoch 7460,loss 0.002897087950259447,lr 1e-05,grad 0.012300265952944756\n",
      "pre_train epoch 7480,loss 0.002891398500651121,lr 1e-05,grad 0.01224576961249113\n",
      "pre_train epoch 7500,loss 0.002885605441406369,lr 1e-05,grad 0.012211170978844166\n",
      "pre_train epoch 7520,loss 0.0028798694256693125,lr 1e-05,grad 0.012248312123119831\n",
      "pre_train epoch 7540,loss 0.0028739944100379944,lr 1e-05,grad 0.012132448144257069\n",
      "pre_train epoch 7560,loss 0.0028681294061243534,lr 1e-05,grad 0.012171117588877678\n",
      "pre_train epoch 7580,loss 0.0028622932732105255,lr 1e-05,grad 0.012251139618456364\n",
      "pre_train epoch 7600,loss 0.0028563416562974453,lr 1e-05,grad 0.012209353037178516\n",
      "pre_train epoch 7620,loss 0.0028503257781267166,lr 1e-05,grad 0.012180650606751442\n",
      "pre_train epoch 7640,loss 0.002844305941835046,lr 1e-05,grad 0.012008622288703918\n",
      "pre_train epoch 7660,loss 0.002838226966559887,lr 1e-05,grad 0.012051519006490707\n",
      "pre_train epoch 7680,loss 0.002832116326317191,lr 1e-05,grad 0.012085827998816967\n",
      "pre_train epoch 7700,loss 0.002825934672728181,lr 1e-05,grad 0.01209266297519207\n",
      "pre_train epoch 7720,loss 0.0028196920175105333,lr 1e-05,grad 0.011884336359798908\n",
      "pre_train epoch 7740,loss 0.00281354202888906,lr 1e-05,grad 0.012061705812811852\n",
      "pre_train epoch 7760,loss 0.002807167125865817,lr 1e-05,grad 0.011928824707865715\n",
      "pre_train epoch 7780,loss 0.0028009077068418264,lr 1e-05,grad 0.012036923319101334\n",
      "pre_train epoch 7800,loss 0.0027945239562541246,lr 1e-05,grad 0.01187364012002945\n",
      "pre_train epoch 7820,loss 0.0027881101705133915,lr 1e-05,grad 0.01189979538321495\n",
      "pre_train epoch 7840,loss 0.0027816612273454666,lr 1e-05,grad 0.011880002915859222\n",
      "pre_train epoch 7860,loss 0.002775276079773903,lr 1e-05,grad 0.011809078976511955\n",
      "pre_train epoch 7880,loss 0.002768749138340354,lr 1e-05,grad 0.011834408156573772\n",
      "pre_train epoch 7900,loss 0.002762098331004381,lr 1e-05,grad 0.01184089295566082\n",
      "pre_train epoch 7920,loss 0.0027555713895708323,lr 1e-05,grad 0.011868844740092754\n",
      "pre_train epoch 7940,loss 0.0027489017229527235,lr 1e-05,grad 0.011894671246409416\n",
      "pre_train epoch 7960,loss 0.0027422255370765924,lr 1e-05,grad 0.011808368377387524\n",
      "pre_train epoch 7980,loss 0.0027354650665074587,lr 1e-05,grad 0.011752030812203884\n",
      "pre_train epoch 8000,loss 0.002728712745010853,lr 1e-05,grad 0.011773125268518925\n",
      "pre_train epoch 8020,loss 0.0027219639159739017,lr 1e-05,grad 0.011615630239248276\n",
      "pre_train epoch 8040,loss 0.0027150562964379787,lr 1e-05,grad 0.01164684072136879\n",
      "pre_train epoch 8060,loss 0.0027080923318862915,lr 1e-05,grad 0.011640004813671112\n",
      "pre_train epoch 8080,loss 0.002701174933463335,lr 1e-05,grad 0.011631373316049576\n",
      "pre_train epoch 8100,loss 0.0026942293625324965,lr 1e-05,grad 0.011548243463039398\n",
      "pre_train epoch 8120,loss 0.0026871515437960625,lr 1e-05,grad 0.011532197706401348\n",
      "pre_train epoch 8140,loss 0.0026800225023180246,lr 1e-05,grad 0.011530873365700245\n",
      "pre_train epoch 8160,loss 0.0026730126701295376,lr 1e-05,grad 0.011569702066481113\n",
      "pre_train epoch 8180,loss 0.0026657769922167063,lr 1e-05,grad 0.011563862673938274\n",
      "pre_train epoch 8200,loss 0.002658528508618474,lr 1e-05,grad 0.011576717719435692\n",
      "pre_train epoch 8220,loss 0.002651272341609001,lr 1e-05,grad 0.011457938700914383\n",
      "pre_train epoch 8240,loss 0.0026440180372446775,lr 1e-05,grad 0.011450606398284435\n",
      "pre_train epoch 8260,loss 0.0026366112288087606,lr 1e-05,grad 0.011399266310036182\n",
      "pre_train epoch 8280,loss 0.0026291715912520885,lr 1e-05,grad 0.01136560644954443\n",
      "pre_train epoch 8300,loss 0.0026217265985906124,lr 1e-05,grad 0.01137514878064394\n",
      "pre_train epoch 8320,loss 0.0026142913848161697,lr 1e-05,grad 0.011368272826075554\n",
      "pre_train epoch 8340,loss 0.002606726950034499,lr 1e-05,grad 0.011403010226786137\n",
      "pre_train epoch 8360,loss 0.0025992097798734903,lr 1e-05,grad 0.01124007161706686\n",
      "pre_train epoch 8380,loss 0.002591562457382679,lr 1e-05,grad 0.01120066549628973\n",
      "pre_train epoch 8400,loss 0.002583882538601756,lr 1e-05,grad 0.011144542135298252\n",
      "pre_train epoch 8420,loss 0.002576231025159359,lr 1e-05,grad 0.011100981384515762\n",
      "pre_train epoch 8440,loss 0.002568539697676897,lr 1e-05,grad 0.011099936440587044\n",
      "pre_train epoch 8460,loss 0.0025606972631067038,lr 1e-05,grad 0.01122867688536644\n",
      "pre_train epoch 8480,loss 0.0025529256090521812,lr 1e-05,grad 0.011167616583406925\n",
      "pre_train epoch 8500,loss 0.0025450685061514378,lr 1e-05,grad 0.011133047752082348\n",
      "pre_train epoch 8520,loss 0.0025371620431542397,lr 1e-05,grad 0.011157941073179245\n",
      "pre_train epoch 8540,loss 0.0025291566271334887,lr 1e-05,grad 0.011072744615375996\n",
      "pre_train epoch 8560,loss 0.0025211821775883436,lr 1e-05,grad 0.011037498712539673\n",
      "pre_train epoch 8580,loss 0.0025131774600595236,lr 1e-05,grad 0.011032146401703358\n",
      "pre_train epoch 8600,loss 0.0025050982367247343,lr 1e-05,grad 0.010997899807989597\n",
      "pre_train epoch 8620,loss 0.0024969850201159716,lr 1e-05,grad 0.010961664840579033\n",
      "pre_train epoch 8640,loss 0.0024888559710234404,lr 1e-05,grad 0.011002731509506702\n",
      "pre_train epoch 8660,loss 0.002480726456269622,lr 1e-05,grad 0.010887334123253822\n",
      "pre_train epoch 8680,loss 0.0024724288377910852,lr 1e-05,grad 0.01082029938697815\n",
      "pre_train epoch 8700,loss 0.0024640867486596107,lr 1e-05,grad 0.01085585355758667\n",
      "pre_train epoch 8720,loss 0.002455890877172351,lr 1e-05,grad 0.010716896504163742\n",
      "pre_train epoch 8740,loss 0.0024475122336298227,lr 1e-05,grad 0.010830101557075977\n",
      "pre_train epoch 8760,loss 0.00243909633718431,lr 1e-05,grad 0.010809927247464657\n",
      "pre_train epoch 8780,loss 0.0024306373670697212,lr 1e-05,grad 0.010776102542877197\n",
      "pre_train epoch 8800,loss 0.002422117628157139,lr 1e-05,grad 0.010696661658585072\n",
      "pre_train epoch 8820,loss 0.0024136113934218884,lr 1e-05,grad 0.010760330595076084\n",
      "pre_train epoch 8840,loss 0.0024050564970821142,lr 1e-05,grad 0.010693155229091644\n",
      "pre_train epoch 8860,loss 0.0023964489810168743,lr 1e-05,grad 0.010621696710586548\n",
      "pre_train epoch 8880,loss 0.00238777999766171,lr 1e-05,grad 0.01058576162904501\n",
      "pre_train epoch 8900,loss 0.002379076089709997,lr 1e-05,grad 0.010631727054715157\n",
      "pre_train epoch 8920,loss 0.0023703749757260084,lr 1e-05,grad 0.01063358411192894\n",
      "pre_train epoch 8940,loss 0.00236164010129869,lr 1e-05,grad 0.010535995475947857\n",
      "pre_train epoch 8960,loss 0.0023528083693236113,lr 1e-05,grad 0.010579169727861881\n",
      "pre_train epoch 8980,loss 0.0023440225049853325,lr 1e-05,grad 0.010417243465781212\n",
      "pre_train epoch 9000,loss 0.002335117431357503,lr 1e-05,grad 0.01033671572804451\n",
      "pre_train epoch 9020,loss 0.0023262191098183393,lr 1e-05,grad 0.010429656133055687\n",
      "pre_train epoch 9040,loss 0.00231724139302969,lr 1e-05,grad 0.010407254099845886\n",
      "pre_train epoch 9060,loss 0.0023082131519913673,lr 1e-05,grad 0.010399320162832737\n",
      "pre_train epoch 9080,loss 0.002299242652952671,lr 1e-05,grad 0.010217315517365932\n",
      "pre_train epoch 9100,loss 0.0022901094052940607,lr 1e-05,grad 0.010330711491405964\n",
      "pre_train epoch 9120,loss 0.002281046938151121,lr 1e-05,grad 0.010330615565180779\n",
      "pre_train epoch 9140,loss 0.002271851757541299,lr 1e-05,grad 0.010205843485891819\n",
      "pre_train epoch 9160,loss 0.0022626605350524187,lr 1e-05,grad 0.010286916978657246\n",
      "pre_train epoch 9180,loss 0.0022535035386681557,lr 1e-05,grad 0.010189828462898731\n",
      "pre_train epoch 9200,loss 0.0022442066110670567,lr 1e-05,grad 0.0101138511672616\n",
      "pre_train epoch 9220,loss 0.0022348922211676836,lr 1e-05,grad 0.010192732326686382\n",
      "pre_train epoch 9240,loss 0.002225602976977825,lr 1e-05,grad 0.01005508191883564\n",
      "pre_train epoch 9260,loss 0.002216198481619358,lr 1e-05,grad 0.010012073442339897\n",
      "pre_train epoch 9280,loss 0.0022067823447287083,lr 1e-05,grad 0.010015186853706837\n",
      "pre_train epoch 9300,loss 0.0021973385009914637,lr 1e-05,grad 0.010052825324237347\n",
      "pre_train epoch 9320,loss 0.002187845529988408,lr 1e-05,grad 0.00981738232076168\n",
      "pre_train epoch 9340,loss 0.002178360940888524,lr 1e-05,grad 0.010052467696368694\n",
      "pre_train epoch 9360,loss 0.002168833278119564,lr 1e-05,grad 0.009984656237065792\n",
      "pre_train epoch 9380,loss 0.002159274183213711,lr 1e-05,grad 0.00989966094493866\n",
      "pre_train epoch 9400,loss 0.0021496517583727837,lr 1e-05,grad 0.009841843508183956\n",
      "pre_train epoch 9420,loss 0.0021400530822575092,lr 1e-05,grad 0.009831252507865429\n",
      "pre_train epoch 9440,loss 0.0021303934045135975,lr 1e-05,grad 0.009854973293840885\n",
      "pre_train epoch 9460,loss 0.0021206915844231844,lr 1e-05,grad 0.009648623876273632\n",
      "pre_train epoch 9480,loss 0.0021109783556312323,lr 1e-05,grad 0.009658902883529663\n",
      "pre_train epoch 9500,loss 0.002101236255839467,lr 1e-05,grad 0.009679310955107212\n",
      "pre_train epoch 9520,loss 0.002091363538056612,lr 1e-05,grad 0.009711666963994503\n",
      "pre_train epoch 9540,loss 0.0020816237665712833,lr 1e-05,grad 0.009696667082607746\n",
      "pre_train epoch 9560,loss 0.0020717752631753683,lr 1e-05,grad 0.009617192670702934\n",
      "pre_train epoch 9580,loss 0.0020619251299649477,lr 1e-05,grad 0.009648778475821018\n",
      "pre_train epoch 9600,loss 0.002052012365311384,lr 1e-05,grad 0.009509453549981117\n",
      "pre_train epoch 9620,loss 0.0020421387162059546,lr 1e-05,grad 0.009521243162453175\n",
      "pre_train epoch 9640,loss 0.002032187068834901,lr 1e-05,grad 0.009452203288674355\n",
      "pre_train epoch 9660,loss 0.002022204687818885,lr 1e-05,grad 0.009455776773393154\n",
      "pre_train epoch 9680,loss 0.0020121894776821136,lr 1e-05,grad 0.009326421655714512\n",
      "pre_train epoch 9700,loss 0.002002178458496928,lr 1e-05,grad 0.009342046454548836\n",
      "pre_train epoch 9720,loss 0.001992071745917201,lr 1e-05,grad 0.009366349317133427\n",
      "pre_train epoch 9740,loss 0.001982023473829031,lr 1e-05,grad 0.009249003604054451\n",
      "pre_train epoch 9760,loss 0.0019719202537089586,lr 1e-05,grad 0.00925974827259779\n",
      "pre_train epoch 9780,loss 0.001961753936484456,lr 1e-05,grad 0.009253657422959805\n",
      "pre_train epoch 9800,loss 0.0019515631720423698,lr 1e-05,grad 0.009192834608256817\n",
      "pre_train epoch 9820,loss 0.0019414378330111504,lr 1e-05,grad 0.009031398221850395\n",
      "pre_train epoch 9840,loss 0.0019312796648591757,lr 1e-05,grad 0.009195663966238499\n",
      "pre_train epoch 9860,loss 0.0019209908787161112,lr 1e-05,grad 0.009163524955511093\n",
      "pre_train epoch 9880,loss 0.0019107300322502851,lr 1e-05,grad 0.009065041318535805\n",
      "pre_train epoch 9900,loss 0.0019004245987161994,lr 1e-05,grad 0.008964957669377327\n",
      "pre_train epoch 9920,loss 0.0018902183510363102,lr 1e-05,grad 0.009042753838002682\n",
      "pre_train epoch 9940,loss 0.0018798541277647018,lr 1e-05,grad 0.008967707864940166\n",
      "pre_train epoch 9960,loss 0.0018695505568757653,lr 1e-05,grad 0.008842157199978828\n",
      "pre_train epoch 9980,loss 0.001859223935753107,lr 1e-05,grad 0.00883762538433075\n",
      "pre_train epoch 10000,loss 0.0018488374771550298,lr 1e-05,grad 0.008746344596147537\n",
      "pre_train epoch 10020,loss 0.0018384396098554134,lr 1e-05,grad 0.008905517868697643\n",
      "pre_train epoch 10040,loss 0.0018280689837411046,lr 1e-05,grad 0.00875968486070633\n",
      "pre_train epoch 10060,loss 0.0018176736775785685,lr 1e-05,grad 0.008819363079965115\n",
      "pre_train epoch 10080,loss 0.0018071968806907535,lr 1e-05,grad 0.008687118999660015\n",
      "pre_train epoch 10100,loss 0.0017967582680284977,lr 1e-05,grad 0.008690427988767624\n",
      "pre_train epoch 10120,loss 0.0017863126704469323,lr 1e-05,grad 0.008703213185071945\n",
      "pre_train epoch 10140,loss 0.0017758465837687254,lr 1e-05,grad 0.008679992519319057\n",
      "pre_train epoch 10160,loss 0.001765336375683546,lr 1e-05,grad 0.008498500101268291\n",
      "pre_train epoch 10180,loss 0.0017548799514770508,lr 1e-05,grad 0.008557068184018135\n",
      "pre_train epoch 10200,loss 0.001744348555803299,lr 1e-05,grad 0.008527630940079689\n",
      "pre_train epoch 10220,loss 0.0017337989993393421,lr 1e-05,grad 0.008428793400526047\n",
      "pre_train epoch 10240,loss 0.0017232840182259679,lr 1e-05,grad 0.008454264141619205\n",
      "pre_train epoch 10260,loss 0.0017127504106611013,lr 1e-05,grad 0.008387349545955658\n",
      "pre_train epoch 10280,loss 0.0017022102838382125,lr 1e-05,grad 0.008419076912105083\n",
      "pre_train epoch 10300,loss 0.001691607409156859,lr 1e-05,grad 0.00832434557378292\n",
      "pre_train epoch 10320,loss 0.001681091613136232,lr 1e-05,grad 0.008313390426337719\n",
      "pre_train epoch 10340,loss 0.0016705108573660254,lr 1e-05,grad 0.008300219662487507\n",
      "pre_train epoch 10360,loss 0.0016599168302491307,lr 1e-05,grad 0.008096735924482346\n",
      "pre_train epoch 10380,loss 0.0016493317671120167,lr 1e-05,grad 0.007994216866791248\n",
      "pre_train epoch 10400,loss 0.0016387205105274916,lr 1e-05,grad 0.007982674986124039\n",
      "pre_train epoch 10420,loss 0.0016281557036563754,lr 1e-05,grad 0.008042349480092525\n",
      "pre_train epoch 10440,loss 0.0016175830969586968,lr 1e-05,grad 0.008079067803919315\n",
      "pre_train epoch 10460,loss 0.0016069618286564946,lr 1e-05,grad 0.007952537387609482\n",
      "pre_train epoch 10480,loss 0.0015963342739269137,lr 1e-05,grad 0.007879781536757946\n",
      "pre_train epoch 10500,loss 0.0015857063699513674,lr 1e-05,grad 0.007939002476632595\n",
      "pre_train epoch 10520,loss 0.001575134927406907,lr 1e-05,grad 0.007961489260196686\n",
      "pre_train epoch 10540,loss 0.0015645254170522094,lr 1e-05,grad 0.007938182912766933\n",
      "pre_train epoch 10560,loss 0.0015539004234597087,lr 1e-05,grad 0.007864521816372871\n",
      "pre_train epoch 10580,loss 0.001543325837701559,lr 1e-05,grad 0.007837544195353985\n",
      "pre_train epoch 10600,loss 0.0015327342553064227,lr 1e-05,grad 0.007869505323469639\n",
      "pre_train epoch 10620,loss 0.0015220807399600744,lr 1e-05,grad 0.007837323471903801\n",
      "pre_train epoch 10640,loss 0.001511503360234201,lr 1e-05,grad 0.007705034222453833\n",
      "pre_train epoch 10660,loss 0.001500944490544498,lr 1e-05,grad 0.007705762982368469\n",
      "pre_train epoch 10680,loss 0.0014903773553669453,lr 1e-05,grad 0.007689179852604866\n",
      "pre_train epoch 10700,loss 0.0014798266347497702,lr 1e-05,grad 0.007569462526589632\n",
      "pre_train epoch 10720,loss 0.0014692237600684166,lr 1e-05,grad 0.007458562031388283\n",
      "pre_train epoch 10740,loss 0.0014586594188585877,lr 1e-05,grad 0.0075978608801960945\n",
      "pre_train epoch 10760,loss 0.0014480921672657132,lr 1e-05,grad 0.007505548652261496\n",
      "pre_train epoch 10780,loss 0.0014375271275639534,lr 1e-05,grad 0.007467507850378752\n",
      "pre_train epoch 10800,loss 0.0014269774546846747,lr 1e-05,grad 0.0074201710522174835\n",
      "pre_train epoch 10820,loss 0.0014164504827931523,lr 1e-05,grad 0.007311901077628136\n",
      "pre_train epoch 10840,loss 0.0014059327077120543,lr 1e-05,grad 0.007367108482867479\n",
      "pre_train epoch 10860,loss 0.001395425177179277,lr 1e-05,grad 0.007493665441870689\n",
      "pre_train epoch 10880,loss 0.0013849862152710557,lr 1e-05,grad 0.007259321864694357\n",
      "pre_train epoch 10900,loss 0.001374491723254323,lr 1e-05,grad 0.007480625994503498\n",
      "pre_train epoch 10920,loss 0.0013640066608786583,lr 1e-05,grad 0.007221027743071318\n",
      "pre_train epoch 10940,loss 0.0013535649050027132,lr 1e-05,grad 0.007142218295484781\n",
      "pre_train epoch 10960,loss 0.0013431253610178828,lr 1e-05,grad 0.007392167113721371\n",
      "pre_train epoch 10980,loss 0.0013327021151781082,lr 1e-05,grad 0.007040124386548996\n",
      "pre_train epoch 11000,loss 0.0013223113492131233,lr 1e-05,grad 0.007051440887153149\n",
      "pre_train epoch 11020,loss 0.0013119678478688002,lr 1e-05,grad 0.006986100692301989\n",
      "pre_train epoch 11040,loss 0.001301620970480144,lr 1e-05,grad 0.006856951396912336\n",
      "pre_train epoch 11060,loss 0.0012912715319544077,lr 1e-05,grad 0.007064232137054205\n",
      "pre_train epoch 11080,loss 0.0012809656327590346,lr 1e-05,grad 0.006772163324058056\n",
      "pre_train epoch 11100,loss 0.0012707196874544024,lr 1e-05,grad 0.006927880924195051\n",
      "pre_train epoch 11120,loss 0.0012604249641299248,lr 1e-05,grad 0.007007588166743517\n",
      "pre_train epoch 11140,loss 0.001250166678801179,lr 1e-05,grad 0.006626106798648834\n",
      "pre_train epoch 11160,loss 0.0012400001287460327,lr 1e-05,grad 0.006761499214917421\n",
      "pre_train epoch 11180,loss 0.0012297866633161902,lr 1e-05,grad 0.007103554904460907\n",
      "pre_train epoch 11200,loss 0.0012195775052532554,lr 1e-05,grad 0.006711650174111128\n",
      "pre_train epoch 11220,loss 0.0012094782432541251,lr 1e-05,grad 0.006615870166569948\n",
      "pre_train epoch 11240,loss 0.0011993242660537362,lr 1e-05,grad 0.0066872755996882915\n",
      "pre_train epoch 11260,loss 0.001189251197502017,lr 1e-05,grad 0.00640441756695509\n",
      "pre_train epoch 11280,loss 0.001179223763756454,lr 1e-05,grad 0.006503559183329344\n",
      "pre_train epoch 11300,loss 0.0011692193802446127,lr 1e-05,grad 0.006574587896466255\n",
      "pre_train epoch 11320,loss 0.0011592218652367592,lr 1e-05,grad 0.00632120855152607\n",
      "pre_train epoch 11340,loss 0.0011493023484945297,lr 1e-05,grad 0.0063762543722987175\n",
      "pre_train epoch 11360,loss 0.0011393368477001786,lr 1e-05,grad 0.006239240523427725\n",
      "pre_train epoch 11380,loss 0.001129484036937356,lr 1e-05,grad 0.006353458389639854\n",
      "pre_train epoch 11400,loss 0.0011196293635293841,lr 1e-05,grad 0.0063434201292693615\n",
      "pre_train epoch 11420,loss 0.0011098020477220416,lr 1e-05,grad 0.006236691027879715\n",
      "pre_train epoch 11440,loss 0.0011000664671882987,lr 1e-05,grad 0.006291422061622143\n",
      "pre_train epoch 11460,loss 0.0010903095826506615,lr 1e-05,grad 0.006354591343551874\n",
      "pre_train epoch 11480,loss 0.0010805977508425713,lr 1e-05,grad 0.006122507620602846\n",
      "pre_train epoch 11500,loss 0.001070972066372633,lr 1e-05,grad 0.006199447438120842\n",
      "pre_train epoch 11520,loss 0.001061327406205237,lr 1e-05,grad 0.006160893011838198\n",
      "pre_train epoch 11540,loss 0.001051731058396399,lr 1e-05,grad 0.005997410509735346\n",
      "pre_train epoch 11560,loss 0.0010422185296192765,lr 1e-05,grad 0.005942882504314184\n",
      "pre_train epoch 11580,loss 0.0010327458148822188,lr 1e-05,grad 0.005954807158559561\n",
      "pre_train epoch 11600,loss 0.0010233059292659163,lr 1e-05,grad 0.005838038399815559\n",
      "pre_train epoch 11620,loss 0.0010138638317584991,lr 1e-05,grad 0.005891011096537113\n",
      "pre_train epoch 11640,loss 0.001004525925964117,lr 1e-05,grad 0.005880970042198896\n",
      "pre_train epoch 11660,loss 0.0009951723041012883,lr 1e-05,grad 0.005895461421459913\n",
      "pre_train epoch 11680,loss 0.000985941500402987,lr 1e-05,grad 0.005707534030079842\n",
      "pre_train epoch 11700,loss 0.0009767068549990654,lr 1e-05,grad 0.005456630606204271\n",
      "pre_train epoch 11720,loss 0.0009675188921391964,lr 1e-05,grad 0.005759320221841335\n",
      "pre_train epoch 11740,loss 0.0009583827340975404,lr 1e-05,grad 0.005460764747112989\n",
      "pre_train epoch 11760,loss 0.0009492929675616324,lr 1e-05,grad 0.0056416792795062065\n",
      "pre_train epoch 11780,loss 0.0009402814321219921,lr 1e-05,grad 0.005306769162416458\n",
      "pre_train epoch 11800,loss 0.0009312682668678463,lr 1e-05,grad 0.005856564734131098\n",
      "pre_train epoch 11820,loss 0.0009223889210261405,lr 1e-05,grad 0.0053576137870550156\n",
      "pre_train epoch 11840,loss 0.0009134707506746054,lr 1e-05,grad 0.005203087814152241\n",
      "pre_train epoch 11860,loss 0.0009046379709616303,lr 1e-05,grad 0.005518564023077488\n",
      "pre_train epoch 11880,loss 0.0008958019898273051,lr 1e-05,grad 0.005423020105808973\n",
      "pre_train epoch 11900,loss 0.0008871323079802096,lr 1e-05,grad 0.00565236434340477\n",
      "pre_train epoch 11920,loss 0.0008784212404862046,lr 1e-05,grad 0.005123624578118324\n",
      "pre_train epoch 11940,loss 0.0008698368910700083,lr 1e-05,grad 0.005479980725795031\n",
      "pre_train epoch 11960,loss 0.0008612209931015968,lr 1e-05,grad 0.004940613638609648\n",
      "pre_train epoch 11980,loss 0.0008527216850779951,lr 1e-05,grad 0.005427749361842871\n",
      "pre_train epoch 12000,loss 0.0008442438556812704,lr 1e-05,grad 0.0054993159137666225\n",
      "pre_train epoch 12020,loss 0.0008358542690984905,lr 1e-05,grad 0.0050573283806443214\n",
      "pre_train epoch 12040,loss 0.0008274816791526973,lr 1e-05,grad 0.005172006320208311\n",
      "pre_train epoch 12060,loss 0.000819159671664238,lr 1e-05,grad 0.00484575517475605\n",
      "pre_train epoch 12080,loss 0.0008109122863970697,lr 1e-05,grad 0.005001672077924013\n",
      "pre_train epoch 12100,loss 0.0008026869036257267,lr 1e-05,grad 0.004770865198224783\n",
      "pre_train epoch 12120,loss 0.000794560241047293,lr 1e-05,grad 0.004900061059743166\n",
      "pre_train epoch 12140,loss 0.0007864591316320002,lr 1e-05,grad 0.004783150274306536\n",
      "pre_train epoch 12160,loss 0.0007784410263411701,lr 1e-05,grad 0.004746849182993174\n",
      "pre_train epoch 12180,loss 0.0007704526651650667,lr 1e-05,grad 0.00499239657074213\n",
      "pre_train epoch 12200,loss 0.0007625388097949326,lr 1e-05,grad 0.005122016184031963\n",
      "pre_train epoch 12220,loss 0.0007546822307631373,lr 1e-05,grad 0.004659832920879126\n",
      "pre_train epoch 12240,loss 0.0007468516123481095,lr 1e-05,grad 0.004464942496269941\n",
      "pre_train epoch 12260,loss 0.0007391278049908578,lr 1e-05,grad 0.0044142454862594604\n",
      "pre_train epoch 12280,loss 0.0007314318208955228,lr 1e-05,grad 0.004742955788969994\n",
      "pre_train epoch 12300,loss 0.0007237856043502688,lr 1e-05,grad 0.004350306466221809\n",
      "pre_train epoch 12320,loss 0.0007162091787904501,lr 1e-05,grad 0.005129612982273102\n",
      "pre_train epoch 12340,loss 0.000708674022462219,lr 1e-05,grad 0.006758987437933683\n",
      "pre_train epoch 12360,loss 0.0007012300193309784,lr 1e-05,grad 0.004732735920697451\n",
      "pre_train epoch 12380,loss 0.000693814770784229,lr 1e-05,grad 0.004643579944968224\n",
      "pre_train epoch 12400,loss 0.0006864398019388318,lr 1e-05,grad 0.004151671193540096\n",
      "pre_train epoch 12420,loss 0.0006791581981815398,lr 1e-05,grad 0.005062611773610115\n",
      "pre_train epoch 12440,loss 0.0006719169323332608,lr 1e-05,grad 0.004644808825105429\n",
      "pre_train epoch 12460,loss 0.0006647343398071826,lr 1e-05,grad 0.0041413805447518826\n",
      "pre_train epoch 12480,loss 0.0006576074520125985,lr 1e-05,grad 0.004242869559675455\n",
      "pre_train epoch 12500,loss 0.0006505222991108894,lr 1e-05,grad 0.003962067421525717\n",
      "pre_train epoch 12520,loss 0.000643519451841712,lr 1e-05,grad 0.003939931746572256\n",
      "pre_train epoch 12540,loss 0.0006365748704411089,lr 1e-05,grad 0.0044534350745379925\n",
      "pre_train epoch 12560,loss 0.0006296790088526905,lr 1e-05,grad 0.007462873123586178\n",
      "pre_train epoch 12580,loss 0.0006351338233798742,lr 1e-05,grad 0.5226426720619202\n",
      "pre_train epoch 12600,loss 0.0006169942207634449,lr 1e-05,grad 0.1354353129863739\n",
      "pre_train epoch 12620,loss 0.0006099097663536668,lr 1e-05,grad 0.08825173228979111\n",
      "pre_train epoch 12640,loss 0.0006030492950230837,lr 1e-05,grad 0.015275044366717339\n",
      "pre_train epoch 12660,loss 0.0005965602467767894,lr 1e-05,grad 0.006772336084395647\n",
      "pre_train epoch 12680,loss 0.0005901432014070451,lr 1e-05,grad 0.005486094858497381\n",
      "pre_train epoch 12700,loss 0.0005837870994582772,lr 1e-05,grad 0.005555220413953066\n",
      "pre_train epoch 12720,loss 0.0005774539895355701,lr 1e-05,grad 0.006644147913902998\n",
      "pre_train epoch 12740,loss 0.0005711884587071836,lr 1e-05,grad 0.0047590844333171844\n",
      "pre_train epoch 12760,loss 0.0005649742670357227,lr 1e-05,grad 0.009109186939895153\n",
      "pre_train epoch 12780,loss 0.0005589036154560745,lr 1e-05,grad 0.047263745218515396\n",
      "pre_train epoch 12800,loss 0.0005590543150901794,lr 1e-05,grad 0.37677422165870667\n",
      "pre_train epoch 12820,loss 0.000546820810995996,lr 1e-05,grad 0.04275818541646004\n",
      "pre_train epoch 12840,loss 0.000540832697879523,lr 1e-05,grad 0.013069532811641693\n",
      "pre_train epoch 12860,loss 0.0005349518614821136,lr 1e-05,grad 0.010128665715456009\n",
      "pre_train epoch 12880,loss 0.0005291030392982066,lr 1e-05,grad 0.007535544689744711\n",
      "pre_train epoch 12900,loss 0.0005233523552305996,lr 1e-05,grad 0.009311406873166561\n",
      "pre_train epoch 12920,loss 0.0005177554558031261,lr 1e-05,grad 0.056203462183475494\n",
      "pre_train epoch 12940,loss 0.0005122825386933982,lr 1e-05,grad 0.08719807118177414\n",
      "pre_train epoch 12960,loss 0.0005068217287771404,lr 1e-05,grad 0.0893193706870079\n",
      "pre_train epoch 12980,loss 0.0005011348403058946,lr 1e-05,grad 0.060202617198228836\n",
      "pre_train epoch 13000,loss 0.0004955558106303215,lr 1e-05,grad 0.007070181425660849\n",
      "pre_train epoch 13020,loss 0.0004901838256046176,lr 1e-05,grad 0.0033723008818924427\n",
      "pre_train epoch 13040,loss 0.00048486576997675,lr 1e-05,grad 0.00997981894761324\n",
      "pre_train epoch 13060,loss 0.0004796012071892619,lr 1e-05,grad 0.010071597062051296\n",
      "pre_train epoch 13080,loss 0.0004743649042211473,lr 1e-05,grad 0.02266254648566246\n",
      "pre_train epoch 13100,loss 0.00048262698692269623,lr 1e-05,grad 0.5463120937347412\n",
      "pre_train epoch 13120,loss 0.0004664478765334934,lr 1e-05,grad 0.2283434122800827\n",
      "pre_train epoch 13140,loss 0.0004591118486132473,lr 1e-05,grad 0.02513989992439747\n",
      "pre_train epoch 13160,loss 0.0004541336966212839,lr 1e-05,grad 0.0030796893406659365\n",
      "pre_train epoch 13180,loss 0.0004492166335694492,lr 1e-05,grad 0.008918650448322296\n",
      "pre_train epoch 13200,loss 0.0004443111829459667,lr 1e-05,grad 0.004825728479772806\n",
      "pre_train epoch 13220,loss 0.00043948169331997633,lr 1e-05,grad 0.003092213300988078\n",
      "pre_train epoch 13240,loss 0.00043470069067552686,lr 1e-05,grad 0.003249778412282467\n",
      "pre_train epoch 13260,loss 0.0004316152771934867,lr 1e-05,grad 0.19262957572937012\n",
      "pre_train epoch 13280,loss 0.0004375606367830187,lr 1e-05,grad 0.5212429165840149\n",
      "pre_train epoch 13300,loss 0.0004217367386445403,lr 1e-05,grad 0.13952180743217468\n",
      "pre_train epoch 13320,loss 0.00041640130802989006,lr 1e-05,grad 0.003299429314211011\n",
      "pre_train epoch 13340,loss 0.000412045483244583,lr 1e-05,grad 0.026414917781949043\n",
      "pre_train epoch 13360,loss 0.0004076280165463686,lr 1e-05,grad 0.004957637749612331\n",
      "pre_train epoch 13380,loss 0.0004033198347315192,lr 1e-05,grad 0.0032225390896201134\n",
      "pre_train epoch 13400,loss 0.0003990223631262779,lr 1e-05,grad 0.0032490554731339216\n",
      "pre_train epoch 13420,loss 0.00039478277903981507,lr 1e-05,grad 0.003124844515696168\n",
      "pre_train epoch 13440,loss 0.00039056901005096734,lr 1e-05,grad 0.004627414513379335\n",
      "pre_train epoch 13460,loss 0.0003863805904984474,lr 1e-05,grad 0.003519268473610282\n",
      "pre_train epoch 13480,loss 0.00038223047158680856,lr 1e-05,grad 0.0062186699360609055\n",
      "pre_train epoch 13500,loss 0.0003781401610467583,lr 1e-05,grad 0.012836869806051254\n",
      "pre_train epoch 13520,loss 0.0003744877176359296,lr 1e-05,grad 0.0980943813920021\n",
      "pre_train epoch 13540,loss 0.00038244525785557926,lr 1e-05,grad 0.5263564586639404\n",
      "pre_train epoch 13560,loss 0.00036739540519192815,lr 1e-05,grad 0.16357019543647766\n",
      "pre_train epoch 13580,loss 0.00036239082692191005,lr 1e-05,grad 0.03236447647213936\n",
      "pre_train epoch 13600,loss 0.0003585349768400192,lr 1e-05,grad 0.02172253094613552\n",
      "pre_train epoch 13620,loss 0.00035473500611260533,lr 1e-05,grad 0.0034369893837720156\n",
      "pre_train epoch 13640,loss 0.0003509704547468573,lr 1e-05,grad 0.009088189341127872\n",
      "pre_train epoch 13660,loss 0.0003472830285318196,lr 1e-05,grad 0.0037821372970938683\n",
      "pre_train epoch 13680,loss 0.00034360415884293616,lr 1e-05,grad 0.013706350699067116\n",
      "pre_train epoch 13700,loss 0.00034058591700159013,lr 1e-05,grad 0.11912851780653\n",
      "pre_train epoch 13720,loss 0.00034444016637280583,lr 1e-05,grad 0.42626726627349854\n",
      "pre_train epoch 13740,loss 0.000332944153342396,lr 1e-05,grad 0.046574074774980545\n",
      "pre_train epoch 13760,loss 0.00032961383112706244,lr 1e-05,grad 0.06866051256656647\n",
      "pre_train epoch 13780,loss 0.0003260014927946031,lr 1e-05,grad 0.025209806859493256\n",
      "pre_train epoch 13800,loss 0.0003225607506465167,lr 1e-05,grad 0.007308922708034515\n",
      "pre_train epoch 13820,loss 0.00031920542824082077,lr 1e-05,grad 0.010823944583535194\n",
      "pre_train epoch 13840,loss 0.00031586200930178165,lr 1e-05,grad 0.011953525245189667\n",
      "pre_train epoch 13860,loss 0.00031476127333007753,lr 1e-05,grad 0.22377680242061615\n",
      "pre_train epoch 13880,loss 0.0003157213795930147,lr 1e-05,grad 0.3783595561981201\n",
      "pre_train epoch 13900,loss 0.0003071977407671511,lr 1e-05,grad 0.15473057329654694\n",
      "pre_train epoch 13920,loss 0.00030307285487651825,lr 1e-05,grad 0.03747823089361191\n",
      "pre_train epoch 13940,loss 0.00029993124189786613,lr 1e-05,grad 0.008563731797039509\n",
      "pre_train epoch 13960,loss 0.000296837737550959,lr 1e-05,grad 0.011300981044769287\n",
      "pre_train epoch 13980,loss 0.00029378829640336335,lr 1e-05,grad 0.006372921168804169\n",
      "pre_train epoch 14000,loss 0.0002907713642343879,lr 1e-05,grad 0.01997382380068302\n",
      "pre_train epoch 14020,loss 0.0002937952522188425,lr 1e-05,grad 0.3673502504825592\n",
      "pre_train epoch 14040,loss 0.00028491541161201894,lr 1e-05,grad 0.047590386122465134\n",
      "pre_train epoch 14060,loss 0.0002829715667758137,lr 1e-05,grad 0.14933548867702484\n",
      "pre_train epoch 14080,loss 0.0002791706065181643,lr 1e-05,grad 0.031115049496293068\n",
      "pre_train epoch 14100,loss 0.00027634279103949666,lr 1e-05,grad 0.011730064637959003\n",
      "pre_train epoch 14120,loss 0.000273518031463027,lr 1e-05,grad 0.008648804388940334\n",
      "pre_train epoch 14140,loss 0.0002707413805183023,lr 1e-05,grad 0.0062328604981303215\n",
      "pre_train epoch 14160,loss 0.000268118194071576,lr 1e-05,grad 0.05149257928133011\n",
      "pre_train epoch 14180,loss 0.0002953419752884656,lr 1e-05,grad 0.8202947974205017\n",
      "pre_train epoch 14200,loss 0.0002640427846927196,lr 1e-05,grad 0.1762896329164505\n",
      "pre_train epoch 14220,loss 0.00026021592202596366,lr 1e-05,grad 0.0554511584341526\n",
      "pre_train epoch 14240,loss 0.00025754261878319085,lr 1e-05,grad 0.019632011651992798\n",
      "pre_train epoch 14260,loss 0.0002549727214500308,lr 1e-05,grad 0.0067503200843930244\n",
      "pre_train epoch 14280,loss 0.00025244185235351324,lr 1e-05,grad 0.005399474874138832\n",
      "pre_train epoch 14300,loss 0.0002499483525753021,lr 1e-05,grad 0.0023978562094271183\n",
      "pre_train epoch 14320,loss 0.00024745261180214584,lr 1e-05,grad 0.005121029447764158\n",
      "pre_train epoch 14340,loss 0.00024863722501322627,lr 1e-05,grad 0.2857643961906433\n",
      "pre_train epoch 14360,loss 0.00024348513397853822,lr 1e-05,grad 0.13963469862937927\n",
      "pre_train epoch 14380,loss 0.00024122651666402817,lr 1e-05,grad 0.1465490460395813\n",
      "pre_train epoch 14400,loss 0.00023815540771465749,lr 1e-05,grad 0.058841925114393234\n",
      "pre_train epoch 14420,loss 0.00023570928897242993,lr 1e-05,grad 0.004721899051219225\n",
      "pre_train epoch 14440,loss 0.00023345033696386963,lr 1e-05,grad 0.011868320405483246\n",
      "pre_train epoch 14460,loss 0.00023121191770769656,lr 1e-05,grad 0.005535178817808628\n",
      "pre_train epoch 14480,loss 0.00022898626048117876,lr 1e-05,grad 0.004429997876286507\n",
      "pre_train epoch 14500,loss 0.00022678772802464664,lr 1e-05,grad 0.002270346973091364\n",
      "pre_train epoch 14520,loss 0.00022458724561147392,lr 1e-05,grad 0.01294700987637043\n",
      "pre_train epoch 14540,loss 0.0002266363298986107,lr 1e-05,grad 0.3078524172306061\n",
      "pre_train epoch 14560,loss 0.00022279350378084928,lr 1e-05,grad 0.2388393133878708\n",
      "pre_train epoch 14580,loss 0.00021980571909807622,lr 1e-05,grad 0.18899530172348022\n",
      "pre_train epoch 14600,loss 0.0002162118471460417,lr 1e-05,grad 0.03244871646165848\n",
      "pre_train epoch 14620,loss 0.00021418722462840378,lr 1e-05,grad 0.02636411227285862\n",
      "pre_train epoch 14640,loss 0.00021213857689872384,lr 1e-05,grad 0.009700118564069271\n",
      "pre_train epoch 14660,loss 0.0002101437421515584,lr 1e-05,grad 0.0016005861107259989\n",
      "pre_train epoch 14680,loss 0.00020815333118662238,lr 1e-05,grad 0.001718520768918097\n",
      "pre_train epoch 14700,loss 0.0002061787381535396,lr 1e-05,grad 0.0017010316951200366\n",
      "pre_train epoch 14720,loss 0.00020427007984835654,lr 1e-05,grad 0.034148331731557846\n",
      "pre_train epoch 14740,loss 0.00020258352742530406,lr 1e-05,grad 0.0802793949842453\n",
      "pre_train epoch 14760,loss 0.00020286839571781456,lr 1e-05,grad 0.23203620314598083\n",
      "pre_train epoch 14780,loss 0.0001992121251532808,lr 1e-05,grad 0.10518389940261841\n",
      "pre_train epoch 14800,loss 0.0001971097372006625,lr 1e-05,grad 0.05474061518907547\n",
      "pre_train epoch 14820,loss 0.0001952249149326235,lr 1e-05,grad 0.006460240110754967\n",
      "pre_train epoch 14840,loss 0.00019349827198311687,lr 1e-05,grad 0.006798955611884594\n",
      "pre_train epoch 14860,loss 0.00019176806381437927,lr 1e-05,grad 0.0069842832162976265\n",
      "pre_train epoch 14880,loss 0.00019006944785360247,lr 1e-05,grad 0.001596880960278213\n",
      "pre_train epoch 14900,loss 0.0001883588993223384,lr 1e-05,grad 0.001667650998570025\n",
      "pre_train epoch 14920,loss 0.0001866668462753296,lr 1e-05,grad 0.0041602314449846745\n",
      "pre_train epoch 14940,loss 0.0001849931722972542,lr 1e-05,grad 0.0025727637112140656\n",
      "pre_train epoch 14960,loss 0.0001833097921917215,lr 1e-05,grad 0.00949801318347454\n",
      "pre_train epoch 14980,loss 0.0001816392468754202,lr 1e-05,grad 0.0029964155983179808\n",
      "pre_train epoch 15000,loss 0.00018007494509220123,lr 1e-05,grad 0.04403117671608925\n",
      "pre_train epoch 15020,loss 0.00021969841327518225,lr 1e-05,grad 0.9660640358924866\n",
      "pre_train epoch 15040,loss 0.00018166708468925208,lr 1e-05,grad 0.32996615767478943\n",
      "pre_train epoch 15060,loss 0.00017570449563208967,lr 1e-05,grad 0.09764722734689713\n",
      "pre_train epoch 15080,loss 0.00017387674597557634,lr 1e-05,grad 0.04600631073117256\n",
      "pre_train epoch 15100,loss 0.00017226776981260628,lr 1e-05,grad 0.010088099166750908\n",
      "pre_train epoch 15120,loss 0.00017076742369681597,lr 1e-05,grad 0.00725407712161541\n",
      "pre_train epoch 15140,loss 0.0001692736695986241,lr 1e-05,grad 0.0014079168904572725\n",
      "pre_train epoch 15160,loss 0.00016778713325038552,lr 1e-05,grad 0.002205341588705778\n",
      "pre_train epoch 15180,loss 0.00016631974722258747,lr 1e-05,grad 0.00638950988650322\n",
      "pre_train epoch 15200,loss 0.00016487071115989238,lr 1e-05,grad 0.021202677860856056\n",
      "pre_train epoch 15220,loss 0.0002290450647706166,lr 1e-05,grad 1.2165549993515015\n",
      "pre_train epoch 15240,loss 0.00016292820509988815,lr 1.0000000000000002e-06,grad 0.033875830471515656\n",
      "pre_train epoch 15260,loss 0.0001627494493732229,lr 1.0000000000000002e-06,grad 0.01211197953671217\n",
      "pre_train epoch 15280,loss 0.00016260375559795648,lr 1.0000000000000002e-06,grad 0.004249969031661749\n",
      "pre_train epoch 15300,loss 0.00016245932783931494,lr 1.0000000000000002e-06,grad 0.0016580895753577352\n",
      "pre_train epoch 15320,loss 0.000162319207447581,lr 1.0000000000000002e-06,grad 0.0014465112471953034\n",
      "pre_train epoch 15340,loss 0.00016217760276049376,lr 1.0000000000000002e-06,grad 0.0015591246774420142\n",
      "pre_train epoch 15360,loss 0.00016202816914301366,lr 1.0000000000000002e-06,grad 0.0015631201677024364\n",
      "pre_train epoch 15380,loss 0.00016188365407288074,lr 1.0000000000000002e-06,grad 0.0016283084405586123\n",
      "pre_train epoch 15400,loss 0.0001617432717466727,lr 1.0000000000000002e-06,grad 0.0015456213150173426\n",
      "pre_train epoch 15420,loss 0.00016158416110556573,lr 1.0000000000000002e-06,grad 0.0015220374334603548\n",
      "pre_train epoch 15440,loss 0.0001614428765606135,lr 1.0000000000000002e-06,grad 0.001470061019062996\n",
      "pre_train epoch 15460,loss 0.00016128757852129638,lr 1.0000000000000002e-06,grad 0.0015249738935381174\n",
      "pre_train epoch 15480,loss 0.0001611383631825447,lr 1.0000000000000002e-06,grad 0.0015084309270605445\n",
      "pre_train epoch 15500,loss 0.00016098794003482908,lr 1.0000000000000002e-06,grad 0.0015628916444256902\n",
      "pre_train epoch 15520,loss 0.00016081606736406684,lr 1.0000000000000002e-06,grad 0.0014642681926488876\n",
      "pre_train epoch 15540,loss 0.00016066555690485984,lr 1.0000000000000002e-06,grad 0.0015578814782202244\n",
      "pre_train epoch 15560,loss 0.0001605135330464691,lr 1.0000000000000002e-06,grad 0.0013922788202762604\n",
      "pre_train epoch 15580,loss 0.0001603372802492231,lr 1.0000000000000002e-06,grad 0.0016621645772829652\n",
      "pre_train epoch 15600,loss 0.0001601881522219628,lr 1.0000000000000002e-06,grad 0.001467765192501247\n",
      "pre_train epoch 15620,loss 0.00016003356722649187,lr 1.0000000000000002e-06,grad 0.0016184172127395868\n",
      "pre_train epoch 15640,loss 0.00015986045764293522,lr 1.0000000000000002e-06,grad 0.0015175662701949477\n",
      "pre_train epoch 15660,loss 0.00015969641390256584,lr 1.0000000000000002e-06,grad 0.0015169394901022315\n",
      "pre_train epoch 15680,loss 0.00015953570255078375,lr 1.0000000000000002e-06,grad 0.0015210994752123952\n",
      "pre_train epoch 15700,loss 0.00015936815179884434,lr 1.0000000000000002e-06,grad 0.0014909011078998446\n",
      "pre_train epoch 15720,loss 0.00015919606084935367,lr 1.0000000000000002e-06,grad 0.0015123427147045732\n",
      "pre_train epoch 15740,loss 0.00015902111772447824,lr 1.0000000000000002e-06,grad 0.0014885893324390054\n",
      "pre_train epoch 15760,loss 0.00015885703032836318,lr 1.0000000000000002e-06,grad 0.001503345905803144\n",
      "pre_train epoch 15780,loss 0.00015868381888139993,lr 1.0000000000000002e-06,grad 0.0015767707955092192\n",
      "pre_train epoch 15800,loss 0.00015849868941586465,lr 1.0000000000000002e-06,grad 0.0014823373639956117\n",
      "pre_train epoch 15820,loss 0.0001583212724654004,lr 1.0000000000000002e-06,grad 0.0015201270580291748\n",
      "pre_train epoch 15840,loss 0.00015815524966455996,lr 1.0000000000000002e-06,grad 0.001549560809507966\n",
      "pre_train epoch 15860,loss 0.00015796558000147343,lr 1.0000000000000002e-06,grad 0.0014761514030396938\n",
      "pre_train epoch 15880,loss 0.00015779316890984774,lr 1.0000000000000002e-06,grad 0.0014164496678858995\n",
      "pre_train epoch 15900,loss 0.0001575923088239506,lr 1.0000000000000002e-06,grad 0.0014545585727319121\n",
      "pre_train epoch 15920,loss 0.00015741813695058227,lr 1.0000000000000002e-06,grad 0.00142551283352077\n",
      "pre_train epoch 15940,loss 0.00015723941032774746,lr 1.0000000000000002e-06,grad 0.0014930928591638803\n",
      "pre_train epoch 15960,loss 0.00015705768601037562,lr 1.0000000000000002e-06,grad 0.0015016179531812668\n",
      "pre_train epoch 15980,loss 0.00015686881670262665,lr 1.0000000000000002e-06,grad 0.0014044863637536764\n",
      "pre_train epoch 16000,loss 0.00015667894331272691,lr 1.0000000000000002e-06,grad 0.0015093536349013448\n",
      "pre_train epoch 16020,loss 0.00015649310080334544,lr 1.0000000000000002e-06,grad 0.0015010490315034986\n",
      "pre_train epoch 16040,loss 0.00015629202243871987,lr 1.0000000000000002e-06,grad 0.0014502506237477064\n",
      "pre_train epoch 16060,loss 0.00015608538524247706,lr 1.0000000000000002e-06,grad 0.0014326884411275387\n",
      "pre_train epoch 16080,loss 0.00015591905685141683,lr 1.0000000000000002e-06,grad 0.0015049767680466175\n",
      "pre_train epoch 16100,loss 0.00015570719551760703,lr 1.0000000000000002e-06,grad 0.001512420829385519\n",
      "pre_train epoch 16120,loss 0.00015551062824670225,lr 1.0000000000000002e-06,grad 0.001480985782109201\n",
      "pre_train epoch 16140,loss 0.00015530614473391324,lr 1.0000000000000002e-06,grad 0.0014331173151731491\n",
      "pre_train epoch 16160,loss 0.00015510902449022979,lr 1.0000000000000002e-06,grad 0.0015381249831989408\n",
      "pre_train epoch 16180,loss 0.00015490020450670272,lr 1.0000000000000002e-06,grad 0.0014804491074755788\n",
      "pre_train epoch 16200,loss 0.0001547040737932548,lr 1.0000000000000002e-06,grad 0.001422232948243618\n",
      "pre_train epoch 16220,loss 0.00015450568753294647,lr 1.0000000000000002e-06,grad 0.0014402837259694934\n",
      "pre_train epoch 16240,loss 0.00015429590712301433,lr 1.0000000000000002e-06,grad 0.001420210930518806\n",
      "pre_train epoch 16260,loss 0.00015408916806336492,lr 1.0000000000000002e-06,grad 0.0014203533064574003\n",
      "pre_train epoch 16280,loss 0.0001538879587315023,lr 1.0000000000000002e-06,grad 0.0013833529083058238\n",
      "pre_train epoch 16300,loss 0.00015366394654847682,lr 1.0000000000000002e-06,grad 0.0013565198751166463\n",
      "pre_train epoch 16320,loss 0.00015345420979429036,lr 1.0000000000000002e-06,grad 0.0014278693124651909\n",
      "pre_train epoch 16340,loss 0.00015326136781368405,lr 1.0000000000000002e-06,grad 0.0014110084157437086\n",
      "pre_train epoch 16360,loss 0.00015303796681109816,lr 1.0000000000000002e-06,grad 0.0013644443824887276\n",
      "pre_train epoch 16380,loss 0.00015281820378731936,lr 1.0000000000000002e-06,grad 0.001395965227857232\n",
      "pre_train epoch 16400,loss 0.00015258797793649137,lr 1.0000000000000002e-06,grad 0.0013564585242420435\n",
      "pre_train epoch 16420,loss 0.00015237829938996583,lr 1.0000000000000002e-06,grad 0.0014263320481404662\n",
      "pre_train epoch 16440,loss 0.00015215417079161853,lr 1.0000000000000002e-06,grad 0.0014407745329663157\n",
      "pre_train epoch 16460,loss 0.00015192881983239204,lr 1.0000000000000002e-06,grad 0.0013787035131826997\n",
      "pre_train epoch 16480,loss 0.00015170547703746706,lr 1.0000000000000002e-06,grad 0.0014647786738350987\n",
      "pre_train epoch 16500,loss 0.00015147622616495937,lr 1.0000000000000002e-06,grad 0.0014675174606963992\n",
      "pre_train epoch 16520,loss 0.00015123921912163496,lr 1.0000000000000002e-06,grad 0.0014238208532333374\n",
      "pre_train epoch 16540,loss 0.00015101789904292673,lr 1.0000000000000002e-06,grad 0.0013290583156049252\n",
      "pre_train epoch 16560,loss 0.00015077300486154854,lr 1.0000000000000002e-06,grad 0.001413289224728942\n",
      "pre_train epoch 16580,loss 0.00015055021503940225,lr 1.0000000000000002e-06,grad 0.0013698195107281208\n",
      "pre_train epoch 16600,loss 0.00015029721544124186,lr 1.0000000000000002e-06,grad 0.0015131332911550999\n",
      "pre_train epoch 16620,loss 0.00015007948968559504,lr 1.0000000000000002e-06,grad 0.0014638423454016447\n",
      "pre_train epoch 16640,loss 0.0001498219498898834,lr 1.0000000000000002e-06,grad 0.001541987294331193\n",
      "pre_train epoch 16660,loss 0.00014958722749724984,lr 1.0000000000000002e-06,grad 0.0014594201929867268\n",
      "pre_train epoch 16680,loss 0.00014935337821952999,lr 1.0000000000000002e-06,grad 0.0013322234153747559\n",
      "pre_train epoch 16700,loss 0.00014909905439708382,lr 1.0000000000000002e-06,grad 0.0013377388240769506\n",
      "pre_train epoch 16720,loss 0.00014885216660331935,lr 1.0000000000000002e-06,grad 0.0014346747193485498\n",
      "pre_train epoch 16740,loss 0.00014861179806757718,lr 1.0000000000000002e-06,grad 0.0013534380123019218\n",
      "pre_train epoch 16760,loss 0.00014836248010396957,lr 1.0000000000000002e-06,grad 0.0014816399198025465\n",
      "pre_train epoch 16780,loss 0.00014811432629358023,lr 1.0000000000000002e-06,grad 0.001408178242854774\n",
      "pre_train epoch 16800,loss 0.00014785573875997216,lr 1.0000000000000002e-06,grad 0.0013595300260931253\n",
      "pre_train epoch 16820,loss 0.00014760857447981834,lr 1.0000000000000002e-06,grad 0.0014179765712469816\n",
      "pre_train epoch 16840,loss 0.00014735219883732498,lr 1.0000000000000002e-06,grad 0.001354040578007698\n",
      "pre_train epoch 16860,loss 0.00014709979586768895,lr 1.0000000000000002e-06,grad 0.001279678544960916\n",
      "pre_train epoch 16880,loss 0.00014683786139357835,lr 1.0000000000000002e-06,grad 0.001219873083755374\n",
      "pre_train epoch 16900,loss 0.00014656889834441245,lr 1.0000000000000002e-06,grad 0.001269413041882217\n",
      "pre_train epoch 16920,loss 0.00014631179510615766,lr 1.0000000000000002e-06,grad 0.0014205373590812087\n",
      "pre_train epoch 16940,loss 0.00014605677279178053,lr 1.0000000000000002e-06,grad 0.0013346761697903275\n",
      "pre_train epoch 16960,loss 0.0001457871840102598,lr 1.0000000000000002e-06,grad 0.0014012381434440613\n",
      "pre_train epoch 16980,loss 0.00014551512140315026,lr 1.0000000000000002e-06,grad 0.0014531222404912114\n",
      "pre_train epoch 17000,loss 0.0001452453143429011,lr 1.0000000000000002e-06,grad 0.0014337999746203423\n",
      "pre_train epoch 17020,loss 0.00014498402015306056,lr 1.0000000000000002e-06,grad 0.0014034898485988379\n",
      "pre_train epoch 17040,loss 0.0001447029790142551,lr 1.0000000000000002e-06,grad 0.0014104289002716541\n",
      "pre_train epoch 17060,loss 0.000144422214361839,lr 1.0000000000000002e-06,grad 0.0012519832234829664\n",
      "pre_train epoch 17080,loss 0.0001441499771317467,lr 1.0000000000000002e-06,grad 0.0012824778677895665\n",
      "pre_train epoch 17100,loss 0.00014387072587851435,lr 1.0000000000000002e-06,grad 0.001383411232382059\n",
      "pre_train epoch 17120,loss 0.00014361145440489054,lr 1.0000000000000002e-06,grad 0.0013726006727665663\n",
      "pre_train epoch 17140,loss 0.00014331650163512677,lr 1.0000000000000002e-06,grad 0.0013682899298146367\n",
      "pre_train epoch 17160,loss 0.0001430341217201203,lr 1.0000000000000002e-06,grad 0.0012866825563833117\n",
      "pre_train epoch 17180,loss 0.00014274587738327682,lr 1.0000000000000002e-06,grad 0.001257506781257689\n",
      "pre_train epoch 17200,loss 0.00014246097998693585,lr 1.0000000000000002e-06,grad 0.0012444480089470744\n",
      "pre_train epoch 17220,loss 0.00014217864372767508,lr 1.0000000000000002e-06,grad 0.0013492797734215856\n",
      "pre_train epoch 17240,loss 0.00014189475041348487,lr 1.0000000000000002e-06,grad 0.0013819248415529728\n",
      "pre_train epoch 17260,loss 0.00014159100828692317,lr 1.0000000000000002e-06,grad 0.001388400443829596\n",
      "pre_train epoch 17280,loss 0.00014130417548585683,lr 1.0000000000000002e-06,grad 0.00122549245133996\n",
      "pre_train epoch 17300,loss 0.00014100653061177582,lr 1.0000000000000002e-06,grad 0.0012165454681962729\n",
      "pre_train epoch 17320,loss 0.00014071213081479073,lr 1.0000000000000002e-06,grad 0.001310785417445004\n",
      "pre_train epoch 17340,loss 0.0001404096110491082,lr 1.0000000000000002e-06,grad 0.0013619945384562016\n",
      "pre_train epoch 17360,loss 0.00014010285667609423,lr 1.0000000000000002e-06,grad 0.0012861923314630985\n",
      "pre_train epoch 17380,loss 0.00013980352377984673,lr 1.0000000000000002e-06,grad 0.0013447173405438662\n",
      "pre_train epoch 17400,loss 0.0001394958671880886,lr 1.0000000000000002e-06,grad 0.0013459081528708339\n",
      "pre_train epoch 17420,loss 0.00013919160119257867,lr 1.0000000000000002e-06,grad 0.0012125191278755665\n",
      "pre_train epoch 17440,loss 0.00013888368266634643,lr 1.0000000000000002e-06,grad 0.0014974131481721997\n",
      "pre_train epoch 17460,loss 0.00013857861631549895,lr 1.0000000000000002e-06,grad 0.0012224260717630386\n",
      "pre_train epoch 17480,loss 0.00013826599752064794,lr 1.0000000000000002e-06,grad 0.001433177967555821\n",
      "pre_train epoch 17500,loss 0.00013796018902212381,lr 1.0000000000000002e-06,grad 0.001320975017733872\n",
      "pre_train epoch 17520,loss 0.00013764119648840278,lr 1.0000000000000002e-06,grad 0.0012854508822783828\n",
      "pre_train epoch 17540,loss 0.00013732528896071017,lr 1.0000000000000002e-06,grad 0.0013199119130149484\n",
      "pre_train epoch 17560,loss 0.00013702154683414847,lr 1.0000000000000002e-06,grad 0.0011520511470735073\n",
      "pre_train epoch 17580,loss 0.0001366911455988884,lr 1.0000000000000002e-06,grad 0.0012580153997987509\n",
      "pre_train epoch 17600,loss 0.00013636887888424098,lr 1.0000000000000002e-06,grad 0.0012208393309265375\n",
      "pre_train epoch 17620,loss 0.00013603598927147686,lr 1.0000000000000002e-06,grad 0.0012023751623928547\n",
      "pre_train epoch 17640,loss 0.0001357174915028736,lr 1.0000000000000002e-06,grad 0.0012169346446171403\n",
      "pre_train epoch 17660,loss 0.00013538780331145972,lr 1.0000000000000002e-06,grad 0.0014444994740188122\n",
      "pre_train epoch 17680,loss 0.00013505949755199254,lr 1.0000000000000002e-06,grad 0.0012047754134982824\n",
      "pre_train epoch 17700,loss 0.00013472406135406345,lr 1.0000000000000002e-06,grad 0.0012176999589428306\n",
      "pre_train epoch 17720,loss 0.0001343967451248318,lr 1.0000000000000002e-06,grad 0.0012749572051689029\n",
      "pre_train epoch 17740,loss 0.000134078276460059,lr 1.0000000000000002e-06,grad 0.0011398268397897482\n",
      "pre_train epoch 17760,loss 0.00013372948160395026,lr 1.0000000000000002e-06,grad 0.0012935834238305688\n",
      "pre_train epoch 17780,loss 0.00013339512224774808,lr 1.0000000000000002e-06,grad 0.0011368737323209643\n",
      "pre_train epoch 17800,loss 0.00013306182518135756,lr 1.0000000000000002e-06,grad 0.0012098833685740829\n",
      "pre_train epoch 17820,loss 0.00013270886847749352,lr 1.0000000000000002e-06,grad 0.001173560624010861\n",
      "pre_train epoch 17840,loss 0.0001323659671470523,lr 1.0000000000000002e-06,grad 0.0011534428922459483\n",
      "pre_train epoch 17860,loss 0.00013203347043599933,lr 1.0000000000000002e-06,grad 0.001137742307037115\n",
      "pre_train epoch 17880,loss 0.0001316763664362952,lr 1.0000000000000002e-06,grad 0.001173621159978211\n",
      "pre_train epoch 17900,loss 0.00013133249012753367,lr 1.0000000000000002e-06,grad 0.0012850825442001224\n",
      "pre_train epoch 17920,loss 0.00013097540067974478,lr 1.0000000000000002e-06,grad 0.0012603008653968573\n",
      "pre_train epoch 17940,loss 0.0001306435588048771,lr 1.0000000000000002e-06,grad 0.0013729467755183578\n",
      "pre_train epoch 17960,loss 0.00013027986278757453,lr 1.0000000000000002e-06,grad 0.0011796416947618127\n",
      "pre_train epoch 17980,loss 0.0001299297873629257,lr 1.0000000000000002e-06,grad 0.001282535376958549\n",
      "pre_train epoch 18000,loss 0.00012956236605532467,lr 1.0000000000000002e-06,grad 0.001189599046483636\n",
      "pre_train epoch 18020,loss 0.0001292095403186977,lr 1.0000000000000002e-06,grad 0.0010573914041742682\n",
      "pre_train epoch 18040,loss 0.00012885534670203924,lr 1.0000000000000002e-06,grad 0.001141386921517551\n",
      "pre_train epoch 18060,loss 0.00012848895858041942,lr 1.0000000000000002e-06,grad 0.001159832812845707\n",
      "pre_train epoch 18080,loss 0.0001281209843000397,lr 1.0000000000000002e-06,grad 0.0010404783533886075\n",
      "pre_train epoch 18100,loss 0.0001277562405448407,lr 1.0000000000000002e-06,grad 0.001163863344117999\n",
      "pre_train epoch 18120,loss 0.0001273944362765178,lr 1.0000000000000002e-06,grad 0.0011971199419349432\n",
      "pre_train epoch 18140,loss 0.00012703148240689188,lr 1.0000000000000002e-06,grad 0.0010364578338339925\n",
      "pre_train epoch 18160,loss 0.00012665528629440814,lr 1.0000000000000002e-06,grad 0.0010977856582030654\n",
      "pre_train epoch 18180,loss 0.0001263022713828832,lr 1.0000000000000002e-06,grad 0.0011895024217665195\n",
      "pre_train epoch 18200,loss 0.0001259100972674787,lr 1.0000000000000002e-06,grad 0.0011522993445396423\n",
      "pre_train epoch 18220,loss 0.00012554670684039593,lr 1.0000000000000002e-06,grad 0.0011077788658440113\n",
      "pre_train epoch 18240,loss 0.0001251664652954787,lr 1.0000000000000002e-06,grad 0.0011425012489780784\n",
      "pre_train epoch 18260,loss 0.00012478072312660515,lr 1.0000000000000002e-06,grad 0.001108874916099012\n",
      "pre_train epoch 18280,loss 0.00012439890997484326,lr 1.0000000000000002e-06,grad 0.0010649958858266473\n",
      "pre_train epoch 18300,loss 0.00012401652929838747,lr 1.0000000000000002e-06,grad 0.0011429626028984785\n",
      "pre_train epoch 18320,loss 0.0001236398529727012,lr 1.0000000000000002e-06,grad 0.0011666315840557218\n",
      "pre_train epoch 18340,loss 0.00012325817078817636,lr 1.0000000000000002e-06,grad 0.001080797635950148\n",
      "pre_train epoch 18360,loss 0.00012286528362892568,lr 1.0000000000000002e-06,grad 0.0010559245711192489\n",
      "pre_train epoch 18380,loss 0.00012246935511939228,lr 1.0000000000000002e-06,grad 0.0010873122373595834\n",
      "pre_train epoch 18400,loss 0.0001220962731167674,lr 1.0000000000000002e-06,grad 0.0010768555803224444\n",
      "pre_train epoch 18420,loss 0.00012170444097137079,lr 1.0000000000000002e-06,grad 0.00103579496499151\n",
      "pre_train epoch 18440,loss 0.00012131258699810132,lr 1.0000000000000002e-06,grad 0.0013418124290183187\n",
      "pre_train epoch 18460,loss 0.00012091874668840319,lr 1.0000000000000002e-06,grad 0.001258973265066743\n",
      "pre_train epoch 18480,loss 0.00012052318197675049,lr 1.0000000000000002e-06,grad 0.001292779459618032\n",
      "pre_train epoch 18500,loss 0.00012013401283184066,lr 1.0000000000000002e-06,grad 0.0010467410320416093\n",
      "pre_train epoch 18520,loss 0.0001197289748233743,lr 1.0000000000000002e-06,grad 0.0013277839170768857\n",
      "pre_train epoch 18540,loss 0.0001193341231555678,lr 1.0000000000000002e-06,grad 0.0012614050647243857\n",
      "pre_train epoch 18560,loss 0.00011893593182321638,lr 1.0000000000000002e-06,grad 0.001327643753029406\n",
      "pre_train epoch 18580,loss 0.00011852248280774802,lr 1.0000000000000002e-06,grad 0.001006276230327785\n",
      "pre_train epoch 18600,loss 0.00011812488082796335,lr 1.0000000000000002e-06,grad 0.0009897655108943582\n",
      "pre_train epoch 18620,loss 0.00011772366269724444,lr 1.0000000000000002e-06,grad 0.0009402856230735779\n",
      "pre_train epoch 18640,loss 0.00011732246639439836,lr 1.0000000000000002e-06,grad 0.0009598893229849637\n",
      "pre_train epoch 18660,loss 0.00011690754763549194,lr 1.0000000000000002e-06,grad 0.0009940143208950758\n",
      "pre_train epoch 18680,loss 0.00011652325338218361,lr 1.0000000000000002e-06,grad 0.0010828309459611773\n",
      "pre_train epoch 18700,loss 0.00011610204091994092,lr 1.0000000000000002e-06,grad 0.0009488810319453478\n",
      "pre_train epoch 18720,loss 0.00011568720219656825,lr 1.0000000000000002e-06,grad 0.0012921553570777178\n",
      "pre_train epoch 18740,loss 0.00011527081369422376,lr 1.0000000000000002e-06,grad 0.001146579161286354\n",
      "pre_train epoch 18760,loss 0.00011486664152471349,lr 1.0000000000000002e-06,grad 0.0011591340880841017\n",
      "pre_train epoch 18780,loss 0.00011444315896369517,lr 1.0000000000000002e-06,grad 0.0011932236375287175\n",
      "pre_train epoch 18800,loss 0.0001140553576988168,lr 1.0000000000000002e-06,grad 0.0011534022632986307\n",
      "pre_train epoch 18820,loss 0.00011361760698491707,lr 1.0000000000000002e-06,grad 0.00112757901661098\n",
      "pre_train epoch 18840,loss 0.00011320613702991977,lr 1.0000000000000002e-06,grad 0.0009246931876987219\n",
      "pre_train epoch 18860,loss 0.0001127808282035403,lr 1.0000000000000002e-06,grad 0.001093214494176209\n",
      "pre_train epoch 18880,loss 0.00011236783757340163,lr 1.0000000000000002e-06,grad 0.0011791037395596504\n",
      "pre_train epoch 18900,loss 0.00011193842510692775,lr 1.0000000000000002e-06,grad 0.0009830235503613949\n",
      "pre_train epoch 18920,loss 0.00011150932550663128,lr 1.0000000000000002e-06,grad 0.001164361136034131\n",
      "pre_train epoch 18940,loss 0.00011108843318652362,lr 1.0000000000000002e-06,grad 0.0009641846991144121\n",
      "pre_train epoch 18960,loss 0.00011066583829233423,lr 1.0000000000000002e-06,grad 0.0009549548267386854\n",
      "pre_train epoch 18980,loss 0.0001102265014196746,lr 1.0000000000000002e-06,grad 0.0011940799886360765\n",
      "pre_train epoch 19000,loss 0.00010981078958138824,lr 1.0000000000000002e-06,grad 0.0009211611468344927\n",
      "pre_train epoch 19020,loss 0.00010938444756902754,lr 1.0000000000000002e-06,grad 0.001002528821118176\n",
      "pre_train epoch 19040,loss 0.00010896274034166709,lr 1.0000000000000002e-06,grad 0.001203399384394288\n",
      "pre_train epoch 19060,loss 0.00010852834384422749,lr 1.0000000000000002e-06,grad 0.0013358361320570111\n",
      "pre_train epoch 19080,loss 0.00010810529784066603,lr 1.0000000000000002e-06,grad 0.0008932115160860121\n",
      "pre_train epoch 19100,loss 0.00010766938066808507,lr 1.0000000000000002e-06,grad 0.0008869781740941107\n",
      "pre_train epoch 19120,loss 0.0001072353043127805,lr 1.0000000000000002e-06,grad 0.0016041351482272148\n",
      "pre_train epoch 19140,loss 0.00010681869753170758,lr 1.0000000000000002e-06,grad 0.0008746917592361569\n",
      "pre_train epoch 19160,loss 0.00010638241656124592,lr 1.0000000000000002e-06,grad 0.0009286104468628764\n",
      "pre_train epoch 19180,loss 0.00010594566265353933,lr 1.0000000000000002e-06,grad 0.0011895372299477458\n",
      "pre_train epoch 19200,loss 0.00010551047307671979,lr 1.0000000000000002e-06,grad 0.0008504441939294338\n",
      "pre_train epoch 19220,loss 0.00010508082050364465,lr 1.0000000000000002e-06,grad 0.0011083923745900393\n",
      "pre_train epoch 19240,loss 0.00010463609942235053,lr 1.0000000000000002e-06,grad 0.0011441016104072332\n",
      "pre_train epoch 19260,loss 0.00010421083425171673,lr 1.0000000000000002e-06,grad 0.0008422582177445292\n",
      "pre_train epoch 19280,loss 0.00010377116996096447,lr 1.0000000000000002e-06,grad 0.0008730195113457739\n",
      "pre_train epoch 19300,loss 0.00010333101090509444,lr 1.0000000000000002e-06,grad 0.0009821571875363588\n",
      "pre_train epoch 19320,loss 0.00010290060163242742,lr 1.0000000000000002e-06,grad 0.0008639777661301196\n",
      "pre_train epoch 19340,loss 0.00010245363955618814,lr 1.0000000000000002e-06,grad 0.0009997052839025855\n",
      "pre_train epoch 19360,loss 0.00010202112753177062,lr 1.0000000000000002e-06,grad 0.0009036542032845318\n",
      "pre_train epoch 19380,loss 0.00010158002260141075,lr 1.0000000000000002e-06,grad 0.0010191762121394277\n",
      "pre_train epoch 19400,loss 0.00010113485768670216,lr 1.0000000000000002e-06,grad 0.0008373137097805738\n",
      "pre_train epoch 19420,loss 0.00010069918789668009,lr 1.0000000000000002e-06,grad 0.0008449904271401465\n",
      "pre_train epoch 19440,loss 0.0001002571007120423,lr 1.0000000000000002e-06,grad 0.0011877162614837289\n",
      "pre_train epoch 19460,loss 9.98250616248697e-05,lr 1.0000000000000002e-06,grad 0.0008247492951340973\n",
      "pre_train epoch 19480,loss 9.937885624822229e-05,lr 1.0000000000000002e-06,grad 0.00128420430701226\n",
      "pre_train epoch 19500,loss 9.893663809634745e-05,lr 1.0000000000000002e-06,grad 0.000926876615267247\n",
      "pre_train epoch 19520,loss 9.85025180852972e-05,lr 1.0000000000000002e-06,grad 0.0012235871981829405\n",
      "pre_train epoch 19540,loss 9.804464934859425e-05,lr 1.0000000000000002e-06,grad 0.0010935486061498523\n",
      "pre_train epoch 19560,loss 9.760695684235543e-05,lr 1.0000000000000002e-06,grad 0.0007706202450208366\n",
      "pre_train epoch 19580,loss 9.71682820818387e-05,lr 1.0000000000000002e-06,grad 0.0011415258049964905\n",
      "pre_train epoch 19600,loss 9.671800216892734e-05,lr 1.0000000000000002e-06,grad 0.0009841604623943567\n",
      "pre_train epoch 19620,loss 9.62877384154126e-05,lr 1.0000000000000002e-06,grad 0.0014645770424976945\n",
      "pre_train epoch 19640,loss 9.583029168425128e-05,lr 1.0000000000000002e-06,grad 0.0014668868388980627\n",
      "pre_train epoch 19660,loss 9.537801088299602e-05,lr 1.0000000000000002e-06,grad 0.0008144468883983791\n",
      "pre_train epoch 19680,loss 9.494322148384526e-05,lr 1.0000000000000002e-06,grad 0.0007592535694129765\n",
      "pre_train epoch 19700,loss 9.450173092773184e-05,lr 1.0000000000000002e-06,grad 0.0011957654496654868\n",
      "pre_train epoch 19720,loss 9.405688615515828e-05,lr 1.0000000000000002e-06,grad 0.001064830576069653\n",
      "pre_train epoch 19740,loss 9.360591502627358e-05,lr 1.0000000000000002e-06,grad 0.001028148690238595\n",
      "pre_train epoch 19760,loss 9.315382339991629e-05,lr 1.0000000000000002e-06,grad 0.0012615968007594347\n",
      "pre_train epoch 19780,loss 9.27140936255455e-05,lr 1.0000000000000002e-06,grad 0.0031638278160244226\n",
      "pre_train epoch 19800,loss 9.226701513398439e-05,lr 1.0000000000000002e-06,grad 0.0014849232975393534\n",
      "pre_train epoch 19820,loss 9.182104258798063e-05,lr 1.0000000000000002e-06,grad 0.0015863354783505201\n",
      "pre_train epoch 19840,loss 9.136295557254925e-05,lr 1.0000000000000002e-06,grad 0.0008142688893713057\n",
      "pre_train epoch 19860,loss 9.093020344153047e-05,lr 1.0000000000000002e-06,grad 0.0017255039419978857\n",
      "pre_train epoch 19880,loss 9.04859698493965e-05,lr 1.0000000000000002e-06,grad 0.002067657420411706\n",
      "pre_train epoch 19900,loss 9.002276055980474e-05,lr 1.0000000000000002e-06,grad 0.002810579724609852\n",
      "pre_train epoch 19920,loss 8.959237311501056e-05,lr 1.0000000000000002e-06,grad 0.0035922250244766474\n",
      "pre_train epoch 19940,loss 8.914203499443829e-05,lr 1.0000000000000002e-06,grad 0.006776576396077871\n",
      "pre_train epoch 19960,loss 8.870103192748502e-05,lr 1.0000000000000002e-06,grad 0.007315594237297773\n",
      "pre_train epoch 19980,loss 8.842325041769072e-05,lr 1.0000000000000002e-06,grad 0.06298883259296417\n",
      "pre_train epoch 20000,loss 8.796131442068145e-05,lr 1.0000000000000002e-06,grad 0.05831241235136986\n",
      "pre_train epoch 20020,loss 8.739819895708933e-05,lr 1.0000000000000002e-06,grad 0.0009318634984083474\n",
      "pre_train epoch 20040,loss 8.698098099557683e-05,lr 1.0000000000000002e-06,grad 0.006232127547264099\n",
      "pre_train epoch 20060,loss 8.655941201141104e-05,lr 1.0000000000000002e-06,grad 0.0028995105531066656\n",
      "pre_train epoch 20080,loss 8.61418666318059e-05,lr 1.0000000000000002e-06,grad 0.0014856376219540834\n",
      "pre_train epoch 20100,loss 8.573212835472077e-05,lr 1.0000000000000002e-06,grad 0.0013244551373645663\n",
      "pre_train epoch 20120,loss 8.531828643754125e-05,lr 1.0000000000000002e-06,grad 0.0016824064077809453\n",
      "pre_train epoch 20140,loss 8.488894673064351e-05,lr 1.0000000000000002e-06,grad 0.0022774236276745796\n",
      "pre_train epoch 20160,loss 8.447888103546575e-05,lr 1.0000000000000002e-06,grad 0.0032572541385889053\n",
      "pre_train epoch 20180,loss 8.405824337387457e-05,lr 1.0000000000000002e-06,grad 0.0035141464322805405\n",
      "pre_train epoch 20200,loss 8.368535054614767e-05,lr 1.0000000000000002e-06,grad 0.033275626599788666\n",
      "pre_train epoch 20220,loss 8.324920781888068e-05,lr 1.0000000000000002e-06,grad 0.020098410546779633\n",
      "pre_train epoch 20240,loss 8.28495976747945e-05,lr 1.0000000000000002e-06,grad 0.019549472257494926\n",
      "pre_train epoch 20260,loss 8.243990305345505e-05,lr 1.0000000000000002e-06,grad 0.006162839010357857\n",
      "pre_train epoch 20280,loss 8.20434361230582e-05,lr 1.0000000000000002e-06,grad 0.0010847147786989808\n",
      "pre_train epoch 20300,loss 8.165246981661767e-05,lr 1.0000000000000002e-06,grad 0.0028840594459325075\n",
      "pre_train epoch 20320,loss 8.124315354507416e-05,lr 1.0000000000000002e-06,grad 0.000814976345282048\n",
      "pre_train epoch 20340,loss 8.085727313300595e-05,lr 1.0000000000000002e-06,grad 0.0006659005302935839\n",
      "pre_train epoch 20360,loss 8.045427239267156e-05,lr 1.0000000000000002e-06,grad 0.0006708527798764408\n",
      "pre_train epoch 20380,loss 8.006092684809119e-05,lr 1.0000000000000002e-06,grad 0.006224284414201975\n",
      "pre_train epoch 20400,loss 7.983105024322867e-05,lr 1.0000000000000002e-06,grad 0.06316960602998734\n",
      "pre_train epoch 20420,loss 7.938213093439117e-05,lr 1.0000000000000002e-06,grad 0.04660910367965698\n",
      "pre_train epoch 20440,loss 7.893703150330111e-05,lr 1.0000000000000002e-06,grad 0.010360317304730415\n",
      "pre_train epoch 20460,loss 7.856003503547981e-05,lr 1.0000000000000002e-06,grad 0.0015778244705870748\n",
      "pre_train epoch 20480,loss 7.819936581654474e-05,lr 1.0000000000000002e-06,grad 0.001834794646129012\n",
      "pre_train epoch 20500,loss 7.783646287862211e-05,lr 1.0000000000000002e-06,grad 0.0020157236140221357\n",
      "pre_train epoch 20520,loss 7.746273331576958e-05,lr 1.0000000000000002e-06,grad 0.0006438361015170813\n",
      "pre_train epoch 20540,loss 7.709278725087643e-05,lr 1.0000000000000002e-06,grad 0.0025225987192243338\n",
      "pre_train epoch 20560,loss 7.672991341678426e-05,lr 1.0000000000000002e-06,grad 0.0012366732116788626\n",
      "pre_train epoch 20580,loss 7.637041562702507e-05,lr 1.0000000000000002e-06,grad 0.0019413819536566734\n",
      "pre_train epoch 20600,loss 7.600177923450246e-05,lr 1.0000000000000002e-06,grad 0.0037405332550406456\n",
      "pre_train epoch 20620,loss 7.564143015770242e-05,lr 1.0000000000000002e-06,grad 0.00529929855838418\n",
      "pre_train epoch 20640,loss 7.527898560510948e-05,lr 1.0000000000000002e-06,grad 0.0049364520236849785\n",
      "pre_train epoch 20660,loss 7.491255382774398e-05,lr 1.0000000000000002e-06,grad 0.01923607848584652\n",
      "pre_train epoch 20680,loss 7.457382162101567e-05,lr 1.0000000000000002e-06,grad 0.022714246064424515\n",
      "pre_train epoch 20700,loss 7.425920193782076e-05,lr 1.0000000000000002e-06,grad 0.031881123781204224\n",
      "pre_train epoch 20720,loss 7.387460937025025e-05,lr 1.0000000000000002e-06,grad 0.004666591994464397\n",
      "pre_train epoch 20740,loss 7.354786794167012e-05,lr 1.0000000000000002e-06,grad 0.004765867721289396\n",
      "pre_train epoch 20760,loss 7.319997530430555e-05,lr 1.0000000000000002e-06,grad 0.0010626664152368903\n",
      "pre_train epoch 20780,loss 7.285692117875442e-05,lr 1.0000000000000002e-06,grad 0.000671316753141582\n",
      "pre_train epoch 20800,loss 7.251901115523651e-05,lr 1.0000000000000002e-06,grad 0.001024494762532413\n",
      "pre_train epoch 20820,loss 7.218372047645971e-05,lr 1.0000000000000002e-06,grad 0.000671634916216135\n",
      "pre_train epoch 20840,loss 7.18449373380281e-05,lr 1.0000000000000002e-06,grad 0.0011775561142712831\n",
      "pre_train epoch 20860,loss 7.150050078053027e-05,lr 1.0000000000000002e-06,grad 0.0021790468599647284\n",
      "pre_train epoch 20880,loss 7.116312917787582e-05,lr 1.0000000000000002e-06,grad 0.004878009669482708\n",
      "pre_train epoch 20900,loss 7.083618402248248e-05,lr 1.0000000000000002e-06,grad 0.005931501742452383\n",
      "pre_train epoch 20920,loss 7.068015838740394e-05,lr 1.0000000000000002e-06,grad 0.06575106829404831\n",
      "pre_train epoch 20940,loss 7.034545706119388e-05,lr 1.0000000000000002e-06,grad 0.0631202980875969\n",
      "pre_train epoch 20960,loss 6.985344953136519e-05,lr 1.0000000000000002e-06,grad 0.004038986749947071\n",
      "pre_train epoch 20980,loss 6.954158016014844e-05,lr 1.0000000000000002e-06,grad 0.004958089906722307\n",
      "pre_train epoch 21000,loss 6.922067404957488e-05,lr 1.0000000000000002e-06,grad 0.003962376154959202\n",
      "pre_train epoch 21020,loss 6.8902678322047e-05,lr 1.0000000000000002e-06,grad 0.0015135640278458595\n",
      "pre_train epoch 21040,loss 6.858831329736859e-05,lr 1.0000000000000002e-06,grad 0.000760650378651917\n",
      "pre_train epoch 21060,loss 6.826988101238385e-05,lr 1.0000000000000002e-06,grad 0.0005891204928047955\n",
      "pre_train epoch 21080,loss 6.795704393880442e-05,lr 1.0000000000000002e-06,grad 0.0020019523799419403\n",
      "pre_train epoch 21100,loss 6.764449790352955e-05,lr 1.0000000000000002e-06,grad 0.0006608887342736125\n",
      "pre_train epoch 21120,loss 6.732719339197502e-05,lr 1.0000000000000002e-06,grad 0.00920112058520317\n",
      "pre_train epoch 21140,loss 6.729180313413963e-05,lr 1.0000000000000002e-06,grad 0.08086695522069931\n",
      "pre_train epoch 21160,loss 6.670886068604887e-05,lr 1.0000000000000002e-06,grad 0.0029627506155520678\n",
      "pre_train epoch 21180,loss 6.640922219958156e-05,lr 1.0000000000000002e-06,grad 0.007240112870931625\n",
      "pre_train epoch 21200,loss 6.610952550545335e-05,lr 1.0000000000000002e-06,grad 0.002497216919437051\n",
      "pre_train epoch 21220,loss 6.579812179552391e-05,lr 1.0000000000000002e-06,grad 0.0005373817402869463\n",
      "pre_train epoch 21240,loss 6.550249236170202e-05,lr 1.0000000000000002e-06,grad 0.002383392071351409\n",
      "pre_train epoch 21260,loss 6.520430906675756e-05,lr 1.0000000000000002e-06,grad 0.0028669291641563177\n",
      "pre_train epoch 21280,loss 6.48995119263418e-05,lr 1.0000000000000002e-06,grad 0.011217737570405006\n",
      "pre_train epoch 21300,loss 6.507163197966293e-05,lr 1.0000000000000002e-06,grad 0.10403244942426682\n",
      "pre_train epoch 21320,loss 6.432103691622615e-05,lr 1.0000000000000002e-06,grad 0.001633644918911159\n",
      "pre_train epoch 21340,loss 6.40379439573735e-05,lr 1.0000000000000002e-06,grad 0.011985204182565212\n",
      "pre_train epoch 21360,loss 6.375858356477693e-05,lr 1.0000000000000002e-06,grad 0.001057119108736515\n",
      "pre_train epoch 21380,loss 6.347582529997453e-05,lr 1.0000000000000002e-06,grad 0.0026639255229383707\n",
      "pre_train epoch 21400,loss 6.32034134468995e-05,lr 1.0000000000000002e-06,grad 0.0005423289258033037\n",
      "pre_train epoch 21420,loss 6.291118188528344e-05,lr 1.0000000000000002e-06,grad 0.0013429013779386878\n",
      "pre_train epoch 21440,loss 6.263381510507315e-05,lr 1.0000000000000002e-06,grad 0.0013725758763030171\n",
      "pre_train epoch 21460,loss 6.23603627900593e-05,lr 1.0000000000000002e-06,grad 0.0008341866778209805\n",
      "pre_train epoch 21480,loss 6.206454418133944e-05,lr 1.0000000000000002e-06,grad 0.0005146988551132381\n",
      "pre_train epoch 21500,loss 6.178719195304438e-05,lr 1.0000000000000002e-06,grad 0.0006408810731954873\n",
      "pre_train epoch 21520,loss 6.151090929051861e-05,lr 1.0000000000000002e-06,grad 0.000578631239477545\n",
      "pre_train epoch 21540,loss 6.123276398284361e-05,lr 1.0000000000000002e-06,grad 0.0029023373499512672\n",
      "pre_train epoch 21560,loss 6.210076389834285e-05,lr 1.0000000000000002e-06,grad 0.16246551275253296\n",
      "pre_train epoch 21580,loss 6.0766320530092344e-05,lr 1.0000000000000002e-06,grad 0.03986845538020134\n",
      "pre_train epoch 21600,loss 6.0445774579420686e-05,lr 1.0000000000000002e-06,grad 0.011964905075728893\n",
      "pre_train epoch 21620,loss 6.0203401517355815e-05,lr 1.0000000000000002e-06,grad 0.010058954358100891\n",
      "pre_train epoch 21640,loss 5.995099854771979e-05,lr 1.0000000000000002e-06,grad 0.0007771456148475409\n",
      "pre_train epoch 21660,loss 5.97111284150742e-05,lr 1.0000000000000002e-06,grad 0.0015091218519955873\n",
      "pre_train epoch 21680,loss 5.9464808146003634e-05,lr 1.0000000000000002e-06,grad 0.0007690838538110256\n",
      "pre_train epoch 21700,loss 5.921659976593219e-05,lr 1.0000000000000002e-06,grad 0.0005045855068601668\n",
      "pre_train epoch 21720,loss 5.896428046980873e-05,lr 1.0000000000000002e-06,grad 0.0009526236681267619\n",
      "pre_train epoch 21740,loss 5.8716956118587404e-05,lr 1.0000000000000002e-06,grad 0.0015347555745393038\n",
      "pre_train epoch 21760,loss 5.847137072123587e-05,lr 1.0000000000000002e-06,grad 0.000596817466430366\n",
      "pre_train epoch 21780,loss 5.823107130709104e-05,lr 1.0000000000000002e-06,grad 0.0004956063348799944\n",
      "pre_train epoch 21800,loss 5.797252379124984e-05,lr 1.0000000000000002e-06,grad 0.0006381822749972343\n",
      "pre_train epoch 21820,loss 5.772673466708511e-05,lr 1.0000000000000002e-06,grad 0.0006293460028246045\n",
      "pre_train epoch 21840,loss 5.747535760747269e-05,lr 1.0000000000000002e-06,grad 0.0013314132811501622\n",
      "pre_train epoch 21860,loss 5.722798960050568e-05,lr 1.0000000000000002e-06,grad 0.000639774720184505\n",
      "pre_train epoch 21880,loss 5.697929373127408e-05,lr 1.0000000000000002e-06,grad 0.0010056422324851155\n",
      "pre_train epoch 21900,loss 5.672882980434224e-05,lr 1.0000000000000002e-06,grad 0.0007124951807782054\n",
      "pre_train epoch 21920,loss 5.647803845931776e-05,lr 1.0000000000000002e-06,grad 0.0020045165438205004\n",
      "pre_train epoch 21940,loss 5.7468205341137946e-05,lr 1.0000000000000002e-06,grad 0.16859404742717743\n",
      "pre_train epoch 21960,loss 5.6063694501062855e-05,lr 1.0000000000000002e-06,grad 0.039807941764593124\n",
      "pre_train epoch 21980,loss 5.5786058510420844e-05,lr 1.0000000000000002e-06,grad 0.01991860195994377\n",
      "pre_train epoch 22000,loss 5.5553478887304664e-05,lr 1.0000000000000002e-06,grad 0.00295753451064229\n",
      "pre_train epoch 22020,loss 5.5320721003226936e-05,lr 1.0000000000000002e-06,grad 0.002059012185782194\n",
      "pre_train epoch 22040,loss 5.5091568356147036e-05,lr 1.0000000000000002e-06,grad 0.0008026740397326648\n",
      "pre_train epoch 22060,loss 5.485980000230484e-05,lr 1.0000000000000002e-06,grad 0.0011650529922917485\n",
      "pre_train epoch 22080,loss 5.4629224905511364e-05,lr 1.0000000000000002e-06,grad 0.0012826694874092937\n",
      "pre_train epoch 22100,loss 5.439677624963224e-05,lr 1.0000000000000002e-06,grad 0.001904300763271749\n",
      "pre_train epoch 22120,loss 5.417480133473873e-05,lr 1.0000000000000002e-06,grad 0.000961864076089114\n",
      "pre_train epoch 22140,loss 5.394541221903637e-05,lr 1.0000000000000002e-06,grad 0.0006917199934832752\n",
      "pre_train epoch 22160,loss 5.371225779526867e-05,lr 1.0000000000000002e-06,grad 0.0016098642954602838\n",
      "pre_train epoch 22180,loss 5.3487201512325555e-05,lr 1.0000000000000002e-06,grad 0.005788126494735479\n",
      "pre_train epoch 22200,loss 5.400855661719106e-05,lr 1.0000000000000002e-06,grad 0.1310758739709854\n",
      "pre_train epoch 22220,loss 5.306597813614644e-05,lr 1.0000000000000002e-06,grad 0.02775360457599163\n",
      "pre_train epoch 22240,loss 5.281774065224454e-05,lr 1.0000000000000002e-06,grad 0.0057329414412379265\n",
      "pre_train epoch 22260,loss 5.260096077108756e-05,lr 1.0000000000000002e-06,grad 0.0019605036359280348\n",
      "pre_train epoch 22280,loss 5.2386960305739194e-05,lr 1.0000000000000002e-06,grad 0.0031087284442037344\n",
      "pre_train epoch 22300,loss 5.217292709858157e-05,lr 1.0000000000000002e-06,grad 0.003231259062886238\n",
      "pre_train epoch 22320,loss 5.195654011913575e-05,lr 1.0000000000000002e-06,grad 0.0024599044118076563\n",
      "pre_train epoch 22340,loss 5.1742081268457696e-05,lr 1.0000000000000002e-06,grad 0.0015373729402199388\n",
      "pre_train epoch 22360,loss 5.151758523425087e-05,lr 1.0000000000000002e-06,grad 0.0018660419154912233\n",
      "pre_train epoch 22380,loss 5.132978185429238e-05,lr 1.0000000000000002e-06,grad 0.027754953131079674\n",
      "pre_train epoch 22400,loss 5.109708581585437e-05,lr 1.0000000000000002e-06,grad 0.011401799507439137\n",
      "pre_train epoch 22420,loss 5.09200690430589e-05,lr 1.0000000000000002e-06,grad 0.02664174884557724\n",
      "pre_train epoch 22440,loss 5.0692728109424934e-05,lr 1.0000000000000002e-06,grad 0.009075441397726536\n",
      "pre_train epoch 22460,loss 5.048789898864925e-05,lr 1.0000000000000002e-06,grad 0.0005037908558733761\n",
      "pre_train epoch 22480,loss 5.028700616094284e-05,lr 1.0000000000000002e-06,grad 0.0030618777964264154\n",
      "pre_train epoch 22500,loss 5.008722291677259e-05,lr 1.0000000000000002e-06,grad 0.00043174627353437245\n",
      "pre_train epoch 22520,loss 4.98840345244389e-05,lr 1.0000000000000002e-06,grad 0.003261936130002141\n",
      "pre_train epoch 22540,loss 4.967734275851399e-05,lr 1.0000000000000002e-06,grad 0.0005376515910029411\n",
      "pre_train epoch 22560,loss 4.947618072037585e-05,lr 1.0000000000000002e-06,grad 0.0023582030553370714\n",
      "pre_train epoch 22580,loss 4.9270729505224153e-05,lr 1.0000000000000002e-06,grad 0.002854416612535715\n",
      "pre_train epoch 22600,loss 4.907203765469603e-05,lr 1.0000000000000002e-06,grad 0.0071237958036363125\n",
      "pre_train epoch 22620,loss 4.91510727442801e-05,lr 1.0000000000000002e-06,grad 0.0807710662484169\n",
      "pre_train epoch 22640,loss 4.874084334005602e-05,lr 1.0000000000000002e-06,grad 0.03823651373386383\n",
      "pre_train epoch 22660,loss 4.849298784392886e-05,lr 1.0000000000000002e-06,grad 0.014827663078904152\n",
      "pre_train epoch 22680,loss 4.8294310545315966e-05,lr 1.0000000000000002e-06,grad 0.006547731813043356\n",
      "pre_train epoch 22700,loss 4.810344034922309e-05,lr 1.0000000000000002e-06,grad 0.0012161709601059556\n",
      "pre_train epoch 22720,loss 4.791518585989252e-05,lr 1.0000000000000002e-06,grad 0.001697733299806714\n",
      "pre_train epoch 22740,loss 4.772133252117783e-05,lr 1.0000000000000002e-06,grad 0.0012412788346409798\n",
      "pre_train epoch 22760,loss 4.753477332997136e-05,lr 1.0000000000000002e-06,grad 0.0015469881473109126\n",
      "pre_train epoch 22780,loss 4.733666719403118e-05,lr 1.0000000000000002e-06,grad 0.002859250409528613\n",
      "pre_train epoch 22800,loss 4.716155308415182e-05,lr 1.0000000000000002e-06,grad 0.018625061959028244\n",
      "pre_train epoch 22820,loss 4.7265421017073095e-05,lr 1.0000000000000002e-06,grad 0.08332059532403946\n",
      "pre_train epoch 22840,loss 4.680404526880011e-05,lr 1.0000000000000002e-06,grad 0.02450614422559738\n",
      "pre_train epoch 22860,loss 4.660414924728684e-05,lr 1.0000000000000002e-06,grad 0.004561667796224356\n",
      "pre_train epoch 22880,loss 4.6418546844506636e-05,lr 1.0000000000000002e-06,grad 0.0006685472326353192\n",
      "pre_train epoch 22900,loss 4.6239783841883764e-05,lr 1.0000000000000002e-06,grad 0.0009920595912262797\n",
      "pre_train epoch 22920,loss 4.6054938138695434e-05,lr 1.0000000000000002e-06,grad 0.0021716917399317026\n",
      "pre_train epoch 22940,loss 4.5882512495154515e-05,lr 1.0000000000000002e-06,grad 0.0014494169736281037\n",
      "pre_train epoch 22960,loss 4.569740121951327e-05,lr 1.0000000000000002e-06,grad 0.0003931281971745193\n",
      "pre_train epoch 22980,loss 4.551675374386832e-05,lr 1.0000000000000002e-06,grad 0.010867521166801453\n",
      "pre_train epoch 23000,loss 4.5512009819503874e-05,lr 1.0000000000000002e-06,grad 0.06522366404533386\n",
      "pre_train epoch 23020,loss 4.5202308683656156e-05,lr 1.0000000000000002e-06,grad 0.03023771569132805\n",
      "pre_train epoch 23040,loss 4.499853093875572e-05,lr 1.0000000000000002e-06,grad 0.01084192469716072\n",
      "pre_train epoch 23060,loss 4.4826138037024066e-05,lr 1.0000000000000002e-06,grad 0.003388789016753435\n",
      "pre_train epoch 23080,loss 4.4660020648734644e-05,lr 1.0000000000000002e-06,grad 0.0020500156097114086\n",
      "pre_train epoch 23100,loss 4.448531035450287e-05,lr 1.0000000000000002e-06,grad 0.0007736486149951816\n",
      "pre_train epoch 23120,loss 4.4311742385616526e-05,lr 1.0000000000000002e-06,grad 0.001001733704470098\n",
      "pre_train epoch 23140,loss 4.4146348955109715e-05,lr 1.0000000000000002e-06,grad 0.0021349170710891485\n",
      "pre_train epoch 23160,loss 4.397202064865269e-05,lr 1.0000000000000002e-06,grad 0.0021073713432997465\n",
      "pre_train epoch 23180,loss 4.380448081064969e-05,lr 1.0000000000000002e-06,grad 0.00277591310441494\n",
      "pre_train epoch 23200,loss 4.363208790891804e-05,lr 1.0000000000000002e-06,grad 0.007561712525784969\n",
      "pre_train epoch 23220,loss 4.3555111915338784e-05,lr 1.0000000000000002e-06,grad 0.04697953909635544\n",
      "pre_train epoch 23240,loss 4.3358533730497584e-05,lr 1.0000000000000002e-06,grad 0.03548900783061981\n",
      "pre_train epoch 23260,loss 4.315751721151173e-05,lr 1.0000000000000002e-06,grad 0.01617511175572872\n",
      "pre_train epoch 23280,loss 4.299619831726886e-05,lr 1.0000000000000002e-06,grad 0.002100932877510786\n",
      "pre_train epoch 23300,loss 4.2836010834435e-05,lr 1.0000000000000002e-06,grad 0.0006053943652659655\n",
      "pre_train epoch 23320,loss 4.2682993807829916e-05,lr 1.0000000000000002e-06,grad 0.0007959817303344607\n",
      "pre_train epoch 23340,loss 4.2521780414972454e-05,lr 1.0000000000000002e-06,grad 0.0016804025508463383\n",
      "pre_train epoch 23360,loss 4.2363903048681095e-05,lr 1.0000000000000002e-06,grad 0.0006300923996604979\n",
      "pre_train epoch 23380,loss 4.220393384457566e-05,lr 1.0000000000000002e-06,grad 0.0008080856641754508\n",
      "pre_train epoch 23400,loss 4.20518008468207e-05,lr 1.0000000000000002e-06,grad 0.002558531239628792\n",
      "pre_train epoch 23420,loss 4.1889888962032273e-05,lr 1.0000000000000002e-06,grad 0.0007416005828417838\n",
      "pre_train epoch 23440,loss 4.1735227569006383e-05,lr 1.0000000000000002e-06,grad 0.0014834548346698284\n",
      "pre_train epoch 23460,loss 4.156670183874667e-05,lr 1.0000000000000002e-06,grad 0.0023104711435735226\n",
      "pre_train epoch 23480,loss 4.175947105977684e-05,lr 1.0000000000000002e-06,grad 0.08939434587955475\n",
      "pre_train epoch 23500,loss 4.129489389015362e-05,lr 1.0000000000000002e-06,grad 0.027217451483011246\n",
      "pre_train epoch 23520,loss 4.111694579478353e-05,lr 1.0000000000000002e-06,grad 0.005486613139510155\n",
      "pre_train epoch 23540,loss 4.0976839954964817e-05,lr 1.0000000000000002e-06,grad 0.007066905032843351\n",
      "pre_train epoch 23560,loss 4.0834878745954484e-05,lr 1.0000000000000002e-06,grad 0.0005284741055220366\n",
      "pre_train epoch 23580,loss 4.0684131818125024e-05,lr 1.0000000000000002e-06,grad 0.0003176966856699437\n",
      "pre_train epoch 23600,loss 4.053979500895366e-05,lr 1.0000000000000002e-06,grad 0.0003385450690984726\n",
      "pre_train epoch 23620,loss 4.039039049530402e-05,lr 1.0000000000000002e-06,grad 0.0011141810100525618\n",
      "pre_train epoch 23640,loss 4.025021189590916e-05,lr 1.0000000000000002e-06,grad 0.0006166179664433002\n",
      "pre_train epoch 23660,loss 4.01009529014118e-05,lr 1.0000000000000002e-06,grad 0.00048812138265930116\n",
      "pre_train epoch 23680,loss 3.996121813543141e-05,lr 1.0000000000000002e-06,grad 0.0005265046493150294\n",
      "pre_train epoch 23700,loss 3.9805199776310474e-05,lr 1.0000000000000002e-06,grad 0.0011861694511026144\n",
      "pre_train epoch 23720,loss 3.9663395000388846e-05,lr 1.0000000000000002e-06,grad 0.00038603029679507017\n",
      "pre_train epoch 23740,loss 3.951135295210406e-05,lr 1.0000000000000002e-06,grad 0.0017140874406322837\n",
      "pre_train epoch 23760,loss 3.9363530959235504e-05,lr 1.0000000000000002e-06,grad 0.0037611851003021\n",
      "pre_train epoch 23780,loss 3.921483221347444e-05,lr 1.0000000000000002e-06,grad 0.0017176471883431077\n",
      "pre_train epoch 23800,loss 3.9543348975712433e-05,lr 1.0000000000000002e-06,grad 0.10454337298870087\n",
      "pre_train epoch 23820,loss 3.9293856389122084e-05,lr 1.0000000000000002e-06,grad 0.0910223051905632\n",
      "pre_train epoch 23840,loss 3.8825994124636054e-05,lr 1.0000000000000002e-06,grad 0.019740406423807144\n",
      "pre_train epoch 23860,loss 3.8695798139087856e-05,lr 1.0000000000000002e-06,grad 0.010473135858774185\n",
      "pre_train epoch 23880,loss 3.8576574297621846e-05,lr 1.0000000000000002e-06,grad 0.0007658753311261535\n",
      "pre_train epoch 23900,loss 3.845175524475053e-05,lr 1.0000000000000002e-06,grad 0.000960471632424742\n",
      "pre_train epoch 23920,loss 3.833087612292729e-05,lr 1.0000000000000002e-06,grad 0.0007190252072177827\n",
      "pre_train epoch 23940,loss 3.820594065473415e-05,lr 1.0000000000000002e-06,grad 0.0005550000350922346\n",
      "pre_train epoch 23960,loss 3.807724715443328e-05,lr 1.0000000000000002e-06,grad 0.0009958837181329727\n",
      "pre_train epoch 23980,loss 3.795936208916828e-05,lr 1.0000000000000002e-06,grad 0.000600496307015419\n",
      "pre_train epoch 24000,loss 3.783727515838109e-05,lr 1.0000000000000002e-06,grad 0.000956253963522613\n",
      "pre_train epoch 24020,loss 3.770717012230307e-05,lr 1.0000000000000002e-06,grad 0.0009457356645725667\n",
      "pre_train epoch 24040,loss 3.7583035009447485e-05,lr 1.0000000000000002e-06,grad 0.0007187470910139382\n",
      "pre_train epoch 24060,loss 3.7455356505233794e-05,lr 1.0000000000000002e-06,grad 0.0010148962028324604\n",
      "pre_train epoch 24080,loss 3.733037010533735e-05,lr 1.0000000000000002e-06,grad 0.0009167671087197959\n",
      "pre_train epoch 24100,loss 3.720788663486019e-05,lr 1.0000000000000002e-06,grad 0.0009091261890716851\n",
      "pre_train epoch 24120,loss 3.708195799845271e-05,lr 1.0000000000000002e-06,grad 0.0008774485322646797\n",
      "pre_train epoch 24140,loss 3.695438499562442e-05,lr 1.0000000000000002e-06,grad 0.0006911460659466684\n",
      "pre_train epoch 24160,loss 3.683042086777277e-05,lr 1.0000000000000002e-06,grad 0.0016812753165140748\n",
      "pre_train epoch 24180,loss 3.669891884783283e-05,lr 1.0000000000000002e-06,grad 0.0008537364192306995\n",
      "pre_train epoch 24200,loss 3.657021443359554e-05,lr 1.0000000000000002e-06,grad 0.0020407873671501875\n",
      "pre_train epoch 24220,loss 3.6446803278522566e-05,lr 1.0000000000000002e-06,grad 0.00036369828740134835\n",
      "pre_train epoch 24240,loss 3.631483923527412e-05,lr 1.0000000000000002e-06,grad 0.0002910727052949369\n",
      "pre_train epoch 24260,loss 3.617846596171148e-05,lr 1.0000000000000002e-06,grad 0.0011117022950202227\n",
      "pre_train epoch 24280,loss 3.6058310797670856e-05,lr 1.0000000000000002e-06,grad 0.0014332184800878167\n",
      "pre_train epoch 24300,loss 3.592074426705949e-05,lr 1.0000000000000002e-06,grad 0.0006241066730581224\n",
      "pre_train epoch 24320,loss 3.579348776838742e-05,lr 1.0000000000000002e-06,grad 0.0005027587758377194\n",
      "pre_train epoch 24340,loss 3.885228943545371e-05,lr 1.0000000000000002e-06,grad 0.27068156003952026\n",
      "pre_train epoch 24360,loss 3.5800352634396404e-05,lr 1.0000000000000002e-06,grad 0.0747833326458931\n",
      "pre_train epoch 24380,loss 3.5446071706246585e-05,lr 1.0000000000000002e-06,grad 0.00267211627215147\n",
      "pre_train epoch 24400,loss 3.534548886818811e-05,lr 1.0000000000000002e-06,grad 0.011058693751692772\n",
      "pre_train epoch 24420,loss 3.5245302569819614e-05,lr 1.0000000000000002e-06,grad 0.0038599404506385326\n",
      "pre_train epoch 24440,loss 3.5132234188495204e-05,lr 1.0000000000000002e-06,grad 0.00030233501456677914\n",
      "pre_train epoch 24460,loss 3.502966501400806e-05,lr 1.0000000000000002e-06,grad 0.0009922061581164598\n",
      "pre_train epoch 24480,loss 3.491933603072539e-05,lr 1.0000000000000002e-06,grad 0.0004297196865081787\n",
      "pre_train epoch 24500,loss 3.48202302120626e-05,lr 1.0000000000000002e-06,grad 0.000605726963840425\n",
      "pre_train epoch 24520,loss 3.471265154075809e-05,lr 1.0000000000000002e-06,grad 0.00029623694717884064\n",
      "pre_train epoch 24540,loss 3.4601387596921995e-05,lr 1.0000000000000002e-06,grad 0.0010967605048790574\n",
      "pre_train epoch 24560,loss 3.44925319950562e-05,lr 1.0000000000000002e-06,grad 0.0003036942216567695\n",
      "pre_train epoch 24580,loss 3.4385833714623004e-05,lr 1.0000000000000002e-06,grad 0.0010277106193825603\n",
      "pre_train epoch 24600,loss 3.427426054258831e-05,lr 1.0000000000000002e-06,grad 0.00029060672386549413\n",
      "pre_train epoch 24620,loss 3.4166951081715524e-05,lr 1.0000000000000002e-06,grad 0.0008171634399332106\n",
      "pre_train epoch 24640,loss 3.405728057259694e-05,lr 1.0000000000000002e-06,grad 0.0008894812781363726\n",
      "pre_train epoch 24660,loss 3.393625593162142e-05,lr 1.0000000000000002e-06,grad 0.0010825758799910545\n",
      "pre_train epoch 24680,loss 3.383582952665165e-05,lr 1.0000000000000002e-06,grad 0.0009894424583762884\n",
      "pre_train epoch 24700,loss 3.372673018020578e-05,lr 1.0000000000000002e-06,grad 0.001590366242453456\n",
      "pre_train epoch 24720,loss 3.3610511309234425e-05,lr 1.0000000000000002e-06,grad 0.00032419702620245516\n",
      "pre_train epoch 24740,loss 3.3501182770123705e-05,lr 1.0000000000000002e-06,grad 0.0012500217417255044\n",
      "pre_train epoch 24760,loss 3.3390217140549794e-05,lr 1.0000000000000002e-06,grad 0.000831419718451798\n",
      "pre_train epoch 24780,loss 3.3279586205026135e-05,lr 1.0000000000000002e-06,grad 0.0004419779288582504\n",
      "pre_train epoch 24800,loss 3.31613227899652e-05,lr 1.0000000000000002e-06,grad 0.0011496915249153972\n",
      "pre_train epoch 24820,loss 3.304841084172949e-05,lr 1.0000000000000002e-06,grad 0.0029799086041748524\n",
      "pre_train epoch 24840,loss 3.375046799192205e-05,lr 1.0000000000000002e-06,grad 0.13662143051624298\n",
      "pre_train epoch 24860,loss 3.285682760179043e-05,lr 1.0000000000000002e-06,grad 0.02580554224550724\n",
      "pre_train epoch 24880,loss 3.272600588388741e-05,lr 1.0000000000000002e-06,grad 0.01051135454326868\n",
      "pre_train epoch 24900,loss 3.261869278503582e-05,lr 1.0000000000000002e-06,grad 0.004561750218272209\n",
      "pre_train epoch 24920,loss 3.251252928748727e-05,lr 1.0000000000000002e-06,grad 0.0011057865340262651\n",
      "pre_train epoch 24940,loss 3.24093307426665e-05,lr 1.0000000000000002e-06,grad 0.0011400803923606873\n",
      "pre_train epoch 24960,loss 3.229908543289639e-05,lr 1.0000000000000002e-06,grad 0.0007055124151520431\n",
      "pre_train epoch 24980,loss 3.2193504011956975e-05,lr 1.0000000000000002e-06,grad 0.0006314930506050587\n",
      "pre_train epoch 25000,loss 3.2083939004223794e-05,lr 1.0000000000000002e-06,grad 0.0005547149339690804\n",
      "pre_train epoch 25020,loss 3.197720070602372e-05,lr 1.0000000000000002e-06,grad 0.0010442542843520641\n",
      "pre_train epoch 25040,loss 3.186935282428749e-05,lr 1.0000000000000002e-06,grad 0.0009409285266883671\n",
      "pre_train epoch 25060,loss 3.175748497596942e-05,lr 1.0000000000000002e-06,grad 0.002309469971805811\n",
      "pre_train epoch 25080,loss 3.427056799409911e-05,lr 1.0000000000000002e-06,grad 0.24489037692546844\n",
      "pre_train epoch 25100,loss 3.160577762173489e-05,lr 1.0000000000000002e-06,grad 0.03282569721341133\n",
      "pre_train epoch 25120,loss 3.1471285183215514e-05,lr 1.0000000000000002e-06,grad 0.0035690483637154102\n",
      "pre_train epoch 25140,loss 3.137662133667618e-05,lr 1.0000000000000002e-06,grad 0.0003406866453588009\n",
      "pre_train epoch 25160,loss 3.128799289697781e-05,lr 1.0000000000000002e-06,grad 0.0006007771007716656\n",
      "pre_train epoch 25180,loss 3.1196137570077553e-05,lr 1.0000000000000002e-06,grad 0.0004614235076587647\n",
      "pre_train epoch 25200,loss 3.110155012109317e-05,lr 1.0000000000000002e-06,grad 0.000246856507146731\n",
      "pre_train epoch 25220,loss 3.100718458881602e-05,lr 1.0000000000000002e-06,grad 0.0003871481167152524\n",
      "pre_train epoch 25240,loss 3.091972030233592e-05,lr 1.0000000000000002e-06,grad 0.0005350373685359955\n",
      "pre_train epoch 25260,loss 3.083180490648374e-05,lr 1.0000000000000002e-06,grad 0.00035396820749156177\n",
      "pre_train epoch 25280,loss 3.0737919587409124e-05,lr 1.0000000000000002e-06,grad 0.0006848904304206371\n",
      "pre_train epoch 25300,loss 3.064490738324821e-05,lr 1.0000000000000002e-06,grad 0.00030856067314743996\n",
      "pre_train epoch 25320,loss 3.054750050068833e-05,lr 1.0000000000000002e-06,grad 0.0006031239754520357\n",
      "pre_train epoch 25340,loss 3.0451759812422097e-05,lr 1.0000000000000002e-06,grad 0.001032163156196475\n",
      "pre_train epoch 25360,loss 3.0357434297911823e-05,lr 1.0000000000000002e-06,grad 0.0005654987180605531\n",
      "pre_train epoch 25380,loss 3.0259598133852705e-05,lr 1.0000000000000002e-06,grad 0.00024690042482689023\n",
      "pre_train epoch 25400,loss 3.0168974262778647e-05,lr 1.0000000000000002e-06,grad 0.0013533438323065639\n",
      "pre_train epoch 25420,loss 3.0064642487559468e-05,lr 1.0000000000000002e-06,grad 0.0005145262693986297\n",
      "pre_train epoch 25440,loss 2.9975499273859896e-05,lr 1.0000000000000002e-06,grad 0.0007339800358749926\n",
      "pre_train epoch 25460,loss 2.987662264786195e-05,lr 1.0000000000000002e-06,grad 0.0036268800031393766\n",
      "pre_train epoch 25480,loss 2.9778515454381704e-05,lr 1.0000000000000002e-06,grad 0.0019539131317287683\n",
      "pre_train epoch 25500,loss 2.9681514206458814e-05,lr 1.0000000000000002e-06,grad 0.002210246864706278\n",
      "pre_train epoch 25520,loss 2.9584625735878944e-05,lr 1.0000000000000002e-06,grad 0.0032955710776150227\n",
      "pre_train epoch 25540,loss 3.0084251193329692e-05,lr 1.0000000000000002e-06,grad 0.11682850867509842\n",
      "pre_train epoch 25560,loss 2.941713137261104e-05,lr 1.0000000000000002e-06,grad 0.01216794177889824\n",
      "pre_train epoch 25580,loss 2.9328992241062224e-05,lr 1.0000000000000002e-06,grad 0.012872411869466305\n",
      "pre_train epoch 25600,loss 2.9248294595163316e-05,lr 1.0000000000000002e-06,grad 0.005759833380579948\n",
      "pre_train epoch 25620,loss 2.9170085326768458e-05,lr 1.0000000000000002e-06,grad 0.002993322443217039\n",
      "pre_train epoch 25640,loss 2.908435635617934e-05,lr 1.0000000000000002e-06,grad 0.0023590109776705503\n",
      "pre_train epoch 25660,loss 2.9001386792515405e-05,lr 1.0000000000000002e-06,grad 0.0007204026915132999\n",
      "pre_train epoch 25680,loss 2.8921971534145996e-05,lr 1.0000000000000002e-06,grad 0.002118842676281929\n",
      "pre_train epoch 25700,loss 2.8834840122726746e-05,lr 1.0000000000000002e-06,grad 0.00044991960749030113\n",
      "pre_train epoch 25720,loss 2.874935671570711e-05,lr 1.0000000000000002e-06,grad 0.000831409590318799\n",
      "pre_train epoch 25740,loss 2.8671451218542643e-05,lr 1.0000000000000002e-06,grad 0.0015593123389407992\n",
      "pre_train epoch 25760,loss 2.8586193366209045e-05,lr 1.0000000000000002e-06,grad 0.0004859640612266958\n",
      "pre_train epoch 25780,loss 2.8502618079073727e-05,lr 1.0000000000000002e-06,grad 0.001356944558210671\n",
      "pre_train epoch 25800,loss 2.841927380359266e-05,lr 1.0000000000000002e-06,grad 0.0015551118412986398\n",
      "pre_train epoch 25820,loss 2.833107646438293e-05,lr 1.0000000000000002e-06,grad 0.00027607474476099014\n",
      "pre_train epoch 25840,loss 2.824644798238296e-05,lr 1.0000000000000002e-06,grad 0.000494454347062856\n",
      "pre_train epoch 25860,loss 2.8160233341623098e-05,lr 1.0000000000000002e-06,grad 0.00025802230811677873\n",
      "pre_train epoch 25880,loss 2.807741293509025e-05,lr 1.0000000000000002e-06,grad 0.0037268304731696844\n",
      "pre_train epoch 25900,loss 2.7989774025627412e-05,lr 1.0000000000000002e-06,grad 0.0004631592892110348\n",
      "pre_train epoch 25920,loss 2.7901622161152773e-05,lr 1.0000000000000002e-06,grad 0.003034741384908557\n",
      "pre_train epoch 25940,loss 2.78135557891801e-05,lr 1.0000000000000002e-06,grad 0.0023140404373407364\n",
      "pre_train epoch 25960,loss 2.7726495318347588e-05,lr 1.0000000000000002e-06,grad 0.002529620658606291\n",
      "pre_train epoch 25980,loss 2.764481359918136e-05,lr 1.0000000000000002e-06,grad 0.01163622085005045\n",
      "pre_train epoch 26000,loss 2.7875785235664807e-05,lr 1.0000000000000002e-06,grad 0.08600340038537979\n",
      "pre_train epoch 26020,loss 2.748368206084706e-05,lr 1.0000000000000002e-06,grad 0.006619592197239399\n",
      "pre_train epoch 26040,loss 2.741354182944633e-05,lr 1.0000000000000002e-06,grad 0.012576456181704998\n",
      "pre_train epoch 26060,loss 2.7332687750458717e-05,lr 1.0000000000000002e-06,grad 0.00639372831210494\n",
      "pre_train epoch 26080,loss 2.7258922273176722e-05,lr 1.0000000000000002e-06,grad 0.003214003052562475\n",
      "pre_train epoch 26100,loss 2.718273753998801e-05,lr 1.0000000000000002e-06,grad 0.0009508599177934229\n",
      "pre_train epoch 26120,loss 2.711031265789643e-05,lr 1.0000000000000002e-06,grad 0.0005041883559897542\n",
      "pre_train epoch 26140,loss 2.7032914658775553e-05,lr 1.0000000000000002e-06,grad 0.0011092895874753594\n",
      "pre_train epoch 26160,loss 2.6955125576932915e-05,lr 1.0000000000000002e-06,grad 0.0004460318014025688\n",
      "pre_train epoch 26180,loss 2.688074528123252e-05,lr 1.0000000000000002e-06,grad 0.0011553270742297173\n",
      "pre_train epoch 26200,loss 2.680258512555156e-05,lr 1.0000000000000002e-06,grad 0.0011691444087773561\n",
      "pre_train epoch 26220,loss 2.6725634597823955e-05,lr 1.0000000000000002e-06,grad 0.0003630835562944412\n",
      "pre_train epoch 26240,loss 2.6652425731299445e-05,lr 1.0000000000000002e-06,grad 0.0015799147076904774\n",
      "pre_train epoch 26260,loss 2.656834840308875e-05,lr 1.0000000000000002e-06,grad 0.0012214272283017635\n",
      "pre_train epoch 26280,loss 2.6492332835914567e-05,lr 1.0000000000000002e-06,grad 0.0017232196405529976\n",
      "pre_train epoch 26300,loss 2.6411529688630253e-05,lr 1.0000000000000002e-06,grad 0.0015299535589292645\n",
      "pre_train epoch 26320,loss 2.633436997712124e-05,lr 1.0000000000000002e-06,grad 0.0018038596026599407\n",
      "pre_train epoch 26340,loss 2.6258601792505942e-05,lr 1.0000000000000002e-06,grad 0.001410000491887331\n",
      "pre_train epoch 26360,loss 2.6178457119385712e-05,lr 1.0000000000000002e-06,grad 0.003270884044468403\n",
      "pre_train epoch 26380,loss 2.609353032312356e-05,lr 1.0000000000000002e-06,grad 0.0021704896353185177\n",
      "pre_train epoch 26400,loss 2.6041383534902707e-05,lr 1.0000000000000002e-06,grad 0.023089006543159485\n"
     ]
    }
   ],
   "source": [
    "#pre_train\n",
    "optimizer_pre=torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "scheduler_pre = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_pre,patience=20,threshold=1e-4)\n",
    "V_real_poten=potential(grid,k).detach()\n",
    "model.train()\n",
    "for i in range(50000):\n",
    "    optimizer_pre.zero_grad()\n",
    "    V_diag=model(grid)\n",
    "    loss=loss_fn(V_real_poten,V_diag)\n",
    "    loss.backward()\n",
    "    optimizer_pre.step()\n",
    "    if i%20==0:\n",
    "        print(f'pre_train epoch {i},loss {loss.item()},lr {optimizer_pre.param_groups[0][\"lr\"]},grad {torch.norm(model.MLP[0].weight.grad)}')\n",
    "    scheduler_pre.step(loss)\n",
    "    if optimizer_pre.param_groups[0][\"lr\"] <= 1.1e-7:break\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABgo0lEQVR4nO3dd3hV9eHH8fe52YEMEsiCsCGMQNgBVESgLMU9QFBRnAWtYluLvxbUtuKqs7aOqqg4sYqKirJR2SPMEAgQAoSEmb1zz++PaGpkJiT53pt8Xs9znoec8703n5OTeD+eadm2bSMiIiLiQhymA4iIiIj8mgqKiIiIuBwVFBEREXE5KigiIiLiclRQRERExOWooIiIiIjLUUERERERl6OCIiIiIi7H03SA6nA6naSlpREQEIBlWabjiIiIyDmwbZucnByioqJwOM68j8QtC0paWhrR0dGmY4iIiEg17N+/nxYtWpxxjFsWlICAAKB8BQMDAw2nERERkXORnZ1NdHR0xef4mbhlQfn5sE5gYKAKioiIiJs5l9MzdJKsiIiIuBwVFBEREXE5KigiIiLiclRQRERExOWooIiIiIjLUUERERERl6OCIiIiIi5HBUVERERcjgqKiIiIuJwqFZSZM2fSt29fAgICCAsL48orryQpKanSmMLCQiZPnkxoaCiNGzfmmmuuISMjo9KY1NRULr30Uvz9/QkLC+MPf/gDpaWl5782IiIiUi9UqaAsW7aMyZMns2rVKhYsWEBJSQnDhw8nLy+vYswDDzzAl19+yZw5c1i2bBlpaWlcffXVFcvLysq49NJLKS4uZsWKFbz99tvMmjWL6dOn19xaiYiIiFuzbNu2q/viI0eOEBYWxrJlyxg0aBBZWVk0a9aM999/n2uvvRaAHTt20LlzZ1auXEn//v355ptvuOyyy0hLSyM8PByAV155hYceeogjR47g7e191u+bnZ1NUFAQWVlZehaPiIiIm6jK5/d5nYOSlZUFQEhICADr16+npKSEYcOGVYzp1KkTLVu2ZOXKlQCsXLmSbt26VZQTgBEjRpCdnc22bdtO+X2KiorIzs6uNNWGtO0rSXj1djZ//VqtvL+IiIirS9m8nM0vXsfulZ8bzVHtguJ0Orn//vu54IILiI2NBSA9PR1vb2+Cg4MrjQ0PDyc9Pb1izC/Lyc/Lf152KjNnziQoKKhiio6Orm7sM9qz9ht6HJqD16Z3a+X9RUREXN2RFe/R/fh3HP3xHaM5ql1QJk+ezNatW/nwww9rMs8pTZs2jaysrIpp//79tfJ9Wg0aD0BM4RaOp++rle8hIiLiqmxnGa3SFwDgiL36LKNrV7UKypQpU5g3bx5LliyhRYsWFfMjIiIoLi4mMzOz0viMjAwiIiIqxvz6qp6fv/55zK/5+PgQGBhYaaoN0W1iSPTshMOy2b10dq18DxEREVe1a90iwjhGju1H7KCrjGapUkGxbZspU6bw2WefsXjxYtq0aVNpee/evfHy8mLRokUV85KSkkhNTWXAgAEADBgwgC1btnD48OGKMQsWLCAwMJAuXbqcz7rUiBNtxgAQuPtLw0lERETqVua6jwHYHjQIP39/o1mqVFAmT57M7Nmzef/99wkICCA9PZ309HQKCgoACAoKYtKkSUydOpUlS5awfv16br31VgYMGED//v0BGD58OF26dOGmm25i06ZNfPvtt/z5z39m8uTJ+Pj41PwaVlG7weNx2hYxJYlkpO40HUdERKROOEtLaXu4/PCOd3ezh3egigXl3//+N1lZWQwePJjIyMiK6aOPPqoY89xzz3HZZZdxzTXXMGjQICIiIvj0008rlnt4eDBv3jw8PDwYMGAAEyZM4Oabb+axxx6rubU6D+HN25DoXX7Sb8qy9wynERERqRs71nxHUzLJphFdLrrCdBw8qzL4XG6Z4uvry8svv8zLL7982jGtWrXi66+/rsq3rlM5HS6H7VsISZkHPGo6joiISK3LWV++s2FH8MX08/EznEbP4jmlDoPHU2o76FCWzP7krabjiIiI1KrSkmLaH1sCgF+Paw2nKaeCcgqhYc1J9O0BwIHvdZhHRETqt+0rvyGULDJpTOeBl5mOA6ignFZhTPnxt/D9X53ToS0RERF3lb9xDgDJIZfg6W3+ghVQQTmtmME3Umx70Na5j707NpiOIyIiUiuKigrofGIxAI16X284zf+ooJxGYEgYOxr1AeDQD+8bTiMiIlI7tn//OUHkcYxgYuJHm45TQQXlDMo6l99Fr3naN9hOp+E0IiIiNa908ycA7A77DQ7PKl3cW6tUUM4gZvANFNletLYPkrR5lek4IiIiNaogL4cuWd8DENxvnOE0lamgnIF/QAg7AsrvgHt01QeG04iIiNSs7cs/oZFVyCGrGR16XWI6TiUqKGdh/fQ0x9bp31JWpsM8IiJSf1hb/wvAvshRWA7XqgSulcYFxQy6lgJ8aEEG29cvNR1HRESkRuRkHadrbvnpC83632g4zclUUM7Cxz+QpKALAMhc+9FZRouIiLiHpKUf4GOVsM/Rgrax8abjnEQF5Rx4x5Xf9rf9kQWUlJYaTiMiInL+vBM/AyCtxWiXO7wDKijnpOMFV5GLH5EcY8uqhabjiIiInJfMI4foXFB+E9KoC8YbTnNqKijnwNPHn+QmFwOQt0GHeURExL3tXPYeXlYZyR7taBXTw3ScU1JBOUf+va4DoNOxxRQWFRtOIyIiUn2Ndn4OwJFWlxpOcnoqKOeoff8xZNGYZlYmW3740nQcERGRajl6cC+di7YA0GrQBMNpTk8F5Rw5vHzY3ew3AJRu+thwGhERkerZs2w2Dssm0bMzUa1jTMc5LRWUKgjqV36deGzWMnLzcg2nERERqbrgPeVHAU60u8JwkjNTQamCtr2HkmE1JcAqYNvST0zHERERqZJDKYl0LE2izLZof7FrXr3zMxWUKrAcHqRGjgLAc9scw2lERESqZt+y2QBs8+lBWFRLw2nOTAWlisIuuAmA2LzVZB0/YjiNiIjIuQvfNw+AgpgrzQY5ByooVdSqSz/2OaLxsUrYseR903FERETOyZ7ta2njTKHY9qDTJa59eAdUUKrOsjjU8nIA/Hd+ZjiMiIjIuTn0w3sAJDbqR1BIM8Npzk4FpRpaXnwzAF0KEzh6aJ/hNCIiImfmLHPSOu3r8n/HXmM4zblRQamGqDad2OHZGQ/LZs+Sd0zHEREROaPEtQtpTgZ5ti+dB481HeecqKBU0/Gfrh9vsucLw0lERETOLHtt+TmTicEX4+sfYDjNuVFBqab2l0yg1HbQoXQnh/ZsNR1HRETklIqLiuh0bCEAvr3HGU5z7lRQqiksIpqtvr0A2L9Mh3lERMQ1bV3+KU3I4RjBdB54mek450wF5TwUdboagMj987CdTsNpRERETub86flxu8JG4OHpZTjNuVNBOQ+dh4yjwPYm2nmQ3Zt/NB1HRESkkpys43TN+QGApgNc98nFp6KCch4Cg0LYFnABAEdXvmc4jYiISGWJSz7AzypmvxVFu7gLTcepkioXlOXLlzNmzBiioqKwLIu5c+dWWm5Z1imnp59+umJM69atT1r+xBNPnPfKmOAZdz0A7TLmU1ZaajiNiIjI//ju+C8AB1pejuVwr30SVU6bl5dHXFwcL7/88imXHzp0qNL05ptvYlkW11xT+cYwjz32WKVx9957b/XWwLAug64mi0Y04wTbVn5tOo6IiAgAR9NT6VqwAfjfDUbdiWdVXzBq1ChGjRp12uURERGVvv7888+55JJLaNu2baX5AQEBJ411R94+vmwKHULfY1+Sv/5DuOhy05FERERIXvw2/S2bHZ6d6dS2q+k4VVar+3syMjL46quvmDRp0knLnnjiCUJDQ+nZsydPP/00pWc4PFJUVER2dnalyZUE9rsRgC4nlpCfn2s4jYiICITumQtAZvsrzAapplotKG+//TYBAQFcffXVlebfd999fPjhhyxZsoS77rqLxx9/nD/+8Y+nfZ+ZM2cSFBRUMUVHR9dm7Crr2HcEh61QAq18ti6ZYzqOiIg0cPt3JtChNLn8hqJD3O/wDtRyQXnzzTcZP348vr6+leZPnTqVwYMH0717d+6++27+8Y9/8NJLL1FUVHTK95k2bRpZWVkV0/79+2szdpVZDg9Soi4FwHPrx4bTiIhIQ3dwefkNRLf69SU0rLnhNNVTawXl+++/Jykpidtvv/2sY+Pj4yktLSUlJeWUy318fAgMDKw0uZrIiyYC0C1/NUcz0syGERGRBst2Ook+OA+Akq7u8eTiU6m1gvLGG2/Qu3dv4uLizjo2ISEBh8NBWFhYbcWpddGdepPs2R4vq4xdS942HUdERBqoXRuW0Nwuf3Jxl0vc59k7v1blq3hyc3NJTk6u+Hrv3r0kJCQQEhJCy5YtAcjOzmbOnDn84x//OOn1K1euZPXq1VxyySUEBASwcuVKHnjgASZMmECTJk3OY1XMO972Stj5DCHJnwHTTMcREZEGKHPVbAC2BQ2iX2PXO+Jwrqq8B2XdunX07NmTnj17AuXnk/Ts2ZPp06dXjPnwww+xbZtx405ubj4+Pnz44YdcfPHFdO3alb///e888MADvPbaa+exGq6h3dCJlNoOYkqTSN2ZYDqOiIg0MMVFRXQ4ugAAn15jDac5P5Zt27bpEFWVnZ1NUFAQWVlZLnc+yqYnf0NcwRpWtbiN/rc/ZzqOiIg0IBsXfkjPH+7iGMEE/d8uPL28TUeqpCqf3+5131s3UNK1/Nb3LQ/Ow3aWGU4jIiINSVnChwDsDh/ucuWkqlRQaljXS8aRa/sRZR8mac13puOIiEgDkXX8CN1+enJxswtuMZzm/Kmg1DC/Ro3Z3uQSAHLW6AnHIiJSNxIXvYOPVUKKoyVtul1gOs55U0GpBX59xgMQc3wRxYX5htOIiEhDELiz/MnFh9teBZZlOM35U0GpBV0GjOIQTQkkn8SlH5mOIyIi9dz+3dvoUrKNMtui7ZDbTMepESootcDDw4M9kaMBsDZ/aDiNiIjUdweWvAnANr/eNI1qbTZMDVFBqSURP936vnPeWjKP6Nb3IiJSO5xlZbQ6+CUAxT9dSVofqKDUknZderPTowNeVhk7F80yHUdEROqpHWu+I8rOINf2o+uQG03HqTEqKLXoWLurAGiS/KnhJCIiUl/lrnkXgO1NLsGvUYDhNDVHBaUWdRx6CyW2Bx1Kd5GyY6PpOCIiUs8U5ufQ5fhiABrF32w4Tc1SQalFoeEt2O7fF4CDy98ynEZEROqbbYs/pLFVQJoVRud+w03HqVEqKLXM7n4DAG3TvqKsTLe+FxGRmuO9tfxK0X3NL8fh4WE4Tc1SQallXQbfQA7+RHKULT/MMx1HRETqiaOHUuhSsB6A5hdPNBumFqig1DJvv0Ykhf4GgOL1sw2nERGR+mL3orfwsGwSvbrQskM303FqnApKHWgycCIA3bKWkpV53GwYERFxf7ZN+J7yK0SzOl5rOEztUEGpA217Dma/ozl+VjGJC98xHUdERNzc3i0raO1Mpcj2ovPQ+nX1zs9UUOqA5XCQ1uYaAAKTPjacRkRE3F3GD7MA2BJwIUEhzcyGqSUqKHWk3dBJlNkWXUq2sT95i+k4IiLipoqLiuh4eD4AXr3GGU5Te1RQ6kjTqNZs9esDwIElbxhOIyIi7mrLsk8IIZtjBNP1oqtMx6k1Kih1qLR7edNte/ALnKWlhtOIiIhbSngfgOSIUXh6eRsOU3tUUOpQ10vGkkUjwjlG4oovTccRERE3cyTjAN3zVgIQefHthtPULhWUOuTr14jE0PJbERete9dwGhERcTe7FryBl1XGLs+OtOzcx3ScWqWCUseCL7gVgC5Zy8nNOmY4jYiIuAvb6SRyzycAZMbcYDhN7VNBqWMxPS5ir6MlvlYJOxbOMh1HRETcxM6Ny2njTKXQ9qLTbyaajlPrVFDqmOVwkNb6agACd+ieKCIicm4yV7wFwNagwQQENzWcpvapoBjQYdgkSm0HHUt2sD9po+k4IiLi4grzc+l87DsA/PreZDhN3VBBMSAsqiVb/OMBOLhU90QREZEz27roPQLJ55DVjM4DLzMdp06ooBhi9xgPQLtD8ygtKTacRkREXJnv1vJ7n6REX4XDw8NwmrqhgmJI7ODrOEEgzTjBluWfmY4jIiIu6tC+JGKLEnDaFq2G1O97n/ySCooh3j6+7AofCUDZhtmG04iIiKvat+g/AGz3jSOqdYzhNHVHBcWgiIsnAdAtdwVHDqcZTiMiIq7GWVZGq/3le9kLYuvvgwFPpcoFZfny5YwZM4aoqCgsy2Lu3LmVlk+cOBHLsipNI0eOrDTm+PHjjB8/nsDAQIKDg5k0aRK5ubnntSLuqGWX/uzxbIePVcrO73SyrIiIVJa48isi7SNk2/7EDplgOk6dqnJBycvLIy4ujpdffvm0Y0aOHMmhQ4cqpg8++KDS8vHjx7Nt2zYWLFjAvHnzWL58OXfeeWfV09cDx2PGAhC5Zw6202k4jYiIuJLCNW8DsL3pcPwaNTacpm55VvUFo0aNYtSoUWcc4+PjQ0RExCmXJSYmMn/+fNauXUufPuXPEXjppZcYPXo0zzzzDFFRUVWN5NY6DZ9E4danaOvcR+L6JXTuO9R0JBERcQE5mUfpmrUMLAgaeJvpOHWuVs5BWbp0KWFhYcTExHDPPfdw7Nj/njmzcuVKgoODK8oJwLBhw3A4HKxevfqU71dUVER2dnalqb5oHBTKtuAhAGSveNNwGhERcRWJC2bha5Wwx9GKTj0vMh2nztV4QRk5ciTvvPMOixYt4sknn2TZsmWMGjWKsrIyANLT0wkLC6v0Gk9PT0JCQkhPTz/le86cOZOgoKCKKTo6uqZjG+U/oPwBgt2OLyA3+4ThNCIi4gqCkz4C4HC7a7EcDe+alhpf47Fjx3L55ZfTrVs3rrzySubNm8fatWtZunRptd9z2rRpZGVlVUz79++vucAuoFO/Eey3ovC3iti+4G3TcURExLDkrWvoWLqTEtuDmOEN594nv1Trlaxt27Y0bdqU5ORkACIiIjh8+HClMaWlpRw/fvy05634+PgQGBhYaapPLIeDg22uBSBoxwdnGS0iIvXdkaWvAbA1YCBNmjWsczN/VusF5cCBAxw7dozIyEgABgwYQGZmJuvXr68Ys3jxYpxOJ/Hx8bUdx2W1H35HeVMu2UFq4lrTcURExJCC/Dy6HP0aAK++txpOY06VC0pubi4JCQkkJCQAsHfvXhISEkhNTSU3N5c//OEPrFq1ipSUFBYtWsQVV1xB+/btGTFiBACdO3dm5MiR3HHHHaxZs4Yff/yRKVOmMHbs2AZ3Bc8vNY1oyZZGAwBIX/ofw2lERMSULQveJYg80q1mdLngCtNxjKlyQVm3bh09e/akZ8+eAEydOpWePXsyffp0PDw82Lx5M5dffjkdO3Zk0qRJ9O7dm++//x4fH5+K93jvvffo1KkTQ4cOZfTo0Vx44YW89tprNbdWbsrqfTMAHTPmUVKUbziNiIiY0HjruwDsa3k1Ds8q3w2k3rBs27ZNh6iq7OxsgoKCyMrKqlfno5SUlHDs7zFEcIyE+GfpMWqS6UgiIlKH9iUl0OqDiymzLY7fuYFmzduajlSjqvL53fCuW3JhXl5e7G5+Zfm/E941G0ZEROpc2pJXAdjqH1/vyklVqaC4mFbD7sRpW3Qt2kja3kTTcUREpI4UFebTKX0eAHavWwynMU8FxcW0aNOJrb69AEhd9KrhNCIiUle2Lf6QJmRzmBBiB19rOo5xKiguqCTuJgDaHvickpJiw2lERKQueG96B4Ddza/A08vbcBrzVFBcULchYzlBIGEcZ8vS/5qOIyIitSxtbyKxRRtx2hatht1tOo5LUEFxQd6+fuyKuKz8iw3vmA0jIiK1LnXhKwBs9etFVJtOhtO4BhUUF9V8yJ0AdM9fRdr+PYbTiIhIbSktLqL9wbnA/w7xiwqKy2resSc7vLviaTnZ+90rpuOIiEgt2bZsDk3J5DiBxA4ZazqOy1BBcWEF3ScC0G7/fyktKTEbRkREaoX106H8HRGX4+PjZziN61BBcWFdh00gkwAiOMrmpZ+YjiMiIjXs8P5kuuavAf53aF/KqaC4MG9ff5IixgDg2PCW4TQiIlLT9nz3Ch6WzVbv7rTqGGc6jktRQXFxzYfeA0D3/DUc2pdkOI2IiNSU0pISWu//DIDCbhMMp3E9KigurkWH7mz16YHDstm3QCfLiojUF5uXziGCo2TSmG6/UUH5NRUUN1AUV/5MhvYHPqW0uMhwGhERqQke698EICnicnx8GxlO43pUUNxAt6E3cpRgmpLJ1iUfmY4jIiLn6eCeHXQrWAdA9G8mG07jmlRQ3IC3jy87I68AwHOjTpYVEXF3qQtexmHZbPXtRVS7WNNxXJIKipuI/s09OG2L2MINHNq7zXQcERGppqLCfGIOzQWgtNdtZsO4MBUUNxHdtjObffsAsH/Bvw2nERGR6tqycDYhZHOYEGIvucF0HJelguJGSntNBKBD2ucUFxaYDSMiItXiv/ltAHa3vBZPL2/DaVyXCoobiRtyPRmE0IRstiycbTqOiIhU0b7ta+lSvJVS20G7Eb81HcelqaC4ES8vb/ZEXwuA76a3DacREZGqylhSfoh+c+MLCGvexnAa16aC4mbajbiHMtuia8kW9iSuNx1HRETOUX5uJl2OfA2AZ/zthtO4PhUUNxPWoi1bGw8EIH2RTpYVEXEX2799k8YUcMCKJPaCMabjuDwVFDfkFT8JgNgjX5GTnWk2jIiInJ1tE7L9XQD2tx2Lw8PDcCDXp4LihjpfeCUHrQgCrXy2zn/DdBwRETmL5I3LaFu2hyLbi5iRd5mO4xZUUNyQ5fDgYIfxAITteAfb6TScSEREziRzefkh+U1BQwhpFmk4jXtQQXFTnUbfQ4HtTTtnCttXf2s6joiInEbm0QxiTywCIPAi7T05VyoobiowuBlbQkcAULDiVcNpRETkdLZ/8wq+Vgm7PdoS0/sS03HchgqKGwsdXP4EzLjs5RxN22c4jYiI/FpZWRnRez4A4ESXm7Ac+tg9V/pJubF23QeQ6NUFL6uM5Pn/NB1HRER+ZfPST4i2D5FNI2JH6t4nVaGC4uby4sqfhNk+9WNKiwsNpxERkV9yrH0NgMSIy/FtFGg4jXupckFZvnw5Y8aMISoqCsuymDt3bsWykpISHnroIbp160ajRo2Iiori5ptvJi0trdJ7tG7dGsuyKk1PPPHEea9MQ9TtNxM4SjBNyWTrovdMxxERkZ+k7tpEXOE6nLZF9PD7TMdxO1UuKHl5ecTFxfHyyy+ftCw/P58NGzbwl7/8hQ0bNvDpp5+SlJTE5ZdfftLYxx57jEOHDlVM9957b/XWoIHz8fEjqfk15f/e+KbhNCIi8rO078oPvW/x70dU2y6G07gfz6q+YNSoUYwaNeqUy4KCgliwYEGlef/85z/p168fqamptGzZsmJ+QEAAERERVf32cgptRk6h5D+z6Fy8lZTtq2ndJd50JBGRBi03J5Ouh78ECxz9dWlxddT6OShZWVlYlkVwcHCl+U888QShoaH07NmTp59+mtLS0tO+R1FREdnZ2ZUm+Z+o6LZsbnwhAIcX6mRZERHTtn39KgFWAfutKLpeeKXpOG6pVgtKYWEhDz30EOPGjSMw8H8nB9133318+OGHLFmyhLvuuovHH3+cP/7xj6d9n5kzZxIUFFQxRUdH12Zst+R74T0AxB77lqzjRwynERFpuGynk4ik8ufuHOx4k567U02Wbdt2tV9sWXz22WdceeWVJy0rKSnhmmuu4cCBAyxdurRSQfm1N998k7vuuovc3Fx8fHxOWl5UVERRUVHF19nZ2URHR5OVlXXG921IbKeTlL/1oI1zHyvbP8iACdNNRxIRaZC2ff85XRfdTJ7ti3NqIgFBIaYjuYzs7GyCgoLO6fO7VvaglJSUcP3117Nv3z4WLFhw1hDx8fGUlpaSkpJyyuU+Pj4EBgZWmqQyy+HgWJebAWix+33KysoMJxIRaZhKVr4CwJZmo1VOzkONF5Sfy8muXbtYuHAhoaGhZ31NQkICDoeDsLCwmo7ToMSOuoMc/Ii2D7FpySem44iINDjp+5LolrcSgIhhurT4fFT5Kp7c3FySk5Mrvt67dy8JCQmEhIQQGRnJtddey4YNG5g3bx5lZWWkp6cDEBISgre3NytXrmT16tVccsklBAQEsHLlSh544AEmTJhAkyZNam7NGiDfRkFsjbicPukfld8caNgNpiOJiDQo++a/SIRls9mnF9079TQdx61V+RyUpUuXcsklJz/s6JZbbuGRRx6hTZs2p3zdkiVLGDx4MBs2bOC3v/0tO3bsoKioiDZt2nDTTTcxderUU55/cipVOYbV0KSnJBL21gAcls3eG5bSprP+QERE6kJhfg5FT8UQRB4bLvgXvX4z3nQkl1OVz+8q70EZPHgwZ+o0Z+s7vXr1YtWqVVX9tnKOIlp3ZlPjAcTlrSBj4fO06fy26UgiIg3Clvn/oS95pBFG98HXm47j9vQsnnrI+4IpAHQ/+g1Zxw8bTiMiUv/ZTichW8v/h3Bfuxvx9PIynMj9qaDUQ536j2KPR2v8rSISv9KN20REatu2Vd/QzrmXAtubrqMnm45TL6ig1EOWw8GxruVPOW69+z1KS4oNJxIRqd9KfvjpuTtNRxEYqitSa4IKSj3VbdTtnCCQCI6yRU85FhGpNft3byPu50uLhz9gOE39oYJST/n6NSKx+bUA+G14zXAaEZH66+D853BYNlv8+tIyRldO1hQVlHqs3ejfUWx70Kl4O3s3f286johIvZOVeYxuh78EwBqgc09qkgpKPRbevDUJgUMAOL7oRcNpRETqn8R5L9HIKiTF0ZKuF15hOk69ooJSzwUMvheAbpmLOHYo1XAaEZH6o7SkmNbJswE43PU2LIc+UmuSfpr1XOfeF5Po1QVvq4xdXz1vOo6ISL2xZeF7RHCEEwTSfdQdpuPUOyooDUB+rzsB6HhgDoUFeYbTiIjUDz9fgLCjxXX4+jc2nKb+UUFpAOKGjSfdakYI2SR8/R/TcURE3N7ODUvpVLKdYtuTDpfebzpOvaSC0gB4enmT2q78oVVh297AWeY0nEhExL3lLnkBgITgYTSNbGk4Tf2kgtJAdLnsXvJtH9o697Hp+7mm44iIuK3DB3bTPXspAE2G/M5smHpMBaWBaBzclK0RVwLgsVLP5xERqa69Xz2Hp+Vkq3d3OsQNNB2n3lJBaUBajX6QMtuie9F6kresMh1HRMTt5OVk0vnQpwAU9bnbcJr6TQWlAQlvFUNC4GAATix81mwYERE3tPmrfxNIHgesSHoMHWs6Tr2mgtLABA4pf5BVj8yFHD6wx3AaERH3UVpSQnTSLAAOdZ6Ih4eH2UD1nApKA9Oh58Vs9+6Gl1XGnq+0F0VE5FxtXDCbFnY6mTSm26W/NR2n3lNBaYCK+pb/YXU99Al52ccNpxERcX2200nghn8BkBQ9Ft9GgYYT1X8qKA1Q3JAbSLWaE0AB27962XQcERGXt23VfGJKd1Joe9FxzFTTcRoEFZQGyOHhwcHOkwCI3vk2ZSXFhhOJiLi20u+fB2Bz09E0CWtuNkwDoYLSQPW47G6OEUSEfYTN371tOo6IiMtKSVxPj4LVOG2LqFF/MB2nwVBBaaD8/Buxo2X5JXIBG/6N7dTt70VETuXwt88AsKnxhbRo381wmoZDBaUB6zLmAQpsb9qX7Wbbiq9MxxERcTlH0lLoceJbAPwuecBwmoZFBaUBa9Iski3NLgOg9IcXDacREXE9u798Bm+rjESvrnTqM9R0nAZFBaWBix79e5y2RY/CNezassZ0HBERl5GbfZyuhz4BoLDfFMNpGh4VlAYusm1XNgdcBMDxBc8YTiMi4jq2ffkSARSwz2pB3JAbTMdpcFRQhMBhvwegV9ZCDu5NMpxGRMS8kuJCWu8qv8LxUNc7cOi29nVOBUVo2+Nitvn0wMsqI/Wrp0zHERExLuHrNwjnGEcJpseld5qO0yCpoEi5i8rvjNjjyBccP3zQcBgREXOcZU5CN78KwK42E/D18zecqGFSQREAugwcQ7Jne/ysYnZ+oXNRRKThSlj8EW2d+8izfel6xf2m4zRYVS4oy5cvZ8yYMURFRWFZFnPnzq203LZtpk+fTmRkJH5+fgwbNoxdu3ZVGnP8+HHGjx9PYGAgwcHBTJo0idzc3PNaETk/lsNBdp97Aehy4CPysk8YTiQiUvdspxP/1c8DsDXqOgKDm5kN1IBVuaDk5eURFxfHyy+f+iFzTz31FC+++CKvvPIKq1evplGjRowYMYLCwsKKMePHj2fbtm0sWLCAefPmsXz5cu68U8f4TIsbNoFUK4pA8tj65Qum44iI1LmtK76mU+kOimwv2l/5R9NxGjTLtm272i+2LD777DOuvPJKoHzvSVRUFA8++CC//335lSFZWVmEh4cza9Ysxo4dS2JiIl26dGHt2rX06dMHgPnz5zN69GgOHDhAVFTUWb9vdnY2QUFBZGVlERioR17XpFWfvED/rdM5QhOCpyXi5eNnOpKISJ3ZMvMSuhVtYE3Tq+k35S3Tceqdqnx+1+g5KHv37iU9PZ1hw4ZVzAsKCiI+Pp6VK1cCsHLlSoKDgyvKCcCwYcNwOBysXr36lO9bVFREdnZ2pUlqR4/L7iSDEJpxgs1fvWI6johInUnasIxuRRsotR1Ej/mT6TgNXo0WlPT0dADCw8MrzQ8PD69Ylp6eTlhYWKXlnp6ehISEVIz5tZkzZxIUFFQxRUdH12Rs+QVfXz+S200EIHzLqzhLS80GEhGpI3kLy2+zsDH4N0S2ijGcRtziKp5p06aRlZVVMe3fv990pHqt+xX3kUljWtiHSFjwjuk4IiK1bl/ienrl/4DTtggbpb0nrqBGC0pERAQAGRkZleZnZGRULIuIiODw4cOVlpeWlnL8+PGKMb/m4+NDYGBgpUlqT0BgExKjxwEQuO6f2E6n4UQiIrXr8PwnAUhofCGtOvUynEaghgtKmzZtiIiIYNGiRRXzsrOzWb16NQMGDABgwIABZGZmsn79+ooxixcvxul0Eh8fX5Nx5Dx0ufL35Ns+tC/bTcLST03HERGpNWkpSfTMXABA46G6csdVVLmg5ObmkpCQQEJCAlB+YmxCQgKpqalYlsX999/P3/72N7744gu2bNnCzTffTFRUVMWVPp07d2bkyJHccccdrFmzhh9//JEpU6YwduzYc7qCR+pGUGgEWyOvAsB75bOcx8VeIiIubf+8J/C0nGzx6UXHXoNMx5GfVLmgrFu3jp49e9KzZ08Apk6dSs+ePZk+fToAf/zjH7n33nu588476du3L7m5ucyfPx9fX9+K93jvvffo1KkTQ4cOZfTo0Vx44YW89tprNbRKUlPaXT6NYtuTriXb2Lria9NxRERq3NH0VHoc+RIA66IHDaeRXzqv+6CYovug1J21/5xI36OfscW7B90eXmY6johIjVr16hT6H3qXHZ6diXl4BZbDLa4dcVvG7oMi9U/LK/6PYtuDbsUJJK5ZYDqOiEiNyTyaQbe0OQAU9L9f5cTFaGvIGYVHdyAhdBQAJYufNJxGRKTmJH72JI2sQvZ4tKbHkOtNx5FfUUGRs2p+2Z8ptR10L1zLrg1LTccRETlvWcePEHvg/fJ/952qvScuSFtEzqp5285sCB4OQP7CJwynERE5f9s/e4oAq4C9jlbE/WaC6ThyCioock7CL32YMtsiLn8lKVtXmo4jIlJtWZnH6Lr/PQBO9L0fh4eH4URyKioock5adYxjQ8AlAGTOf9xwGhGR6tv+2VMEkkeKI5oew28xHUdOQwVFzlnIyIdx2hY9cpeTmrjWdBwRkSrLyTpOl33vAnC09++098SFqaDIOWsX25eNjS8C4MjXfzecRkSk6rZ99gxB5LHP0YKeI241HUfOQAVFqiR45P8B0DN7KXt3bDScRkTk3OVln6BTytsAHOlxLx6enoYTyZmooEiVtOvWnwT/C3BYNke++pvpOCIi52zL3H8QTC6pVhQ9Rk0yHUfOQgVFqixo5MMA9M5exN5E7UUREdeXn5tJzJ7yvScZcffi6eVlOJGcjQqKVFmb7heyqdEFeFg2R7961HQcEZGz2vLZczQhmwNWJD0vvd10HDkHKihSLUGjZgDQO2cpe7atMRtGROQM8nKy6LD7TQDSuk3G08vbcCI5FyooUi2tY+PZ0PhiHJZN5td/NR1HROS0Nn32D0LI5qAVTs8xd5mOI+dIBUWqLfTS6Thti155y9m9eYXpOCIiJ8nOOk7nPeV7Tw71+B1e2nviNlRQpNpade7DhsAhAOTOf8xwGhGRk2397xM0Iaf8vieXau+JO1FBkfPS7LLpFc/o2Z2w3HQcEZEKWccy6LbvHQCO9pmq+564GRUUOS+tYnqwPuinJx1/q70oIuI6tv/3cQKsAvY4WtNzxETTcaSKVFDkvIVfPp1S20G3grXsWrfQdBwREY5lHCDu4AcAZA34o56544ZUUOS8tWofy/omowAoWqC7y4qIebs+/Rv+VhE7PTvSY+g403GkGlRQpEZEXzmDYtuD2KKNbP3xK9NxRKQBO3JwLz3SPwGg4MI/YTn0UeeOtNWkRkS1jiGh2eUAWEv/ju10Gk4kIg3Vns8ew9cqYbtXLN0HXWU6jlSTCorUmLZXT6fI9qJryTY2Lvmv6Tgi0gAd2pdEzyOfA+Ac/H/ae+LGtOWkxjSNasumqOsACPzxccrKygwnEpGG5sDcR/G2ytjs04vYC0abjiPnQQVFalSnax8hBz/aO/ew4es3TccRkQZk385N9Dz+DQBev/mz4TRyvlRQpEYFhoazrfVEACI3PENJcaHZQCLSYBz9/M94Wk4S/AfQuc9Q03HkPKmgSI3rfu2fOEowLex0Nsx90XQcEWkAdqxfQu+85Thti+DL9ADT+kAFRWqcf+NgkjvdA0C77S9TmJdtOJGI1Ge200nptzMAWN9kJK279DWcSGqCCorUip5X/Y6DVjhNyWTTf580HUdE6rHNyz4jtngTRbYXLa/W3pP6QgVFaoWPjx8H4h4AoMueN8k+fthwIhGpj8rKymj8fXkp2Rh5HeEtOxhOJDVFBUVqTZ8xd7Lb0ZoA8tk+Rw8SFJGat+Gr12nn3EsOfnS+bobpOFKDarygtG7dGsuyTpomT54MwODBg09advfdd9d0DHEBHh4e5FzwMAA90j7k0IE9hhOJSH1SVJhP843/AGBbm9sICo0wnEhqUo0XlLVr13Lo0KGKacGCBQBcd911FWPuuOOOSmOeeuqpmo4hLiLukutI9I7F1yoh5ZPppuOISD2y8dPniLIPc4QmxF07zXQcqWE1XlCaNWtGRERExTRv3jzatWvHxRdfXDHG39+/0pjAwMCajiEuwnI48Bz+CAB9T3xF8rZ1ZgOJSL2Qk3WcmJ2vALA39l78GgUYTiQ1rVbPQSkuLmb27NncdtttWJZVMf+9996jadOmxMbGMm3aNPLz88/4PkVFRWRnZ1eaxH106PMbNjW+EE/LSc6XD5uOIyL1wNY5f6cJ2ey3ouh1xb2m40gtqNWCMnfuXDIzM5k4cWLFvBtvvJHZs2ezZMkSpk2bxrvvvsuECRPO+D4zZ84kKCioYoqOjq7N2FILml35BCW2Bz0LV7N5+Rem44iIGzt6KJW4/e8CcCT+ITy9vA0nktpg2bZt19abjxgxAm9vb7788svTjlm8eDFDhw4lOTmZdu3anXJMUVERRUVFFV9nZ2cTHR1NVlaWDg+5kdUv3078kTkke7Sj7cNrcXh4mI4kIm5o9QsTiD/xJUmeMXR8eJWeWOxGsrOzCQoKOqfP71rbqvv27WPhwoXcfvvtZxwXHx8PQHJy8mnH+Pj4EBgYWGkS9xNz/V/LHyRYtpsN814xHUdE3NCeravpc3weAPbwv6uc1GO1tmXfeustwsLCuPTSS884LiEhAYDIyMjaiiIuIrhZJNva3gFA9MZ/UJifYziRiLgT2+kk78s/4WHZbGh8MZ36/cZ0JKlFtVJQnE4nb731Frfccguenp4V83fv3s1f//pX1q9fT0pKCl988QU333wzgwYNonv37rURRVxMj+v+RDpNCecYm+bMNB1HRNzI5mX/pVvRBoptT8Kv1iM06rtaKSgLFy4kNTWV2267rdJ8b29vFi5cyPDhw+nUqRMPPvgg11xzzRnPUZH6xdevEak9/wBA7N43yDx8wHAiEXEHpSXFBH//KADrI6+nedvOhhNJbavVk2RrS1VOshHX4ywrI/nxeDqW7WJN06voN2WW6Ugi4uLWfPwU/bb/nRME4LgvgaCQpqYjSTW4xEmyIqfj8PCgeGj5s3l6HfmclB0bDCcSEVeWk3mM9ttfAiApZrLKSQOhgiJGxA4czUb/C/C0nGTO/RNuuCNPROrI9o8fIYRsUq3m9Lp6quk4UkdUUMSYsKvLb97Wo3A1m5Z+YjqOiLigjH1J9Dj4AQBHB/4Zbx8fw4mkrqigiDHN23dnQ+T1ADRZ/gjFv7gZn4gIwIFP/oSPVcIW7x70HDrWdBypQyooYlSXsX/jOIG0sg+w4RNdNigi/7Nt1bf0zlmM07bwu2ymbsrWwGhri1EBwU1J7lZ+TLnLrn9z4vBBw4lExBWUlZbis2AaAOtDL6N994GGE0ldU0ER43pfcS/JHu0IJJ/kjx4yHUdEXMDaz16gfdlusvGn/binTMcRA1RQxDgPT0+Khj0OQO+j89i7dYXhRCJiUtbxw8Rsew6AxJgpNGkWZTiRmKCCIi6h64CRrG18CQ7LpuiL32M7naYjiYghiR9Mowk5pDha0vua35uOI4aooIjLaH7d0xTY3nQq3sbGb94wHUdEDNizdTV9Dn8KQN6Qv+HprcuKGyoVFHEZUa06kNBqIgDN184kLzfbbCARqVO200n+F3/A03KyofEgul54helIYpAKiriUnmNncMhqRjjHSPhghuk4IlKHNnz7NrHFmyi0vYi87hnTccQwFRRxKb7+jTnS/y8A9DnwLvt2bTGcSETqQkFeDs1X/w2AhJYTiWwVYziRmKaCIi6n+/Cb2erbGx+rhMz/3q8TZkUagIQPHiGCo6TTjB7jtPdUVFDEFVkWTa59nmLbk7jCdWz47l3TiUSkFqXu2kzv/bMAOBT/f/j6B5gNJC5BBUVcUvP23dkQfTMALVY9Sn5uptlAIlIrbKeTzE/ux9sqZbNvH3qMuMV0JHERKijisnrc+BhpVhjhHGPLe382HUdEasH6b96ie9F6imwvQq59Qc/bkQr6TRCX5esfQMbARwHolfY++5M2GE4kIjUpJ+s4rdb+FYCNrW6lRftYw4nElaigiEvrMWwcG/wG4GWVkfvp73TCrEg9su29h2jGCQ5YkfQY94jpOOJiVFDEpVmWRdh1z1Nge9O5aDMbvn7ddCQRqQG7N/9I34w5ABwfPBNfv0aGE4mrUUERl9eibSc2tr4dgFbrHifr+FHDiUTkfDjLyij74gE8LJt1jS+h+8VXmY4kLkgFRdxC73F/Yb+jOU3JJPG9B03HEZHzsOGz5+hYmkSu7UfLcc+bjiMuSgVF3IKPrz+5w54GoP+xuWxf/Z3hRCJSHUcy9tNh67MAbO44hbDmrc0GEpelgiJuo/PAS1nXZDQAjb6dSlFhvuFEIlJVKbPvI4g8kj3a0e/6P5qOIy5MBUXcSscJL3CMIFo597Pxfd0OW8SdbFz4IX1zFlNmWzDmBTy9vE1HEhemgiJuJTA0jL19yh8m2Gvfm+xP2mg4kYici5ys40T98DAAa6NupH2PiwwnElengiJup/foSST49sPbKiXvv5OxnWWmI4nIWWx79/eEc4yDVjhxE540HUfcgAqKuB3L4SBs3Mvk2T50Kt7G+k+fMx1JRM5gx5oF9DvyKQAnhjyNXyM9DFDOTgVF3FJUq45s6ngfADFbnuHwwRSzgUTklIoK8/Gb/wAOy2ZN8GhiL7rCdCRxEyoo4rbib/gTOz07EmAVsP+9ydi2bTqSiPzKxvem08q5n2MEEXPT86bjiBtRQRG35eHpiffV/6TE9qB3/g+s/epN05FE5Bf2Ja6jV2r532VKvxkEhYYbTiTupMYLyiOPPIJlWZWmTp06VSwvLCxk8uTJhIaG0rhxY6655hoyMjJqOoY0EK27xJPQaiIA7dfN4GjGAbOBRASA0pISCv/7W7ytMjb6D6DXyFtNRxI3Uyt7ULp27cqhQ4cqph9++KFi2QMPPMCXX37JnDlzWLZsGWlpaVx99dW1EUMaiB4THmevR2tCyCH13XtMxxERYM0HjxFTmkQOfkTd+DKWQzvspWpq5TfG09OTiIiIiqlp06YAZGVl8cYbb/Dss88yZMgQevfuzVtvvcWKFStYtWpVbUSRBsDL2xfnFS9TajvolbucDd/oUI+ISXu3r6PP7n8BsCPu/whv0c5wInFHtVJQdu3aRVRUFG3btmX8+PGkpqYCsH79ekpKShg2bFjF2E6dOtGyZUtWrlx52vcrKioiOzu70iTyS+26X8ja6PJdyG1WzyDz8EHDiUQappLiIkr/exfeVimb/OLpc8Vk05HETdV4QYmPj2fWrFnMnz+ff//73+zdu5eLLrqInJwc0tPT8fb2Jjg4uNJrwsPDSU9PP+17zpw5k6CgoIopOjq6pmNLPdDrpr+z29GaJmST8u7doKt6ROrcuvdm0KEsmWwaEXXTazq0I9VW4785o0aN4rrrrqN79+6MGDGCr7/+mszMTD7++ONqv+e0adPIysqqmPbv31+DiaW+8PHxo3TMy5TYHvTI0aEekbq2e8tqeqe8BsDOXn+hWVRrs4HErdV6tQ0ODqZjx44kJycTERFBcXExmZmZlcZkZGQQERFx2vfw8fEhMDCw0iRyKjE9f3GoZ80MjhxKNZxIpGEoLiqEuXfjbZWR4D+Q3pfdZTqSuLlaLyi5ubns3r2byMhIevfujZeXF4sWLapYnpSURGpqKgMGDKjtKNJA9Lnp7+zxaEMTckh9525sp9N0JJF6b/3sP9OubA8nCKDFzTq0I+evxn+Dfv/737Ns2TJSUlJYsWIFV111FR4eHowbN46goCAmTZrE1KlTWbJkCevXr+fWW29lwIAB9O/fv6ajSAPl7eOLx9WvUmx70LvgR1Z9+qLpSCL12s6N39Pnpxuy7en7CE0jdJ6gnL8aLygHDhxg3LhxxMTEcP311xMaGsqqVato1qwZAM899xyXXXYZ11xzDYMGDSIiIoJPP/20pmNIA9eqazwJHaYA0H3L4xzYvc1wIpH6KT83C98v7sTLKmND44vpNeo205GknrBsN3yASXZ2NkFBQWRlZel8FDktZ2kpO566hC7Fm0ny7ET7P/2Ah6eX6Vgi9crqF28m/vjnHCYEn3tXEhR6+vMJRary+a2DhFJvOTw9CZnwBjm2HzGlO1g3+8+mI4nUKwkL3if++OcAHB76vMqJ1CgVFKnXIlp2ZHuvRwDovfc1dm9cZjSPSH1xND2VVj8+BMCq8HHEXnSF4URS36igSL3Xb8ydrG18CZ6WE58v7iIvJ9N0JBG3ZjudpM26jSZks8ejNT1vfdZ0JKmHVFCk3rMcDjre9hoZhNLCPsSWN6eYjiTi1tZ8/CTdC9dSZHvhuPYNfHz9TUeSekgFRRqEoJAwjv3mBQD6n/iStV/PMhtIxE3tS1xPXOI/ANjY6QFad+5jOJHUVyoo0mB0uWAMa5vfDEDM6mmkpSQZTiTiXgrzcymbcxu+VgmbffsQf8M005GkHlNBkQal5y3PkOTViUArn5zZN1NSXGQ6kojb2PSfe2jrTOEYQUTd8qbuFiu1Sr9d0qB4evsQOOEdsmlETOkONsx60HQkEbew7qvXiT/+BU7bIm3ISzSNbGU6ktRzKijS4ES2imFX/EwA4tPeZfuyTwwnEnFtB5K30HlN+X2E1kRPpNsgXVIstU8FRRqk3qNuYWXo1QBELnmAo2kpZgOJuKiiwjwKP7iFRlYh271i6XPLU6YjSQOhgiINVo9J/yTZoy1NyCZ91s2UlpSYjiTichLeuI/2Zbs5QQCht7yDp5e36UjSQKigSIPl598In7GzyLd9iC3exKpZD5mOJOJSEr59m/gj5YdAUwf9g/AW7QwnkoZEBUUatOgOcST1+xsAFx58g40LPzScSMQ17E/eQrsV5aV9ZcR44obcYDiRNDQqKNLg9bz0TtY0uwaAdj9M5eCeRMOJRMzKz82i9P3xBFgF7PDqQp/bnjMdSRogFRQRoMft/yLJsxOB5FH43o0U5ueajiRihO10sv3VW2nj3MdRgml664d4efuYjiUNkAqKCODt40vQLe9znEDale1h2+uTwLZNxxKpc2s+epw+OYsosT04MupVmkbpfidihgqKyE8iotuxf+jLlNkWvU/MZ80n/zAdSaROJa6eT68d5U8mXh8zlc7xIw0nkoZMBUXkF+IuupzVbe8FoMfWx0lcu8hwIpG6cTQthbBv7sLLKmNtwDDixz5sOpI0cCooIr8y4KZHSWh0Ed5WGSFf3U7GwRTTkURqVVFhPkffGksomexxtKLLnXrOjpin30CRX7EcDjre/Q77HNGEc5yst66jsCDPdCyRWmE7nWx6ZRKdShLJxh/Pce/RKCDIdCwRFRSRU/EPCMF7wkdk0YiOpTvZ8spEbKfTdCyRGrf6g7/RL/NrymyLfYNfomWHbqYjiQAqKCKnFdm2K/uH/ptS20HfrO9Y+/6jpiOJ1KhNS+bQd2f5SbFrYx6k2+BrDScS+R8VFJEziL3oCtZ0+iMAfXa9wObFHxlOJFIz9u3YSNul9+Jh2axpcinxY//PdCSRSlRQRM5iwA0PsarJGByWTdtlv2P39nWmI4mcl+xjGXh+NJYAq4DtXrH0uFsnxYrr0W+kyFlYDge97v4Pid7daGwV4PvxjRzJ2G86lki1lBQXkfra9TS30zlEM8Lv+BhvH1/TsUROooIicg68fXxpfucnpFkRNCeD469fQ0FejulYIlViO50k/OsWYosSyLN9yb/2PULDmpuOJXJKKigi5yiwaQTOGz8mk8bElCaR9K8bcJaWmo4lcs5Wz3qIvpnfUGZb7L74RdrFxpuOJHJaKigiVdCiQxxpI9+kyPaiR96PrP/Pb01HEjkn6z57if6pr5X/O/YvdB9yg+FEImemgiJSRV36jyChz0wA+qZ/xJoP/mY4kciZbVv+GXEJMwBYETWR+OseNJxI5OxUUESqIX7MHfzQ5j4A+ux4hnXfzDIbSOQ0UrauptWieyqesdN/0nOmI4mcExUUkWq64KZHWR16FQ7Lptuq37Plx69MRxKpJC0lCf9PxtLYKmCrd3e6/fZdHB76z764hxr/TZ05cyZ9+/YlICCAsLAwrrzySpKSkiqNGTx4MJZlVZruvvvumo4iUqssh4M+97zOpkYD8bFKaPPdJJI3/WA6lggAR9P3U/b2lYRxnBRHS6Lv/hRfP3/TsUTOWY0XlGXLljF58mRWrVrFggULKCkpYfjw4eTlVX7Y2h133MGhQ4cqpqeeeqqmo4jUOg9PLzrd+wnbvbvT2Cog5LNxHNi12XQsaeCyThwl6/XLibbTSLPCaDTpC4JCmpmOJVIlnjX9hvPnz6/09axZswgLC2P9+vUMGjSoYr6/vz8RERE1/e1F6pyPbyOiJ3/OrheH0aFsN8XvX82R27+lWfN2pqNJA1SQl0Pavy6nc9kejhKMc/xnNGvexnQskSqr9YORWVlZAISEhFSa/95779G0aVNiY2OZNm0a+fn5p32PoqIisrOzK00iriQgKITgO79gn9WcCPsI+W9czrHDaaZjSQNTUlzIrn9eTeeSbWTjT9Y1H9GifazpWCLVUqsFxel0cv/993PBBRcQG/u/P5Ibb7yR2bNns2TJEqZNm8a7777LhAkTTvs+M2fOJCgoqGKKjo6uzdgi1dIsvAVeE+eSQSitnAc49uoYso4fMR1LGojSkhI2vTSO7gVrKLC9OTjqbdp16286lki1WbZt27X15vfccw/ffPMNP/zwAy1atDjtuMWLFzN06FCSk5Np1+7k3eJFRUUUFRVVfJ2dnU10dDRZWVkEBgbWSnaR6tq/M4HG74+hCdns8Iwh6t75BAaFnP2FItVUVlrK+hfH0S/7O4ptDxIHv0bcJdeajiVykuzsbIKCgs7p87vW9qBMmTKFefPmsWTJkjOWE4D4+PLbLScnJ59yuY+PD4GBgZUmEVcV3bEHWdd/QiaN6VSaRNpLo8nNPmE6ltRTZWVlrHtpPP2yv6PUdrB94HMqJ1Iv1HhBsW2bKVOm8Nlnn7F48WLatDn7yVkJCQkAREZG1nQcESNad4nn2NVzyKIRnUoTOfDSaPJUUqSGOcvKWPfSBOKz5lNqO9gy4Fl6jLjFdCyRGlHjBWXy5MnMnj2b999/n4CAANLT00lPT6egoACA3bt389e//pX169eTkpLCF198wc0338ygQYPo3r17TccRMaZd94FkXPERWXYjOpVsZ99Ll5GjkiI1xFlWxrp/3kx85teU2Rab45+h58hbTccSqTE1fg6KZVmnnP/WW28xceJE9u/fz4QJE9i6dSt5eXlER0dz1VVX8ec///mcD92c6zGssrIySkpKqrUeDYWXlxceHh6mY9RrSeuXEvXlWAIoYKtXLNGTvyQoWOekSPWVlZWx7p8TiT/xBWW2RULfp+l92R2mY4mcVVXOQanVk2Rry9lW0LZt0tPTyczMrPtwbig4OJiIiIjTlks5f8kblhL+RXlJSfLsSMQ98wgKDTcdS9xQSUkxCS+Oo2/OQpy2xYbeM+lz+T2mY4mck6oUlBq/UZsr+LmchIWF4e/vrw/e07Btm/z8fA4fPgzoHKDa1L7XYPZ4zKHss7HElO5kz8vDKbtrHiHhumRezl1RYT7bX7yWvvk/UmJ7sCX+afqMnmQ6lkitqHcFpaysrKKchIaGmo7j8vz8/AA4fPgwYWFhOtxTi9rGXUSK11xKP76Wts4U9r86AuekL2mqO87KOSjIzSb5n1fQs3ADRbYXOy56iV7DxpmOJVJr6t1jLX8+58TfXw/FOlc//6x0vk7ta92lL/njv+QQTYl2HqTkPyPYn7zVdCxxcdmZx9j3wgi6FW4gz/Zh57A3iFM5kXqu3hWUn+mwzrnTz6putezQHefEb9hvRRFpH8F/9mh2bVxuOpa4qMMHUzjy4lA6lWwny27Evkvfp9tFV5iOJVLr6m1BEXFlzVt3xO+ub9nt0ZZQsmg+91q2LP3EdCxxMfsS1+F8fSjtnHs5SjCHr/6ELv2GmY4lUidUUEQMaRrRkvDfLWKLTy/8rSI6L7mDDXNfMB1LXMT2FV/T5KPLieAoqY7mFN/yLR3iBpqOJVJnVFBcyMSJE7EsiyeeeKLS/Llz51Ychlm6dCmWZdG1a1fKysoqjQsODmbWrFl1FVdqQOPAEGKmzmd14Ag8LSe9Eqaz8j8P4ixzmo4mBq3/+g3af3sTgeSxw6sLgb9dTFSbTqZjidQpFRQX4+vry5NPPsmJE2e+4+iePXt455136iiV1CZvHx/6/u5DfowqvwvogAP/Yc3zN1CQn2c4mdQ12+nkh7em0XvNVLytUjY2upDWDywguGmE6WgidU4FxcUMGzaMiIgIZs6cecZx9957LzNmzKj0lGdxXw4PBxfc+Tzru02n1HbQP+c79j07hCNp+0xHkzpSkJfLuueu4cJ9/wJgddj1dH/gc3z9GxtOJmJGgygotm2TX1xa51N1btLr4eHB448/zksvvcSBAwdOO+7++++ntLSUl1566Xx+NOJiel/zIMkj3vnpIYM7sF8bTHKCrvCp7w4f3MuBZy+mb85iSmwP1sX+hfjfvo6HZ727VZXIOWsQv/0FJWV0mf5tnX/f7Y+NwN+76j/iq666ih49ejBjxgzeeOONU47x9/dnxowZPPzww9xxxx0EBQWdb1xxEZ0GjiEtvC2Z791AK+d+Aj+7mvUHZ9L7Uj1rpT7atX4JTb6cSAcyOUEAh0a8Rp+Bo03HEjGuQexBcUdPPvkkb7/9NomJiacdM2nSJEJDQ3nyySfrMJnUhah2XWly3zISfPvha5XQe+3vWfWvOykuKjQdTWqI7XSy+uMnafXFtTQlk72OVhTcspAuKiciQAPZg+Ln5cH2x0YY+b7VNWjQIEaMGMG0adOYOHHiKcd4enry97//nYkTJzJlypRqfy9xTYHBocQ++DUr35rKgLR36H/4I5Ke2UTILe/RrEV70/HkPORlnyDp9duIz1kMFmzwv5AOd71LQJCeci3yswZRUCzLqtahFtOeeOIJevToQUxMzGnHXHfddTz99NM8+uijdZhM6oqnlxcD7nyJjQv60+6HPxBTsoPM/1zMtsEv0HXwtabjSTWkJq6FObfQy3mQEtuD9R1+R/yNf8FyaIe2yC/pL8KFdevWjfHjx/Piiy+ecdwTTzzBm2++SV6eLkutr3r+ZjzZtyxip0d7gsml69JJ/PjKFIqKCkxHk3NkO52s+uR5mn04mpbOg2QQwq7RH9J/wgyVE5FT0F+Fi3vsscdwOs98064hQ4YwZMgQSktL6yiVmNCibWeiH/yeVaFXAXBB+rukPnUh+5I2Gk4mZ3P8cBobn7mM/ltn4GcVs8WnFx73/ECX+OGmo4m4LMuuzrWwhmVnZxMUFERWVhaBgYGVlhUWFrJ3717atGmDr6+voYTuRT8z97Pp21m0XvkwQeRRYHuzpcvv6XvdH/R/4i5o05JPaL7sQZqSSbHtwcb2U+g7bjoOXUIsDdCZPr9/Tf81E3FDcSMmUnLHj2zx6YWfVUy/xMfZ/NRwDu3fbTqa/CQ3+wSr/nkrccsm0ZRMUhzR7L/mK+JvekzlROQcqKCIuKmmzdvQ9Y8LWdnxDxTZXsQVrqXxfy5g1cdP4/zVc5qkbm1aPIfcZ/vQ/+inAKxqdh0Rv19Fu+4DDCcTcR8qKCJuzOHhwYAb/0zGjd+x0yuGAKuA/tv/RtITF3FwV4LpeA3OiSNprHv2GuKW304ERzlkhbF16Dv0n/wf3bJepIpUUETqgZYxvWj/0ApWxTxEnu1D55JtNJs9lBVv/IGCvFzT8eo9Z1kZaz59AV6Op0/2Qspsi1Xh4wh6cB2xF11hOp6IW1JBEaknHJ6e9B/3MJkTv2eTTx+8rVIG7n+NzKd7sHH+W9hnuRpMqidxzSKSH+9Pv83TaUI2exyt2X3FXPrf8wr+jfUICpHqUkERqWeat4mh+0ML2NjvWTIIJZIj9Fx1P4lPDiZ5yyrT8eqNjIMprH72ejp/fTUdy3aSa/uxuv0DtHhoNR17DTYdT8TtqaCI1EOWw0HP0ZMI/H0CK1vcTqHtRZeiTbT5ZCSrnxtLWkqS6YhuKzvzCKtev4+A1/oRn13+ENJ1TUZTeM8a4ic8grePLtUXqQkqKCL1mF/jQAbc/g+OTfyRjY0vxsOyic/6hqZvDWD1y7dzImO/6YhuozAvmzXv/Bmej6P/wbfxt4pI8upE8uVf0Od3H9A0oqXpiCL1ii7GF2kAmreJofnvv2D3xqUUzH+E2KKNxB+ZQ/6/vmBVxNW0u/whmjVvYzqmS8rJOs62z5+jw5536EcmAHsdLTnR/yF6DrtRN8cTqSX6y6rnUlJSsCyLhIQE01HEBbTrOZjYaUvZPPRdEj1i8LeK6J/xAUGv9WbtCzeSujPBdESXcSQ9lZWv3Yf9XFf673mRUDJJI4zVPR6n5cMb6TV8gsqJSC3SHhSRBqj7RZdjX3AZm5Z+gtfKF+lSsoW+J77C+d7XbPDvD33vIO7iK/Hw8DAdtc4lbfyezGX/oseJBQywSgDY52hBRre76TH6DqJ0jolInVBBcWHFxcV4e3ubjiH1lOVwEDfkehhyPYlrFlGw9B/0yv+RXgUrYflKDnz/MPvb3kDnkXcT3CzKdNxaVZify+Zv3yJ42zvElO4sn2nBTs+O5PX7HXFDx9GqAZY1EZO0f9KFDB48mClTpnD//ffTtGlTRowYwdatWxk1ahSNGzcmPDycm266iaNHj1a8Zv78+Vx44YUEBwcTGhrKZZddxu7deh6LVE3nfkPp9cevSRu/nLXh15GDHy3sdAbsfgH/f8aS8NQoEr55k8L8+nPTN2dpKdt/+Jx1z99A6ZPt6bfpz3Qs3Umx7cn6wGHsuvQTOv7fGnoOn4BD5USkzjWMPSi2DSX5df99vfzBsqr0krfffpt77rmHH3/8kczMTIYMGcLtt9/Oc889R0FBAQ899BDXX389ixcvBiAvL4+pU6fSvXt3cnNzmT59OldddRUJCQk4dHxcqiiqQxxRHf5DQW42a799k+Dt79KhLJke+Stg9QryVk9jXdDFeHS9nI4DLqNRQLDpyFVSXFTErjXzyd0yj7aHF9CFE+ULLEizwtjX+npiRt5D7/AWZoOKCJZt27apb/7yyy/z9NNPk56eTlxcHC+99BL9+vU76+vO9LjmwsJC9u7dS5s2bfD1/elYcXEePG5gF/XDaeDd6JyHDx48mOzsbDZs2ADA3/72N77//nu+/fbbijEHDhwgOjqapKQkOnbseNJ7HD16lGbNmrFlyxZiY2NJSUmhTZs2bNy4kR49epzy+57yZybykz1bV5O+4j3apH1NJEcq5hfbnuzw60FB62GE9xhFq47dXfKk0fT9u9m//lus5O+IyVlFgFVQsSyTxuwIGUpA3xvp3O832lMiUsvO9Pn9a8b2oHz00UdMnTqVV155hfj4eJ5//nlGjBhBUlISYWFhpmIZ17t374p/b9q0iSVLltC48ckPGdu9ezcdO3Zk165dTJ8+ndWrV3P06FGcP93OPDU1ldjY2DrLLfVX29h42sbG4yx7ni1rFpK3cQ7RR5bRnAy6F66DHetgxxMcI4jUxnEUN+9PQIeBtOrUh0aNA+o0a3FxMak7N3J85yrYt5IW2RuIsjOI+HmABccIYk/wBXh0vYwuF11Nf1+/Os0oIufGWEF59tlnueOOO7j11lsBeOWVV/jqq6948803+dOf/lSz38zLv3xvRl3z8q/ySxo1+t8el9zcXMaMGcOTTz550rjIyEgAxowZQ6tWrXj99deJiorC6XQSGxtLcXFx9XOLnILDw0G3AcNhwHBsp5OUnQkcWjOXoANLaFeUSKiVRWjuckhaDklQ9qVFikdzjjaKoSQ0Bq9m7QiKbE94y04EhoZX+fDnLxXkZJKxL4kTB5MoOpyMdXw3TbKTaFWaQvufrrz5WantYI9Xe46HD6RJz8vp0HMwfbWnRMTlGSkoxcXFrF+/nmnTplXMczgcDBs2jJUrV540vqioiKKiooqvs7Ozq/YNLatKh1pcRa9evfjvf/9L69at8fQ8eVMdO3aMpKQkXn/9dS666CIAfvjhh7qOKQ2Q5XDQulMvWnfqBUBRYT6Jm74nc/tSGh9eS4uCJJpY2bR2HqB1zgHIWQQp/3t9ge1NliOIXI8gCj0DKfQMxOnwxnZ4YXt4YePAUVaM5SzGw1mEZ1khfiUnCCjLJMjOwp8iWgOtTwoGufix37sdWU174d/hYtr2HkrHwCZ18nMRkZpjpKAcPXqUsrIywsPDK80PDw9nx44dJ42fOXMmjz76aF3FcxmTJ0/m9ddfZ9y4cfzxj38kJCSE5ORkPvzwQ/7zn//QpEkTQkNDee2114iMjCQ1NbXm9z6JnAMfX386x4+A+BHlM2ybo+n7OLhjLfmpG/E8nkzj/AOElBwinOP4WcX42Ueg9AiUVu97niCAw56R5PhFUxrUGp/m3QiPiSeydQydHdpDIuLu3OIqnmnTpjF16tSKr7Ozs4mOjjaYqG5ERUXx448/8tBDDzF8+HCKiopo1aoVI0eOxOFwYFkWH374Iffddx+xsbHExMTw4osvMnjwYNPRpaGzLJpGtqZpZGvgukqL8vJyOZ6eSu6JdAqyjlKSfQRHUSbOslIoK4bSIrCd4OEDnj5Ynt7YXn54BTTDNyicRiHhBDVtTpMmoWi/iEj9ZaSgNG3aFA8PDzIyMirNz8jIICIi4qTxPj4++Pj41FU8Y5YuXXrSvA4dOvDpp5+e9jXDhg1j+/btleb98sKs1q1bY/BCLZGTNGrUmEbtugBdTEcRERdm5JpAb29vevfuzaJFiyrmOZ1OFi1axIABA0xEEhERERdi7BDP1KlTueWWW+jTpw/9+vXj+eefJy8vr+KqHhEREWm4jBWUG264gSNHjjB9+nTS09Pp0aMH8+fPP+nEWREREWl4jJ4kO2XKFKZMmWIygoiIiLgg17svtYiIiDR49bag/HzLdzk7/axERMTVuMV9UKrC29sbh8NBWloazZo1w9vbG+s8bqldn9m2TXFxMUeOHMHhcODt7W06koiICFAPC4rD4aBNmzYcOnSItDQDz99xQ/7+/rRs2RKHCz6JVkREGqZ6V1CgfC9Ky5YtKS0tpayszHQcl+bh4YGnp6f2MomIiEuplwUFwLIsvLy88PLyMh1FREREqkj79EVERMTlqKCIiIiIy1FBEREREZfjlueg/Px03uzsbMNJRERE5Fz9/Ln98+f4mbhlQcnJyQEgOjracBIRERGpqpycHIKCgs44xrLPpca4GKfTSVpaGgEBATV6eWx2djbR0dHs37+fwMDAGntfV1Lf17G+rx/U/3XU+rm/+r6O9X39oPbW0bZtcnJyiIqKOuu9t9xyD4rD4aBFixa19v6BgYH19pfuZ/V9Hev7+kH9X0etn/ur7+tY39cPamcdz7bn5Gc6SVZERERcjgqKiIiIuBwVlF/w8fFhxowZ+Pj4mI5Sa+r7Otb39YP6v45aP/dX39exvq8fuMY6uuVJsiIiIlK/aQ+KiIiIuBwVFBEREXE5KigiIiLiclRQRERExOU0uILy97//nYEDB+Lv709wcPApx6SmpnLppZfi7+9PWFgYf/jDHygtLT3j+x4/fpzx48cTGBhIcHAwkyZNIjc3txbWoGqWLl2KZVmnnNauXXva1w0ePPik8XfffXcdJj93rVu3PinrE088ccbXFBYWMnnyZEJDQ2ncuDHXXHMNGRkZdZT43KWkpDBp0iTatGmDn58f7dq1Y8aMGRQXF5/xda6+/V5++WVat26Nr68v8fHxrFmz5ozj58yZQ6dOnfD19aVbt258/fXXdZS06mbOnEnfvn0JCAggLCyMK6+8kqSkpDO+ZtasWSdtL19f3zpKXDWPPPLISVk7dep0xte40/Y71X9PLMti8uTJpxzvDttu+fLljBkzhqioKCzLYu7cuZWW27bN9OnTiYyMxM/Pj2HDhrFr166zvm9V/46rqsEVlOLiYq677jruueeeUy4vKyvj0ksvpbi4mBUrVvD2228za9Yspk+ffsb3HT9+PNu2bWPBggXMmzeP5cuXc+edd9bGKlTJwIEDOXToUKXp9ttvp02bNvTp0+eMr73jjjsqve6pp56qo9RV99hjj1XKeu+9955x/AMPPMCXX37JnDlzWLZsGWlpaVx99dV1lPbc7dixA6fTyauvvsq2bdt47rnneOWVV3j44YfP+lpX3X4fffQRU6dOZcaMGWzYsIG4uDhGjBjB4cOHTzl+xYoVjBs3jkmTJrFx40auvPJKrrzySrZu3VrHyc/NsmXLmDx5MqtWrWLBggWUlJQwfPhw8vLyzvi6wMDASttr3759dZS46rp27Vop6w8//HDase62/dauXVtp3RYsWADAddddd9rXuPq2y8vLIy4ujpdffvmUy5966ilefPFFXnnlFVavXk2jRo0YMWIEhYWFp33Pqv4dV4vdQL311lt2UFDQSfO//vpr2+Fw2Onp6RXz/v3vf9uBgYF2UVHRKd9r+/btNmCvXbu2Yt4333xjW5ZlHzx4sMazn4/i4mK7WbNm9mOPPXbGcRdffLH9u9/9rm5CnadWrVrZzz333DmPz8zMtL28vOw5c+ZUzEtMTLQBe+XKlbWQsGY99dRTdps2bc44xpW3X79+/ezJkydXfF1WVmZHRUXZM2fOPOX466+/3r700ksrzYuPj7fvuuuuWs1ZUw4fPmwD9rJly0475nT/PXJFM2bMsOPi4s55vLtvv9/97nd2u3btbKfTecrl7rTtbNu2Afuzzz6r+NrpdNoRERH2008/XTEvMzPT9vHxsT/44IPTvk9V/46ro8HtQTmblStX0q1bN8LDwyvmjRgxguzsbLZt23ba1wQHB1faIzFs2DAcDgerV6+u9cxV8cUXX3Ds2DFuvfXWs4597733aNq0KbGxsUybNo38/Pw6SFg9TzzxBKGhofTs2ZOnn376jIfk1q9fT0lJCcOGDauY16lTJ1q2bMnKlSvrIu55ycrKIiQk5KzjXHH7FRcXs379+ko/e4fDwbBhw077s1+5cmWl8VD+N+kO2wrKtxdw1m2Wm5tLq1atiI6O5oorrjjtf29cwa5du4iKiqJt27aMHz+e1NTU04515+1XXFzM7Nmzue222874YFp32na/tnfvXtLT0ytto6CgIOLj40+7jarzd1wdbvmwwNqUnp5eqZwAFV+np6ef9jVhYWGV5nl6ehISEnLa15jyxhtvMGLEiLM+bPHGG2+kVatWREVFsXnzZh566CGSkpL49NNP6yjpubvvvvvo1asXISEhrFixgmnTpnHo0CGeffbZU45PT0/H29v7pHOQwsPDXW57/VpycjIvvfQSzzzzzBnHuer2O3r0KGVlZaf8G9uxY8cpX3O6v0lX31ZQ/uT1+++/nwsuuIDY2NjTjouJieHNN9+ke/fuZGVl8cwzzzBw4EC2bdtWqw9GrY74+HhmzZpFTEwMhw4d4tFHH+Wiiy5i69atBAQEnDTenbff3LlzyczMZOLEiacd407b7lR+3g5V2UbV+TuujnpRUP70pz/x5JNPnnFMYmLiWU/kcifVWecDBw7w7bff8vHHH5/1/X95/ky3bt2IjIxk6NCh7N69m3bt2lU/+DmqyvpNnTq1Yl737t3x9vbmrrvuYubMmS57K+rqbL+DBw8ycuRIrrvuOu64444zvtb09pNykydPZuvWrWc8RwNgwIABDBgwoOLrgQMH0rlzZ1599VX++te/1nbMKhk1alTFv7t37058fDytWrXi448/ZtKkSQaT1bw33niDUaNGERUVddox7rTt3E29KCgPPvjgGRsuQNu2bc/pvSIiIk46E/nnqzsiIiJO+5pfnxhUWlrK8ePHT/ua81WddX7rrbcIDQ3l8ssvr/L3i4+PB8r/D74uPuDOZ5vGx8dTWlpKSkoKMTExJy2PiIiguLiYzMzMSntRMjIyam17/VpV1y8tLY1LLrmEgQMH8tprr1X5+9X19judpk2b4uHhcdIVU2f62UdERFRpvKuYMmVKxQnzVf0/aS8vL3r27ElycnItpas5wcHBdOzY8bRZ3XX77du3j4ULF1Z5r6M7bTv43+daRkYGkZGRFfMzMjLo0aPHKV9Tnb/jaqmxs1nczNlOks3IyKiY9+qrr9qBgYF2YWHhKd/r55Nk161bVzHv22+/damTZJ1Op92mTRv7wQcfrNbrf/jhBxuwN23aVMPJat7s2bNth8NhHz9+/JTLfz5J9pNPPqmYt2PHDpc9SfbAgQN2hw4d7LFjx9qlpaXVeg9X2n79+vWzp0yZUvF1WVmZ3bx58zOeJHvZZZdVmjdgwACXPcnS6XTakydPtqOiouydO3dW6z1KS0vtmJgY+4EHHqjhdDUvJyfHbtKkif3CCy+ccrm7bb+fzZgxw46IiLBLSkqq9DpX33ac5iTZZ555pmJeVlbWOZ0kW5W/42plrbF3chP79u2zN27caD/66KN248aN7Y0bN9obN260c3JybNsu/+WKjY21hw8fbickJNjz58+3mzVrZk+bNq3iPVavXm3HxMTYBw4cqJg3cuRIu2fPnvbq1avtH374we7QoYM9bty4Ol+/01m4cKEN2ImJiSctO3DggB0TE2OvXr3atm3bTk5Oth977DF73bp19t69e+3PP//cbtu2rT1o0KC6jn1WK1assJ977jk7ISHB3r17tz179my7WbNm9s0331wx5tfrZ9u2fffdd9stW7a0Fy9ebK9bt84eMGCAPWDAABOrcEYHDhyw27dvbw8dOtQ+cOCAfejQoYrpl2Pcaft9+OGHto+Pjz1r1ix7+/bt9p133mkHBwdXXDl300032X/6058qxv/444+2p6en/cwzz9iJiYn2jBkzbC8vL3vLli2mVuGM7rnnHjsoKMheunRppe2Vn59fMebX6/joo4/a3377rb179257/fr19tixY21fX19727ZtJlbhjB588EF76dKl9t69e+0ff/zRHjZsmN20aVP78OHDtm27//az7fIP25YtW9oPPfTQScvccdvl5ORUfNYB9rPPPmtv3LjR3rdvn23btv3EE0/YwcHB9ueff25v3rzZvuKKK+w2bdrYBQUFFe8xZMgQ+6WXXqr4+mx/xzWhwRWUW265xQZOmpYsWVIxJiUlxR41apTt5+dnN23a1H7wwQcrteglS5bYgL13796KeceOHbPHjRtnN27c2A4MDLRvvfXWitLjCsaNG2cPHDjwlMv27t1b6WeQmppqDxo0yA4JCbF9fHzs9u3b23/4wx/srKysOkx8btavX2/Hx8fbQUFBtq+vr925c2f78ccfr7S369frZ9u2XVBQYP/2t7+1mzRpYvv7+9tXXXVVpQ99V/HWW2+d8vf1lzs/3XH7vfTSS3bLli1tb29vu1+/fvaqVasqll188cX2LbfcUmn8xx9/bHfs2NH29va2u3btan/11Vd1nPjcnW57vfXWWxVjfr2O999/f8XPIzw83B49erS9YcOGug9/Dm644QY7MjLS9vb2tps3b27fcMMNdnJycsVyd99+tl2+Bxywk5KSTlrmjtvu58+sX08/r4fT6bT/8pe/2OHh4baPj489dOjQk9a9VatW9owZMyrNO9PfcU2wbNu2a+6AkYiIiMj5031QRERExOWooIiIiIjLUUERERERl6OCIiIiIi5HBUVERERcjgqKiIiIuBwVFBEREXE5KigiIiLiclRQRERExOWooIiIiIjLUUERERERl6OCIiIiIi7n/wEcPvbqFUkSoQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pre train image\n",
    "model.eval()\n",
    "\n",
    "torch.save(model.state_dict(),f'./pre_train.pth')\n",
    "\n",
    "V_diag=model(grid)\n",
    "V_real_poten=potential(grid,k)\n",
    "V_diag=V_diag.cpu().detach().numpy()\n",
    "V_real_poten=V_real_poten.cpu().detach().numpy()\n",
    "plt.plot(grid.cpu().detach().numpy(),V_diag,label='NN')\n",
    "plt.plot(grid.cpu().detach().numpy(),V_real_poten,label='real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_loss(model,grid):\n",
    "    loss=0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.requires_grad:\n",
    "            loss+=torch.nn.MSELoss()(param,torch.zeros_like(param))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dd_loss(model,grid):\n",
    "    potential=model(grid)\n",
    "    grad_1=torch.autograd.grad(potential,grid,grad_outputs=torch.ones_like(potential),create_graph=True)[0]\n",
    "    grad_2=torch.autograd.grad(grad_1,grid,grad_outputs=torch.ones_like(potential),create_graph=True)[0]\n",
    "    \n",
    "    dd_loss=torch.nn.MSELoss()(grad_2,torch.zeros_like(grad_2))\n",
    "    \n",
    "    return dd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 0\n",
      "tensor([ 1.0174,  3.0078,  5.0035,  6.9464,  8.9583, 11.0346, 13.0321, 15.0547,\n",
      "        17.0726, 19.0668, 21.0641, 23.0530, 25.0199, 26.9845, 28.9560, 30.9351,\n",
      "        32.9316, 34.9447, 36.9687, 38.9851, 40.9971, 42.9963, 45.0006, 47.0018,\n",
      "        49.0084, 51.0037, 53.0021, 54.9981, 56.9973, 58.9901, 60.9839, 62.9748,\n",
      "        64.9694, 66.9629, 68.9593, 70.9588, 72.9633, 74.9701, 76.9817, 78.9908],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:0,loss:0.0014259801246225834,time:0.0841526985168457,lr:0.01\n",
      "val_loss: tensor(0.0014, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3593, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(31.3733, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 1\n",
      "tensor([ -4.5341,  -1.4562,   1.9475,   5.2752,   8.3827,  11.4502,  14.4858,\n",
      "         17.4275,  20.3579,  23.2370,  26.0746,  28.9064,  31.7141,  34.5282,\n",
      "         37.3577,  40.1758,  42.9946,  45.7756,  48.5382,  51.2852,  54.0169,\n",
      "         56.7325,  59.4211,  62.0965,  64.7562,  67.3974,  70.0216,  72.6315,\n",
      "         75.2293,  77.8080,  80.3761,  82.9307,  85.4726,  88.0011,  90.5208,\n",
      "         93.0279,  95.5257,  98.0152, 100.4950, 102.9661], device='cuda:2',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 2\n",
      "tensor([-8.9667, -6.9502, -4.7062, -2.5024, -0.2858,  2.0034,  4.3176,  6.5894,\n",
      "         8.8831, 11.1719, 13.4367, 15.7076, 17.9732, 20.2339, 22.5138, 24.8070,\n",
      "        27.1137, 29.4331, 31.7395, 34.0401, 36.3302, 38.6203, 40.9107, 43.1937,\n",
      "        45.4722, 47.7482, 50.0257, 52.2992, 54.5675, 56.8324, 59.0928, 61.3462,\n",
      "        63.5928, 65.8363, 68.0747, 70.3086, 72.5400, 74.7703, 77.0011, 79.2341],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 3\n",
      "tensor([-8.1671, -6.7146, -5.0078, -3.2467, -1.3608,  0.6254,  2.6367,  4.6328,\n",
      "         6.6679,  8.7140, 10.7546, 12.8014, 14.8455, 16.8839, 18.9441, 21.0076,\n",
      "        23.0965, 25.2079, 27.3282, 29.4629, 31.5915, 33.7233, 35.8583, 38.0019,\n",
      "        40.1456, 42.2885, 44.4250, 46.5641, 48.7031, 50.8430, 52.9821, 55.1272,\n",
      "        57.2783, 59.4386, 61.6060, 63.7850, 65.9704, 68.1599, 70.3532, 72.5486],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 4\n",
      "tensor([-4.9344, -3.2994, -1.4724,  0.3943,  2.3458,  4.4187,  6.4911,  8.5641,\n",
      "        10.6704, 12.7787, 14.8733, 16.9837, 19.0831, 21.1764, 23.2789, 25.3803,\n",
      "        27.4987, 29.6332, 31.7881, 33.9600, 36.1388, 38.3177, 40.4922, 42.6659,\n",
      "        44.8428, 47.0214, 49.1990, 51.3721, 53.5474, 55.7240, 57.9054, 60.0884,\n",
      "        62.2807, 64.4833, 66.6967, 68.9173, 71.1470, 73.3817, 75.6189, 77.8564],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 5\n",
      "tensor([-1.2359,  0.8169,  2.9738,  5.0969,  7.2475,  9.5057, 11.7440, 13.9813,\n",
      "        16.2409, 18.4915, 20.7276, 22.9739, 25.1996, 27.4227, 29.6408, 31.8516,\n",
      "        34.0720, 36.2947, 38.5388, 40.7941, 43.0664, 45.3384, 47.6102, 49.8705,\n",
      "        52.1285, 54.3869, 56.6470, 58.9037, 61.1555, 63.4054, 65.6571, 67.9099,\n",
      "        70.1637, 72.4331, 74.7016, 76.9711, 79.2604, 81.5585, 83.8616, 86.1639],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 6\n",
      "tensor([ 1.7784,  4.1158,  6.4922,  8.7575, 11.0467, 13.4084, 15.7328, 18.0589,\n",
      "        20.4042, 22.7201, 25.0336, 27.3440, 29.6317, 31.9183, 34.1896, 36.4563,\n",
      "        38.7203, 40.9867, 43.2631, 45.5503, 47.8542, 50.1620, 52.4730, 54.7734,\n",
      "        57.0679, 59.3574, 61.6481, 63.9364, 66.2198, 68.4981, 70.7764, 73.0564,\n",
      "        75.3382, 77.6200, 79.9100, 82.2078, 84.5164, 86.8309, 89.1544, 91.4820],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 7\n",
      "tensor([ 3.4270,  5.7286,  8.0177, 10.1843, 12.4212, 14.7062, 16.9560, 19.2137,\n",
      "        21.4918, 23.7351, 25.9843, 28.2234, 30.4443, 32.6637, 34.8684, 37.0708,\n",
      "        39.2679, 41.4712, 43.6817, 45.9051, 48.1430, 50.3855, 52.6338, 54.8719,\n",
      "        57.1079, 59.3368, 61.5695, 63.8000, 66.0292, 68.2562, 70.4864, 72.7224,\n",
      "        74.9662, 77.2170, 79.4791, 81.7519, 84.0338, 86.3189, 88.6069, 90.8964],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 8\n",
      "tensor([ 3.9563,  5.9414,  7.8856,  9.7915, 11.8241, 13.8956, 15.9540, 18.0309,\n",
      "        20.1267, 22.1960, 24.2756, 26.3471, 28.4084, 30.4708, 32.5224, 34.5775,\n",
      "        36.6311, 38.6958, 40.7684, 42.8571, 44.9596, 47.0707, 49.1878, 51.2990,\n",
      "        53.4117, 55.5223, 57.6441, 59.7722, 61.9110, 64.0577, 66.2195, 68.3953,\n",
      "        70.5827, 72.7744, 74.9705, 77.1707, 79.3762, 81.5840, 83.7963, 86.0135],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 9\n",
      "tensor([ 3.9007,  5.4049,  6.9116,  8.5284, 10.3043, 12.1185, 13.9468, 15.8004,\n",
      "        17.6647, 19.5303, 21.4032, 23.2778, 25.1514, 27.0233, 28.8974, 30.7829,\n",
      "        32.6692, 34.5725, 36.4872, 38.4255, 40.3798, 42.3512, 44.3289, 46.3135,\n",
      "        48.3117, 50.3272, 52.3661, 54.4186, 56.4825, 58.5498, 60.6286, 62.7125,\n",
      "        64.8070, 66.9060, 69.0148, 71.1259, 73.2424, 75.3584, 77.4812, 79.6081],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 10\n",
      "tensor([ 3.7025,  4.7268,  5.8944,  7.3124,  8.8654, 10.4693, 12.1015, 13.7594,\n",
      "        15.4538, 17.1354, 18.8423, 20.5515, 22.2672, 23.9937, 25.7228, 27.4750,\n",
      "        29.2354, 30.9935, 32.8053, 34.6386, 36.4994, 38.3850, 40.2919, 42.2149,\n",
      "        44.1530, 46.1047, 48.0687, 50.0407, 52.0227, 54.0142, 56.0201, 58.0334,\n",
      "        60.0537, 62.0756, 64.1057, 66.1424, 68.1866, 70.2320, 72.2825, 74.3398],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:10,loss:12.315994262695312,time:0.8046741485595703,lr:0.01\n",
      "val_loss: tensor(12.3160, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3523, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(30.0697, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 11\n",
      "tensor([ 3.6871,  4.4379,  5.4734,  6.7980,  8.2435,  9.7367, 11.2721, 12.8455,\n",
      "        14.4354, 16.0386, 17.6664, 19.3005, 20.9499, 22.6081, 24.2782, 25.9618,\n",
      "        27.6599, 29.3933, 31.1586, 32.9645, 34.8064, 36.6741, 38.5607, 40.4595,\n",
      "        42.3623, 44.2817, 46.2121, 48.1648, 50.1264, 52.0955, 54.0710, 56.0533,\n",
      "        58.0483, 60.0474, 62.0520, 64.0617, 66.0789, 68.1039, 70.1330, 72.1670],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 12\n",
      "tensor([ 4.0238,  4.7941,  5.8222,  7.1484,  8.6024, 10.0970, 11.6460, 13.2209,\n",
      "        14.8305, 16.4462, 18.0897, 19.7401, 21.4077, 23.0823, 24.7659, 26.4663,\n",
      "        28.1834, 29.9394, 31.7290, 33.5580, 35.4235, 37.3143, 39.2289, 41.1547,\n",
      "        43.0884, 45.0394, 47.0028, 48.9859, 50.9760, 52.9732, 54.9772, 56.9884,\n",
      "        59.0092, 61.0375, 63.0704, 65.1116, 67.1587, 69.2153, 71.2756, 73.3419],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 13\n",
      "tensor([ 4.6142,  5.5937,  6.7212,  8.0989,  9.6354, 11.2175, 12.8521, 14.4966,\n",
      "        16.1997, 17.8910, 19.6153, 21.3456, 23.0895, 24.8361, 26.5920, 28.3626,\n",
      "        30.1505, 31.9762, 33.8281, 35.7212, 37.6433, 39.5957, 41.5715, 43.5652,\n",
      "        45.5769, 47.6027, 49.6415, 51.6884, 53.7423, 55.8044, 57.8793, 59.9596,\n",
      "        62.0513, 64.1432, 66.2503, 68.3610, 70.4847, 72.6077, 74.7383, 76.8737],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 14\n",
      "tensor([ 5.2095,  6.4556,  7.7430,  9.2164, 10.8610, 12.5657, 14.3076, 16.0689,\n",
      "        17.8760, 19.6798, 21.5080, 23.3478, 25.1948, 27.0472, 28.8988, 30.7710,\n",
      "        32.6587, 34.5798, 36.5276, 38.5100, 40.5196, 42.5537, 44.6061, 46.6766,\n",
      "        48.7667, 50.8782, 53.0054, 55.1409, 57.2825, 59.4296, 61.5888, 63.7588,\n",
      "        65.9405, 68.1250, 70.3178, 72.5159, 74.7214, 76.9285, 79.1385, 81.3491],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 15\n",
      "tensor([ 5.5891,  7.0473,  8.4998, 10.0667, 11.8092, 13.6203, 15.4689, 17.3280,\n",
      "        19.2345, 21.1446, 23.0809, 25.0152, 26.9546, 28.9053, 30.8519, 32.8235,\n",
      "        34.8057, 36.8234, 38.8646, 40.9395, 43.0394, 45.1576, 47.2885, 49.4304,\n",
      "        51.5897, 53.7673, 55.9688, 58.1877, 60.4029, 62.6357, 64.8770, 67.1240,\n",
      "        69.3762, 71.6329, 73.8982, 76.1658, 78.4340, 80.7010, 82.9691, 85.2364],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 16\n",
      "tensor([ 5.6060,  7.1725,  8.7284, 10.3580, 12.1569, 14.0326, 15.9458, 17.8682,\n",
      "        19.8365, 21.8103, 23.8023, 25.7991, 27.8052, 29.8103, 31.8167, 33.8506,\n",
      "        35.8971, 37.9747, 40.0793, 42.2152, 44.3750, 46.5502, 48.7339, 50.9233,\n",
      "        53.1288, 55.3496, 57.5933, 59.8492, 62.1173, 64.3918, 66.6768, 68.9650,\n",
      "        71.2578, 73.5513, 75.8515, 78.1547, 80.4579, 82.7580, 85.0572, 87.3565],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 17\n",
      "tensor([ 5.2309,  6.8005,  8.3776, 10.0211, 11.8224, 13.7099, 15.6380, 17.5704,\n",
      "        19.5493, 21.5364, 23.5354, 25.5481, 27.5574, 29.5775, 31.5972, 33.6409,\n",
      "        35.7009, 37.7939, 39.9144, 42.0627, 44.2379, 46.4232, 48.6168, 50.8163,\n",
      "        53.0313, 55.2617, 57.5136, 59.7755, 62.0496, 64.3300, 66.6210, 68.9145,\n",
      "        71.2111, 73.5087, 75.8123, 78.1181, 80.4233, 82.7260, 85.0296, 87.3306],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 18\n",
      "tensor([ 4.5497,  6.0424,  7.5735,  9.1906, 10.9523, 12.8082, 14.7086, 16.6106,\n",
      "        18.5584, 20.5171, 22.4858, 24.4678, 26.4483, 28.4379, 30.4293, 32.4453,\n",
      "        34.4801, 36.5473, 38.6428, 40.7683, 42.9152, 45.0743, 47.2421, 49.4185,\n",
      "        51.6143, 53.8266, 56.0594, 58.3024, 60.5555, 62.8134, 65.0806, 67.3499,\n",
      "        69.6227, 71.8990, 74.1808, 76.4632, 78.7429, 81.0217, 83.3007, 85.5777],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 19\n",
      "tensor([ 3.7173,  5.0956,  6.5511,  8.1226,  9.8234, 11.6283, 13.4802, 15.3313,\n",
      "        17.2227, 19.1349, 21.0553, 22.9861, 24.9185, 26.8581, 28.8030, 30.7710,\n",
      "        32.7619, 34.7838, 36.8356, 38.9144, 41.0148, 43.1292, 45.2535, 47.3936,\n",
      "        49.5548, 51.7355, 53.9340, 56.1345, 58.3626, 60.5762, 62.8001, 65.0268,\n",
      "        67.2660, 69.5067, 71.7519, 73.9939, 76.2349, 78.4756, 80.7179, 82.9559],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 20\n",
      "tensor([ 2.8927,  4.1662,  5.5563,  7.0841,  8.7289, 10.4858, 12.2906, 14.0929,\n",
      "        15.9430, 17.8031, 19.6683, 21.5560, 23.4413, 25.3354, 27.2383, 29.1606,\n",
      "        31.1091, 33.0880, 35.0955, 37.1317, 39.1874, 41.2593, 43.3443, 45.4501,\n",
      "        47.5782, 49.7256, 51.8868, 54.0523, 56.2263, 58.4044, 60.5928, 62.7852,\n",
      "        64.9849, 67.1872, 69.3938, 71.5988, 73.8032, 76.0068, 78.2105, 80.4101],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:20,loss:1.9187697172164917,time:1.5480988025665283,lr:0.01\n",
      "val_loss: tensor(1.9188, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3483, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(30.6938, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 21\n",
      "tensor([ 2.2248,  3.4408,  4.8025,  6.3097,  7.9220,  9.6534, 11.4320, 13.2102,\n",
      "        15.0254, 16.8641, 18.7103, 20.5684, 22.4233, 24.2970, 26.1740, 28.0714,\n",
      "        29.9964, 31.9505, 33.9317, 35.9437, 37.9734, 40.0213, 42.0840, 44.1691,\n",
      "        46.2773, 48.4023, 50.5399, 52.6790, 54.8268, 56.9800, 59.1444, 61.3146,\n",
      "        63.4885, 65.6665, 67.8466, 70.0288, 72.2083, 74.3885, 76.5668, 78.7422],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 22\n",
      "tensor([ 1.7978,  3.0213,  4.4020,  5.9230,  7.5369,  9.2747, 11.0604, 12.8472,\n",
      "        14.6646, 16.5099, 18.3590, 20.2204, 22.0845, 23.9573, 25.8395, 27.7378,\n",
      "        29.6681, 31.6258, 33.6027, 35.6193, 37.6487, 39.6976, 41.7607, 43.8469,\n",
      "        45.9549, 48.0802, 50.2159, 52.3540, 54.4987, 56.6504, 58.8121, 60.9791,\n",
      "        63.1490, 65.3229, 67.4987, 69.6761, 71.8518, 74.0266, 76.2005, 78.3701],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 23\n",
      "tensor([ 1.7799,  3.0108,  4.3975,  5.9238,  7.5406,  9.2820, 11.0715, 12.8625,\n",
      "        14.6837, 16.5333, 18.3846, 20.2495, 22.1198, 23.9926, 25.8792, 27.7829,\n",
      "        29.7110, 31.6657, 33.6612, 35.6701, 37.7043, 39.7552, 41.8219, 43.9089,\n",
      "        46.0197, 48.1470, 50.2847, 52.4247, 54.5719, 56.7254, 58.8891, 61.0577,\n",
      "        63.2294, 65.4052, 67.5827, 69.7611, 71.9381, 74.1146, 76.2900, 78.4607],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 24\n",
      "tensor([ 1.7842,  3.0287,  4.4266,  5.9601,  7.5842,  9.3323, 11.1285, 12.9258,\n",
      "        14.7614, 16.6122, 18.4630, 20.3366, 22.2153, 24.0906, 25.9848, 27.8918,\n",
      "        29.8277, 31.7905, 33.7827, 35.8014, 37.8394, 39.8951, 41.9654, 44.0568,\n",
      "        46.1720, 48.3029, 50.4454, 52.5897, 54.7410, 56.8983, 59.0653, 61.2378,\n",
      "        63.4126, 65.5915, 67.7729, 69.9545, 72.1347, 74.3144, 76.4933, 78.6675],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 25\n",
      "tensor([ 1.8093,  3.0710,  4.4829,  6.0258,  7.6597,  9.4156, 11.2217, 13.0281,\n",
      "        14.8694, 16.7307, 18.5923, 20.4742, 22.3568, 24.2436, 26.1439, 28.0573,\n",
      "        30.0008, 31.9703, 33.9694, 35.9946, 38.0400, 40.1021, 42.1780, 44.2761,\n",
      "        46.3956, 48.5335, 50.6805, 52.8316, 54.9880, 57.1513, 59.3230, 61.5004,\n",
      "        63.6805, 65.8644, 68.0507, 70.2362, 72.4218, 74.6051, 76.7896, 78.9683],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 26\n",
      "tensor([ 1.8495,  3.1337,  4.5629,  6.1187,  7.7651,  9.5305, 11.3487, 13.1663,\n",
      "        15.0164, 16.8892, 18.7620, 20.6536, 22.5462, 24.4433, 26.3510, 28.2745,\n",
      "        30.2261, 32.2044, 34.2120, 36.2462, 38.2996, 40.3685, 42.4524, 44.5567,\n",
      "        46.6837, 48.8272, 50.9828, 53.1405, 55.3049, 57.4742, 59.6524, 61.8354,\n",
      "        64.0226, 66.2133, 68.4057, 70.5958, 72.7839, 74.9790, 77.1717, 79.3624],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 27\n",
      "tensor([ 1.9029,  3.2124,  4.6613,  6.2321,  7.8904,  9.6685, 11.4991, 13.3294,\n",
      "        15.1895, 17.0755, 18.9619, 20.8641, 22.7679, 24.6750, 26.5930, 28.5274,\n",
      "        30.4885, 32.4765, 34.4942, 36.5375, 38.6004, 40.6787, 42.7710, 44.8829,\n",
      "        47.0174, 49.1694, 51.3327, 53.4990, 55.6713, 57.8480, 60.0340, 62.2241,\n",
      "        64.4184, 66.6161, 68.8152, 71.0136, 73.2113, 75.4091, 77.6070, 79.7988],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 28\n",
      "tensor([ 1.9692,  3.3042,  4.7752,  6.3617,  8.0342,  9.8258, 11.6707, 13.5151,\n",
      "        15.3854, 17.2869, 19.1876, 21.1004, 23.0166, 24.9349, 26.8643, 28.8098,\n",
      "        30.7816, 32.7801, 34.8079, 36.8616, 38.9350, 41.0231, 43.1248, 45.2433,\n",
      "        47.3906, 49.5483, 51.7215, 53.8948, 56.0770, 58.2621, 60.4554, 62.6533,\n",
      "        64.8554, 67.0607, 69.2676, 71.4730, 73.6785, 75.8841, 78.0900, 80.2895],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 29\n",
      "tensor([ 2.0393,  3.4050,  4.8987,  6.5011,  8.1888,  9.9935, 11.8540, 13.7116,\n",
      "        15.5946, 17.5098, 19.4267, 21.3515, 23.2805, 25.2116, 27.1517, 29.1101,\n",
      "        31.0921, 33.1027, 35.1411, 37.2056, 39.2915, 41.3895, 43.5019, 45.6295,\n",
      "        47.7819, 49.9498, 52.1312, 54.3154, 56.5063, 58.6995, 60.9020, 63.1070,\n",
      "        65.3179, 67.5310, 69.7470, 71.9600, 74.1746, 76.3874, 78.6012, 80.8082],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 30\n",
      "tensor([ 2.1152,  3.5092,  5.0262,  6.6449,  8.3483, 10.1678, 12.0427, 13.9158,\n",
      "        15.8115, 17.7404, 19.6741, 21.6103, 23.5527, 25.4962, 27.4476, 29.4186,\n",
      "        31.4115, 33.4336, 35.4827, 37.5599, 39.6552, 41.7650, 43.8865, 46.0241,\n",
      "        48.1848, 50.3606, 52.5519, 54.7419, 56.9496, 59.1491, 61.3575, 63.5689,\n",
      "        65.7891, 68.0112, 70.2345, 72.4555, 74.6780, 76.9001, 79.1229, 81.3372],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:30,loss:1.234621286392212,time:2.275446891784668,lr:0.001\n",
      "val_loss: tensor(1.2346, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3477, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(30.0911, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 31\n",
      "tensor([ 2.1925,  3.6154,  5.1566,  6.7922,  8.5105, 10.3438, 12.2331, 14.1209,\n",
      "        16.0294, 17.9728, 19.9220, 21.8695, 23.8253, 25.7811, 27.7422, 29.7268,\n",
      "        31.7300, 33.7648, 35.8252, 37.9108, 40.0187, 42.1387, 44.2701, 46.4165,\n",
      "        48.5849, 50.7692, 52.9680, 55.1707, 57.3794, 59.5898, 61.8081, 64.0294,\n",
      "        66.2561, 68.4855, 70.7179, 72.9470, 75.1777, 77.4078, 79.6384, 81.8602],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 32\n",
      "tensor([ 2.2694,  3.7189,  5.2836,  6.9357,  8.6691, 10.5161, 12.4195, 14.3208,\n",
      "        16.2483, 18.2018, 20.1568, 22.1216, 24.0880, 26.0561, 28.0316, 30.0248,\n",
      "        32.0397, 34.0843, 36.1547, 38.2538, 40.3701, 42.5004, 44.6410, 46.7957,\n",
      "        48.9716, 51.1631, 53.3705, 55.5806, 57.7981, 60.0165, 62.2423, 64.4712,\n",
      "        66.7053, 68.9419, 71.1820, 73.4200, 75.6586, 77.8964, 80.1341, 82.3629],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 33\n",
      "tensor([ 2.3413,  3.8162,  5.4032,  7.0706,  8.8169, 10.6777, 12.5939, 14.5076,\n",
      "        16.4484, 18.4138, 20.3809, 22.3577, 24.3355, 26.3143, 28.3009, 30.3050,\n",
      "        32.3302, 34.3841, 36.4637, 38.5725, 40.6990, 42.8390, 44.9881, 47.1508,\n",
      "        49.3336, 51.5315, 53.7460, 55.9632, 58.1877, 60.4138, 62.6474, 64.8827,\n",
      "        67.1233, 69.3668, 71.6141, 73.8596, 76.1053, 78.3500, 80.5939, 82.8289],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 34\n",
      "tensor([ 2.4096,  3.9086,  5.5157,  7.1971,  8.9557, 10.8288, 12.7570, 14.6835,\n",
      "        16.6351, 18.6116, 20.5899, 22.5768, 24.5652, 26.5539, 28.5494, 30.5626,\n",
      "        32.5962, 34.6592, 36.7475, 38.8650, 41.0005, 43.1482, 45.3054, 47.4747,\n",
      "        49.6632, 51.8668, 54.0873, 56.3110, 58.5420, 60.7740, 63.0138, 65.2548,\n",
      "        67.5010, 69.7511, 72.0056, 74.2575, 76.5096, 78.7599, 81.0093, 83.2498],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 35\n",
      "tensor([ 2.4690,  3.9895,  5.6147,  7.3089,  9.0785, 10.9613, 12.9003, 14.8370,\n",
      "        16.7988, 18.7850, 20.7752, 22.7687, 24.7702, 26.7639, 28.7667, 30.7892,\n",
      "        32.8309, 34.9009, 36.9967, 39.1212, 41.2634, 43.4190, 45.5825, 47.7577,\n",
      "        49.9507, 52.1588, 54.3838, 56.6126, 58.8493, 61.0868, 63.3320, 65.5776,\n",
      "        67.8291, 70.0839, 72.3431, 74.6010, 76.8587, 79.1137, 81.3675, 83.6122],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 36\n",
      "tensor([ 2.5167,  4.0563,  5.6972,  7.4034,  9.1828, 11.0746, 13.0221, 14.9679,\n",
      "        16.9384, 18.9322, 20.9279, 22.9322, 24.9363, 26.9423, 28.9527, 30.9805,\n",
      "        33.0278, 35.1032, 37.2050, 39.3355, 41.4835, 43.6445, 45.8134, 47.9925,\n",
      "        50.1901, 52.4015, 54.6299, 56.8628, 59.1033, 61.3451, 63.5940, 65.8435,\n",
      "        68.0985, 70.3573, 72.6212, 74.8835, 77.1452, 79.4039, 81.6612, 83.9092],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 37\n",
      "tensor([ 2.5583,  4.1132,  5.7684,  7.4829,  9.2699, 11.1697, 13.1239, 15.0760,\n",
      "        17.0542, 19.0544, 21.0538, 23.0676, 25.0732, 27.0873, 29.1047, 31.1352,\n",
      "        33.1856, 35.2686, 37.3774, 39.5024, 41.6616, 43.8246, 45.9978, 48.1791,\n",
      "        50.3801, 52.5940, 54.8247, 57.0601, 59.3037, 61.5478, 63.7994, 66.0523,\n",
      "        68.3094, 70.5707, 72.8378, 75.1035, 77.3681, 79.6287, 81.8884, 84.1394],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 38\n",
      "tensor([ 2.5865,  4.1552,  5.8213,  7.5427,  9.3366, 11.2413, 13.2010, 15.1587,\n",
      "        17.1412, 19.1462, 21.1485, 23.1689, 25.1764, 27.1962, 29.2179, 31.2515,\n",
      "        33.3059, 35.3868, 37.4907, 39.6367, 41.7870, 43.9550, 46.1297, 48.3141,\n",
      "        50.5147, 52.7301, 54.9623, 57.1996, 59.4437, 61.6893, 63.9425, 66.1959,\n",
      "        68.4550, 70.7176, 72.9865, 75.2535, 77.5195, 79.7818, 82.0427, 84.2949],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 39\n",
      "tensor([ 2.6040,  4.1828,  5.8570,  7.5836,  9.3812, 11.2894, 13.2521, 15.2131,\n",
      "        17.1994, 19.2074, 21.2109, 23.2354, 25.2439, 27.2672, 29.2909, 31.3262,\n",
      "        33.3825, 35.4614, 37.5622, 39.7193, 41.8642, 44.0341, 46.2097, 48.3954,\n",
      "        50.5952, 52.8112, 55.0432, 57.2803, 59.5248, 61.7699, 64.0233, 66.2769,\n",
      "        68.5356, 70.7981, 73.0672, 75.3349, 77.6012, 79.8636, 82.1242, 84.3765],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 40\n",
      "tensor([ 2.6116,  4.1971,  5.8767,  7.6062,  9.4059, 11.3161, 13.2773, 15.2463,\n",
      "        17.2350, 19.2399, 21.2588, 23.2695, 25.2827, 27.3034, 29.3225, 31.3616,\n",
      "        33.4162, 35.4990, 37.6068, 39.7436, 41.8989, 44.0665, 46.2415, 48.4257,\n",
      "        50.6254, 52.8396, 55.0699, 57.3058, 59.5494, 61.7935, 64.0457, 66.2978,\n",
      "        68.5561, 70.8170, 73.0854, 75.3515, 77.6173, 79.8786, 82.1388, 84.3902],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:40,loss:1.4340131282806396,time:3.0160069465637207,lr:0.001\n",
      "val_loss: tensor(1.4340, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3491, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(28.7473, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 41\n",
      "tensor([ 2.6093,  4.1984,  5.8806,  7.6118,  9.4118, 11.3223, 13.2842, 15.2521,\n",
      "        17.2407, 19.2454, 21.2628, 23.2729, 25.2859, 27.3062, 29.3234, 31.3621,\n",
      "        33.4156, 35.4919, 37.5917, 39.7423, 41.8855, 44.0532, 46.2256, 48.4083,\n",
      "        50.6044, 52.8172, 55.0454, 57.2791, 59.5198, 61.7618, 64.0111, 66.2610,\n",
      "        68.5165, 70.7753, 73.0415, 75.3059, 77.5697, 79.8289, 82.0873, 84.3371],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 42\n",
      "tensor([ 2.5982,  4.1896,  5.8730,  7.6037,  9.4024, 11.3112, 13.2748, 15.2362,\n",
      "        17.2219, 19.2286, 21.2307, 23.2534, 25.2593, 27.2802, 29.3008, 31.3306,\n",
      "        33.3800, 35.4560, 37.5547, 39.6904, 41.8370, 43.9993, 46.1689, 48.3475,\n",
      "        50.5422, 52.7512, 54.9770, 57.2070, 59.4448, 61.6828, 63.9290, 66.1753,\n",
      "        68.4275, 70.6830, 72.9452, 75.2064, 77.4665, 79.7237, 81.9787, 84.2253],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 43\n",
      "tensor([ 2.5774,  4.1685,  5.8502,  7.5792,  9.3752, 11.2821, 13.2430, 15.2010,\n",
      "        17.1833, 19.1874, 21.1871, 23.2052, 25.2097, 27.2254, 29.2427, 31.2676,\n",
      "        33.3113, 35.3858, 37.4861, 39.6000, 41.7514, 43.9077, 46.0734, 48.2479,\n",
      "        50.4391, 52.6439, 54.8651, 57.0916, 59.3255, 61.5590, 63.8013, 66.0451,\n",
      "        68.2866, 70.5380, 72.8014, 75.0577, 77.3139, 79.5659, 81.8182, 84.0622],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 44\n",
      "tensor([ 2.5744,  4.1657,  5.8471,  7.5756,  9.3716, 11.2780, 13.2385, 15.1963,\n",
      "        17.1778, 19.1810, 21.1806, 23.1989, 25.2029, 27.2183, 29.2348, 31.2593,\n",
      "        33.3020, 35.3768, 37.4773, 39.5880, 41.7406, 43.8956, 46.0609, 48.2344,\n",
      "        50.4256, 52.6290, 54.8507, 57.0767, 59.3100, 61.5428, 63.7842, 66.0270,\n",
      "        68.2706, 70.5214, 72.7813, 75.0378, 77.2937, 79.5458, 81.7975, 84.0411],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 45\n",
      "tensor([ 2.5721,  4.1622,  5.8435,  7.5715,  9.3667, 11.2725, 13.2322, 15.1896,\n",
      "        17.1711, 19.1738, 21.1734, 23.1902, 25.1941, 27.2089, 29.2246, 31.2490,\n",
      "        33.2907, 35.3655, 37.4664, 39.5740, 41.7273, 43.8814, 46.0465, 48.2194,\n",
      "        50.4105, 52.6136, 54.8347, 57.0600, 59.2929, 61.5248, 63.7655, 66.0061,\n",
      "        68.2525, 70.5027, 72.7591, 75.0152, 77.2710, 79.5232, 81.7741, 84.0171],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 46\n",
      "tensor([ 2.5668,  4.1562,  5.8368,  7.5646,  9.3594, 11.2649, 13.2238, 15.1806,\n",
      "        17.1616, 19.1639, 21.1627, 23.1787, 25.1819, 27.1960, 29.2115, 31.2348,\n",
      "        33.2760, 35.3505, 37.4516, 39.5556, 41.7099, 43.8632, 46.0276, 48.1996,\n",
      "        50.3905, 52.5926, 54.8129, 57.0370, 59.2699, 61.5010, 63.7406, 65.9799,\n",
      "        68.2298, 70.4791, 72.7322, 74.9876, 77.2426, 79.4952, 81.7455, 83.9880],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 47\n",
      "tensor([ 2.5640,  4.1526,  5.8328,  7.5599,  9.3541, 11.2587, 13.2172, 15.1734,\n",
      "        17.1531, 19.1546, 21.1530, 23.1682, 25.1716, 27.1847, 29.1981, 31.2220,\n",
      "        33.2641, 35.3330, 37.4261, 39.5485, 41.6902, 43.8454, 46.0086, 48.1811,\n",
      "        50.3697, 52.5717, 54.7912, 57.0151, 59.2474, 61.4774, 63.7160, 65.9533,\n",
      "        68.2069, 70.4546, 72.7041, 74.9593, 77.2142, 79.4663, 81.7157, 83.9581],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 48\n",
      "tensor([ 2.5587,  4.1473,  5.8258,  7.5520,  9.3460, 11.2500, 13.2080, 15.1629,\n",
      "        17.1426, 19.1436, 21.1413, 23.1555, 25.1577, 27.1705, 29.1830, 31.2059,\n",
      "        33.2468, 35.3152, 37.4071, 39.5294, 41.6701, 43.8242, 45.9870, 48.1578,\n",
      "        50.3460, 52.5474, 54.7670, 56.9896, 59.2206, 61.4505, 63.6892, 65.9279,\n",
      "        68.1728, 70.4206, 72.6749, 74.9293, 77.1824, 79.4332, 81.6821, 83.9241],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 49\n",
      "tensor([ 2.5528,  4.1404,  5.8183,  7.5444,  9.3368, 11.2401, 13.1970, 15.1512,\n",
      "        17.1293, 19.1298, 21.1270, 23.1397, 25.1427, 27.1539, 29.1654, 31.1878,\n",
      "        33.2282, 35.2953, 37.3864, 39.5072, 41.6468, 43.8002, 45.9618, 48.1322,\n",
      "        50.3198, 52.5207, 54.7391, 56.9615, 59.1910, 61.4203, 63.6579, 65.8961,\n",
      "        68.1401, 70.3874, 72.6414, 74.8945, 77.1473, 79.3967, 81.6455, 83.8865],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 50\n",
      "tensor([ 2.5469,  4.1334,  5.8105,  7.5358,  9.3275, 11.2297, 13.1861, 15.1395,\n",
      "        17.1170, 19.1164, 21.1132, 23.1247, 25.1270, 27.1372, 29.1482, 31.1695,\n",
      "        33.2087, 35.2749, 37.3647, 39.4850, 41.6237, 43.7763, 45.9368, 48.1065,\n",
      "        50.2932, 52.4935, 54.7109, 56.9322, 59.1616, 61.3900, 63.6269, 65.8642,\n",
      "        68.1073, 70.3535, 72.6061, 74.8587, 77.1106, 79.3597, 81.6077, 83.8479],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:50,loss:0.9835351705551147,time:3.743377685546875,lr:0.0001\n",
      "val_loss: tensor(0.9835, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3486, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(28.9566, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 51\n",
      "tensor([ 2.5425,  4.1267,  5.8034,  7.5274,  9.3182, 11.2195, 13.1752, 15.1276,\n",
      "        17.1047, 19.1029, 21.0992, 23.1094, 25.1108, 27.1198, 29.1292, 31.1498,\n",
      "        33.1883, 35.2538, 37.3428, 39.4617, 41.5995, 43.7510, 45.9104, 48.0794,\n",
      "        50.2653, 52.4648, 54.6818, 56.9029, 59.1310, 61.3588, 63.5944, 65.8310,\n",
      "        68.0737, 70.3188, 72.5705, 74.8219, 77.0731, 79.3215, 81.5686, 83.8080],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 52\n",
      "tensor([ 2.5341,  4.1190,  5.7939,  7.5173,  9.3074, 11.2079, 13.1630, 15.1142,\n",
      "        17.0899, 19.0867, 21.0823, 23.0916, 25.0925, 27.1003, 29.1085, 31.1285,\n",
      "        33.1662, 35.2300, 37.3187, 39.4363, 41.5728, 43.7232, 45.8821, 48.0506,\n",
      "        50.2350, 52.4340, 54.6494, 56.8697, 59.0972, 61.3236, 63.5585, 65.7940,\n",
      "        68.0360, 70.2805, 72.5314, 74.7822, 77.0324, 79.2799, 81.5265, 83.7649],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 53\n",
      "tensor([ 2.5291,  4.1122,  5.7860,  7.5076,  9.2970, 11.1966, 13.1500, 15.1010,\n",
      "        17.0751, 19.0716, 21.0664, 23.0743, 25.0754, 27.0814, 29.0887, 31.1070,\n",
      "        33.1442, 35.2068, 37.2939, 39.4105, 41.5458, 43.6958, 45.8532, 48.0210,\n",
      "        50.2049, 52.4032, 54.6178, 56.8367, 59.0633, 61.2893, 63.5229, 65.7577,\n",
      "        67.9988, 70.2420, 72.4923, 74.7416, 76.9915, 79.2375, 81.4837, 83.7217],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 54\n",
      "tensor([ 2.5236,  4.1044,  5.7776,  7.4980,  9.2859, 11.1852, 13.1379, 15.0873,\n",
      "        17.0612, 19.0563, 21.0508, 23.0573, 25.0572, 27.0617, 29.0677, 31.0856,\n",
      "        33.1214, 35.1835, 37.2692, 39.3849, 41.5190, 43.6676, 45.8246, 47.9910,\n",
      "        50.1742, 52.3716, 54.5856, 56.8038, 59.0292, 61.2539, 63.4864, 65.7204,\n",
      "        67.9602, 70.2028, 72.4523, 74.7012, 76.9497, 79.1952, 81.4403, 83.6771],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 55\n",
      "tensor([ 2.5174,  4.0966,  5.7678,  7.4877,  9.2749, 11.1724, 13.1244, 15.0730,\n",
      "        17.0451, 19.0396, 21.0327, 23.0378, 25.0378, 27.0409, 29.0458, 31.0625,\n",
      "        33.0969, 35.1581, 37.2432, 39.3575, 41.4903, 43.6379, 45.7940, 47.9595,\n",
      "        50.1421, 52.3382, 54.5514, 56.7686, 58.9928, 61.2166, 63.4487, 65.6816,\n",
      "        67.9204, 70.1621, 72.4102, 74.6582, 76.9057, 79.1504, 81.3947, 83.6312],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 56\n",
      "tensor([ 2.5089,  4.0869,  5.7568,  7.4760,  9.2619, 11.1587, 13.1097, 15.0572,\n",
      "        17.0281, 19.0218, 21.0143, 23.0180, 25.0179, 27.0189, 29.0229, 31.0391,\n",
      "        33.0721, 35.1320, 37.2158, 39.3290, 41.4609, 43.6075, 45.7619, 47.9263,\n",
      "        50.1081, 52.3035, 54.5162, 56.7326, 58.9561, 61.1791, 63.4102, 65.6419,\n",
      "        67.8796, 70.1204, 72.3678, 74.6143, 76.8611, 79.1051, 81.3487, 83.5845],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 57\n",
      "tensor([ 2.5018,  4.0792,  5.7482,  7.4659,  9.2508, 11.1467, 13.0966, 15.0424,\n",
      "        17.0127, 19.0044, 20.9965, 22.9986, 24.9977, 26.9980, 29.0003, 31.0156,\n",
      "        33.0483, 35.1069, 37.1894, 39.3016, 41.4322, 43.5773, 45.7305, 47.8944,\n",
      "        50.0750, 52.2696, 54.4813, 56.6968, 58.9190, 61.1411, 63.3711, 65.6019,\n",
      "        67.8391, 70.0788, 72.3245, 74.5706, 76.8164, 79.0597, 81.3024, 83.5371],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 58\n",
      "tensor([ 2.4942,  4.0702,  5.7380,  7.4551,  9.2389, 11.1333, 13.0819, 15.0276,\n",
      "        16.9962, 18.9876, 20.9783, 22.9792, 24.9781, 26.9767, 28.9780, 30.9923,\n",
      "        33.0239, 35.0809, 37.1627, 39.2735, 41.4033, 43.5472, 45.6995, 47.8627,\n",
      "        50.0420, 52.2358, 54.4464, 56.6609, 58.8826, 61.1036, 63.3327, 65.5626,\n",
      "        67.7989, 70.0372, 72.2829, 74.5272, 76.7726, 79.0148, 81.2568, 83.4908],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 59\n",
      "tensor([ 2.4867,  4.0619,  5.7282,  7.4438,  9.2263, 11.1198, 13.0676, 15.0120,\n",
      "        16.9796, 18.9698, 20.9604, 22.9592, 24.9576, 26.9547, 28.9549, 30.9687,\n",
      "        32.9990, 35.0551, 37.1350, 39.2443, 41.3727, 43.5163, 45.6678, 47.8296,\n",
      "        50.0083, 52.2012, 54.4111, 56.6245, 58.8453, 61.0651, 63.2933, 65.5225,\n",
      "        67.7577, 69.9951, 72.2392, 74.4830, 76.7270, 78.9687, 81.2093, 83.4426],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 60\n",
      "tensor([ 2.4802,  4.0529,  5.7185,  7.4335,  9.2145, 11.1071, 13.0538, 14.9974,\n",
      "        16.9640, 18.9527, 20.9428, 22.9404, 24.9380, 26.9338, 28.9324, 30.9449,\n",
      "        32.9741, 35.0289, 37.1081, 39.2162, 41.3435, 43.4856, 45.6361, 47.7969,\n",
      "        49.9746, 52.1667, 54.3758, 56.5884, 58.8081, 61.0270, 63.2539, 65.4821,\n",
      "        67.7160, 69.9527, 72.1956, 74.4382, 76.6814, 78.9219, 81.1622, 83.3948],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:60,loss:0.654966413974762,time:4.462672472000122,lr:0.0001\n",
      "val_loss: tensor(0.6550, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3484, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(28.3947, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 61\n",
      "tensor([ 2.4732,  4.0452,  5.7079,  7.4219,  9.2029, 11.0941, 13.0393, 14.9819,\n",
      "        16.9472, 18.9357, 20.9241, 22.9203, 24.9176, 26.9118, 28.9095, 30.9206,\n",
      "        32.9492, 35.0026, 37.0810, 39.1879, 41.3138, 43.4548, 45.6041, 47.7644,\n",
      "        49.9414, 52.1328, 54.3405, 56.5522, 58.7708, 60.9885, 63.2145, 65.4417,\n",
      "        67.6750, 69.9105, 72.1532, 74.3944, 76.6369, 78.8762, 81.1159, 83.3474],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 62\n",
      "tensor([ 2.4662,  4.0370,  5.6991,  7.4122,  9.1908, 11.0819, 13.0265, 14.9673,\n",
      "        16.9322, 18.9190, 20.9070, 22.9014, 24.8983, 26.8907, 28.8867, 30.8980,\n",
      "        32.9248, 34.9777, 37.0545, 39.1601, 41.2853, 43.4251, 45.5737, 47.7325,\n",
      "        49.9088, 52.0989, 54.3064, 56.5168, 58.7346, 60.9514, 63.1765, 65.4027,\n",
      "        67.6349, 69.8699, 72.1108, 74.3517, 76.5928, 78.8317, 81.0704, 83.3011],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 63\n",
      "tensor([ 2.4600,  4.0297,  5.6900,  7.4014,  9.1801, 11.0695, 13.0131, 14.9534,\n",
      "        16.9169, 18.9023, 20.8896, 22.8828, 24.8791, 26.8701, 28.8651, 30.8750,\n",
      "        32.9007, 34.9524, 37.0283, 39.1328, 41.2565, 43.3956, 45.5434, 47.7010,\n",
      "        49.8761, 52.0653, 54.2719, 56.4817, 58.6984, 60.9142, 63.1383, 65.3639,\n",
      "        67.5950, 69.8287, 72.0688, 74.3082, 76.5483, 78.7866, 81.0243, 83.2541],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 64\n",
      "tensor([ 2.4516,  4.0199,  5.6794,  7.3897,  9.1670, 11.0558, 12.9985, 14.9378,\n",
      "        16.9000, 18.8848, 20.8716, 22.8632, 24.8594, 26.8488, 28.8426, 30.8518,\n",
      "        32.8765, 34.9272, 37.0017, 39.1052, 41.2280, 43.3656, 45.5124, 47.6695,\n",
      "        49.8432, 52.0317, 54.2375, 56.4459, 58.6619, 60.8769, 63.1001, 65.3245,\n",
      "        67.5551, 69.7880, 72.0271, 74.2659, 76.5049, 78.7421, 80.9789, 83.2081],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 65\n",
      "tensor([ 2.4517,  4.0207,  5.6794,  7.3901,  9.1671, 11.0558, 12.9981, 14.9368,\n",
      "        16.8994, 18.8835, 20.8703, 22.8620, 24.8583, 26.8478, 28.8413, 30.8503,\n",
      "        32.8748, 34.9252, 36.9997, 39.1028, 41.2252, 43.3628, 45.5092, 47.6662,\n",
      "        49.8404, 52.0292, 54.2341, 56.4427, 58.6587, 60.8734, 63.0965, 65.3207,\n",
      "        67.5510, 69.7839, 72.0227, 74.2613, 76.5005, 78.7375, 80.9745, 83.2038],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 66\n",
      "tensor([ 2.4524,  4.0199,  5.6785,  7.3894,  9.1663, 11.0545, 12.9969, 14.9357,\n",
      "        16.8983, 18.8824, 20.8692, 22.8605, 24.8561, 26.8461, 28.8394, 30.8481,\n",
      "        32.8726, 34.9229, 36.9977, 39.1005, 41.2230, 43.3603, 45.5067, 47.6634,\n",
      "        49.8378, 52.0261, 54.2309, 56.4394, 58.6550, 60.8702, 63.0929, 65.3178,\n",
      "        67.5475, 69.7802, 72.0186, 74.2573, 76.4966, 78.7335, 80.9706, 83.1994],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 67\n",
      "tensor([ 2.4497,  4.0178,  5.6770,  7.3874,  9.1648, 11.0532, 12.9958, 14.9345,\n",
      "        16.8964, 18.8811, 20.8672, 22.8582, 24.8547, 26.8440, 28.8374, 30.8462,\n",
      "        32.8709, 34.9208, 36.9948, 39.0976, 41.2199, 43.3576, 45.5035, 47.6602,\n",
      "        49.8341, 52.0226, 54.2277, 56.4362, 58.6518, 60.8661, 63.0891, 65.3133,\n",
      "        67.5434, 69.7759, 72.0152, 74.2533, 76.4922, 78.7295, 80.9661, 83.1954],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 68\n",
      "tensor([ 2.4500,  4.0173,  5.6764,  7.3864,  9.1637, 11.0525, 12.9940, 14.9329,\n",
      "        16.8947, 18.8791, 20.8649, 22.8566, 24.8527, 26.8416, 28.8360, 30.8435,\n",
      "        32.8685, 34.9186, 36.9926, 39.0951, 41.2171, 43.3550, 45.5007, 47.6574,\n",
      "        49.8309, 52.0194, 54.2239, 56.4326, 58.6481, 60.8629, 63.0854, 65.3096,\n",
      "        67.5400, 69.7718, 72.0108, 74.2488, 76.4880, 78.7245, 80.9614, 83.1904],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 69\n",
      "tensor([ 2.4490,  4.0168,  5.6761,  7.3852,  9.1627, 11.0506, 12.9926, 14.9313,\n",
      "        16.8931, 18.8777, 20.8642, 22.8548, 24.8507, 26.8396, 28.8329, 30.8412,\n",
      "        32.8654, 34.9155, 36.9896, 39.0923, 41.2142, 43.3514, 45.4975, 47.6541,\n",
      "        49.8281, 52.0161, 54.2209, 56.4298, 58.6448, 60.8590, 63.0818, 65.3056,\n",
      "        67.5357, 69.7685, 72.0069, 74.2452, 76.4840, 78.7207, 80.9573, 83.1859],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 70\n",
      "tensor([ 2.4485,  4.0161,  5.6755,  7.3850,  9.1620, 11.0497, 12.9910, 14.9300,\n",
      "        16.8919, 18.8755, 20.8620, 22.8528, 24.8489, 26.8372, 28.8308, 30.8394,\n",
      "        32.8633, 34.9131, 36.9869, 39.0894, 41.2113, 43.3488, 45.4945, 47.6514,\n",
      "        49.8248, 52.0128, 54.2178, 56.4261, 58.6415, 60.8559, 63.0786, 65.3022,\n",
      "        67.5322, 69.7645, 72.0026, 74.2407, 76.4795, 78.7165, 80.9527, 83.1816],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:70,loss:0.5390586853027344,time:5.1836230754852295,lr:1e-05\n",
      "val_loss: tensor(0.5391, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3482, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(28.6581, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 71\n",
      "tensor([ 2.4471,  4.0157,  5.6738,  7.3838,  9.1605, 11.0483, 12.9899, 14.9284,\n",
      "        16.8901, 18.8741, 20.8605, 22.8514, 24.8469, 26.8353, 28.8284, 30.8365,\n",
      "        32.8610, 34.9104, 36.9843, 39.0869, 41.2086, 43.3462, 45.4920, 47.6481,\n",
      "        49.8216, 52.0101, 54.2149, 56.4228, 58.6380, 60.8525, 63.0748, 65.2986,\n",
      "        67.5284, 69.7606, 71.9991, 74.2370, 76.4757, 78.7121, 80.9486, 83.1773],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 72\n",
      "tensor([ 2.4460,  4.0133,  5.6722,  7.3823,  9.1587, 11.0469, 12.9890, 14.9269,\n",
      "        16.8885, 18.8724, 20.8585, 22.8488, 24.8447, 26.8335, 28.8264, 30.8348,\n",
      "        32.8588, 34.9080, 36.9818, 39.0844, 41.2060, 43.3431, 45.4886, 47.6453,\n",
      "        49.8189, 52.0064, 54.2113, 56.4191, 58.6342, 60.8483, 63.0709, 65.2946,\n",
      "        67.5242, 69.7567, 71.9949, 74.2330, 76.4713, 78.7080, 80.9442, 83.1729],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 73\n",
      "tensor([ 2.4468,  4.0134,  5.6726,  7.3825,  9.1588, 11.0463, 12.9880, 14.9262,\n",
      "        16.8873, 18.8711, 20.8576, 22.8479, 24.8436, 26.8321, 28.8249, 30.8328,\n",
      "        32.8572, 34.9068, 36.9800, 39.0824, 41.2038, 43.3408, 45.4865, 47.6427,\n",
      "        49.8162, 52.0040, 54.2088, 56.4166, 58.6316, 60.8457, 63.0681, 65.2915,\n",
      "        67.5211, 69.7532, 71.9910, 74.2293, 76.4677, 78.7042, 80.9402, 83.1686],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 74\n",
      "tensor([ 2.4458,  4.0122,  5.6701,  7.3799,  9.1567, 11.0439, 12.9856, 14.9234,\n",
      "        16.8849, 18.8686, 20.8547, 22.8453, 24.8410, 26.8292, 28.8220, 30.8299,\n",
      "        32.8536, 34.9032, 36.9767, 39.0788, 41.2003, 43.3373, 45.4826, 47.6387,\n",
      "        49.8122, 51.9998, 54.2042, 56.4123, 58.6271, 60.8412, 63.0633, 65.2866,\n",
      "        67.5162, 69.7485, 71.9865, 74.2242, 76.4627, 78.6987, 80.9351, 83.1638],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 75\n",
      "tensor([ 2.4433,  4.0101,  5.6688,  7.3782,  9.1546, 11.0424, 12.9832, 14.9214,\n",
      "        16.8825, 18.8663, 20.8526, 22.8427, 24.8389, 26.8268, 28.8194, 30.8274,\n",
      "        32.8513, 34.9004, 36.9739, 39.0758, 41.1973, 43.3344, 45.4793, 47.6356,\n",
      "        49.8088, 51.9963, 54.2009, 56.4087, 58.6235, 60.8376, 63.0595, 65.2831,\n",
      "        67.5124, 69.7441, 71.9827, 74.2199, 76.4582, 78.6946, 80.9308, 83.1592],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 76\n",
      "tensor([ 2.4434,  4.0109,  5.6684,  7.3775,  9.1546, 11.0418, 12.9833, 14.9206,\n",
      "        16.8822, 18.8658, 20.8517, 22.8421, 24.8375, 26.8256, 28.8180, 30.8258,\n",
      "        32.8499, 34.8988, 36.9722, 39.0741, 41.1954, 43.3322, 45.4774, 47.6333,\n",
      "        49.8067, 51.9939, 54.1982, 56.4069, 58.6209, 60.8351, 63.0568, 65.2803,\n",
      "        67.5091, 69.7410, 71.9790, 74.2163, 76.4550, 78.6908, 80.9273, 83.1551],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 77\n",
      "tensor([ 2.4423,  4.0101,  5.6680,  7.3771,  9.1538, 11.0413, 12.9824, 14.9206,\n",
      "        16.8816, 18.8650, 20.8510, 22.8406, 24.8363, 26.8238, 28.8160, 30.8240,\n",
      "        32.8475, 34.8968, 36.9700, 39.0717, 41.1929, 43.3296, 45.4749, 47.6304,\n",
      "        49.8037, 51.9910, 54.1950, 56.4027, 58.6178, 60.8314, 63.0530, 65.2766,\n",
      "        67.5059, 69.7377, 71.9755, 74.2132, 76.4514, 78.6870, 80.9231, 83.1516],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 78\n",
      "tensor([ 2.4424,  4.0098,  5.6681,  7.3773,  9.1537, 11.0408, 12.9821, 14.9191,\n",
      "        16.8801, 18.8635, 20.8495, 22.8396, 24.8350, 26.8224, 28.8149, 30.8228,\n",
      "        32.8465, 34.8954, 36.9684, 39.0703, 41.1911, 43.3274, 45.4724, 47.6282,\n",
      "        49.8014, 51.9889, 54.1929, 56.4004, 58.6149, 60.8287, 63.0507, 65.2736,\n",
      "        67.5030, 69.7349, 71.9725, 74.2099, 76.4480, 78.6840, 80.9200, 83.1482],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 79\n",
      "tensor([ 2.4434,  4.0089,  5.6671,  7.3761,  9.1516, 11.0393, 12.9801, 14.9180,\n",
      "        16.8786, 18.8623, 20.8479, 22.8375, 24.8331, 26.8204, 28.8123, 30.8201,\n",
      "        32.8439, 34.8922, 36.9657, 39.0669, 41.1880, 43.3246, 45.4696, 47.6253,\n",
      "        49.7986, 51.9858, 54.1895, 56.3973, 58.6111, 60.8248, 63.0467, 65.2697,\n",
      "        67.4988, 69.7303, 71.9677, 74.2050, 76.4435, 78.6791, 80.9151, 83.1427],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 80\n",
      "tensor([ 2.4413,  4.0079,  5.6659,  7.3749,  9.1512, 11.0385, 12.9790, 14.9160,\n",
      "        16.8770, 18.8598, 20.8459, 22.8353, 24.8311, 26.8180, 28.8099, 30.8178,\n",
      "        32.8413, 34.8901, 36.9630, 39.0646, 41.1855, 43.3219, 45.4665, 47.6224,\n",
      "        49.7953, 51.9824, 54.1860, 56.3936, 58.6080, 60.8212, 63.0430, 65.2663,\n",
      "        67.4952, 69.7266, 71.9643, 74.2016, 76.4396, 78.6755, 80.9109, 83.1392],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:80,loss:0.5186705589294434,time:5.927802801132202,lr:1e-05\n",
      "val_loss: tensor(0.5187, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3482, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(28.8189, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 81\n",
      "tensor([ 2.4401,  4.0075,  5.6654,  7.3740,  9.1506, 11.0374, 12.9784, 14.9158,\n",
      "        16.8761, 18.8593, 20.8452, 22.8343, 24.8297, 26.8170, 28.8092, 30.8164,\n",
      "        32.8402, 34.8884, 36.9614, 39.0628, 41.1837, 43.3204, 45.4648, 47.6205,\n",
      "        49.7928, 51.9804, 54.1838, 56.3917, 58.6057, 60.8190, 63.0405, 65.2632,\n",
      "        67.4925, 69.7237, 71.9613, 74.1982, 76.4365, 78.6718, 80.9076, 83.1354],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 82\n",
      "tensor([ 2.4413,  4.0077,  5.6650,  7.3737,  9.1498, 11.0368, 12.9775, 14.9149,\n",
      "        16.8755, 18.8581, 20.8440, 22.8331, 24.8283, 26.8154, 28.8069, 30.8148,\n",
      "        32.8380, 34.8864, 36.9593, 39.0606, 41.1813, 43.3177, 45.4620, 47.6174,\n",
      "        49.7903, 51.9774, 54.1813, 56.3879, 58.6023, 60.8157, 63.0368, 65.2598,\n",
      "        67.4887, 69.7200, 71.9574, 74.1945, 76.4322, 78.6681, 80.9034, 83.1314],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 83\n",
      "tensor([ 2.4395,  4.0054,  5.6628,  7.3708,  9.1473, 11.0340, 12.9747, 14.9126,\n",
      "        16.8728, 18.8557, 20.8409, 22.8305, 24.8261, 26.8125, 28.8047, 30.8123,\n",
      "        32.8356, 34.8834, 36.9564, 39.0573, 41.1777, 43.3141, 45.4583, 47.6142,\n",
      "        49.7866, 51.9736, 54.1773, 56.3846, 58.5983, 60.8116, 63.0330, 65.2558,\n",
      "        67.4844, 69.7158, 71.9532, 74.1899, 76.4279, 78.6628, 80.8987, 83.1265],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 84\n",
      "tensor([ 2.4388,  4.0053,  5.6626,  7.3713,  9.1470, 11.0334, 12.9736, 14.9108,\n",
      "        16.8719, 18.8548, 20.8402, 22.8295, 24.8250, 26.8114, 28.8028, 30.8102,\n",
      "        32.8329, 34.8814, 36.9539, 39.0547, 41.1749, 43.3112, 45.4559, 47.6112,\n",
      "        49.7838, 51.9705, 54.1742, 56.3815, 58.5953, 60.8084, 63.0294, 65.2523,\n",
      "        67.4813, 69.7124, 71.9491, 74.1859, 76.4235, 78.6589, 80.8944, 83.1219],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 85\n",
      "tensor([ 2.4388,  4.0039,  5.6608,  7.3702,  9.1455, 11.0326, 12.9730, 14.9100,\n",
      "        16.8702, 18.8529, 20.8386, 22.8270, 24.8223, 26.8086, 28.8005, 30.8077,\n",
      "        32.8306, 34.8791, 36.9511, 39.0522, 41.1723, 43.3081, 45.4524, 47.6077,\n",
      "        49.7801, 51.9670, 54.1705, 56.3775, 58.5913, 60.8045, 63.0258, 65.2484,\n",
      "        67.4769, 69.7078, 71.9451, 74.1818, 76.4192, 78.6549, 80.8904, 83.1178],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 86\n",
      "tensor([ 2.4387,  4.0039,  5.6615,  7.3701,  9.1457, 11.0325, 12.9730, 14.9100,\n",
      "        16.8703, 18.8529, 20.8380, 22.8272, 24.8224, 26.8089, 28.8008, 30.8081,\n",
      "        32.8315, 34.8792, 36.9516, 39.0524, 41.1726, 43.3090, 45.4530, 47.6083,\n",
      "        49.7809, 51.9673, 54.1709, 56.3776, 58.5918, 60.8051, 63.0261, 65.2487,\n",
      "        67.4771, 69.7083, 71.9453, 74.1824, 76.4197, 78.6552, 80.8910, 83.1184],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 87\n",
      "tensor([ 2.4391,  4.0041,  5.6615,  7.3702,  9.1460, 11.0328, 12.9731, 14.9101,\n",
      "        16.8702, 18.8529, 20.8381, 22.8271, 24.8222, 26.8093, 28.8011, 30.8081,\n",
      "        32.8309, 34.8786, 36.9512, 39.0521, 41.1722, 43.3083, 45.4525, 47.6078,\n",
      "        49.7803, 51.9668, 54.1705, 56.3774, 58.5916, 60.8046, 63.0255, 65.2481,\n",
      "        67.4767, 69.7076, 71.9446, 74.1817, 76.4191, 78.6544, 80.8899, 83.1178],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 88\n",
      "tensor([ 2.4396,  4.0047,  5.6615,  7.3707,  9.1463, 11.0333, 12.9734, 14.9110,\n",
      "        16.8708, 18.8533, 20.8386, 22.8275, 24.8231, 26.8095, 28.8011, 30.8081,\n",
      "        32.8312, 34.8795, 36.9518, 39.0528, 41.1727, 43.3085, 45.4529, 47.6081,\n",
      "        49.7807, 51.9672, 54.1712, 56.3779, 58.5917, 60.8048, 63.0260, 65.2484,\n",
      "        67.4770, 69.7079, 71.9454, 74.1823, 76.4196, 78.6551, 80.8902, 83.1180],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 89\n",
      "tensor([ 2.4371,  4.0034,  5.6602,  7.3694,  9.1458, 11.0326, 12.9728, 14.9097,\n",
      "        16.8702, 18.8521, 20.8379, 22.8267, 24.8220, 26.8084, 28.7999, 30.8076,\n",
      "        32.8303, 34.8783, 36.9503, 39.0510, 41.1709, 43.3072, 45.4516, 47.6068,\n",
      "        49.7795, 51.9665, 54.1699, 56.3768, 58.5906, 60.8037, 63.0247, 65.2474,\n",
      "        67.4763, 69.7073, 71.9441, 74.1806, 76.4183, 78.6536, 80.8890, 83.1167],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 90\n",
      "tensor([ 2.4374,  4.0034,  5.6601,  7.3693,  9.1455, 11.0326, 12.9727, 14.9096,\n",
      "        16.8697, 18.8517, 20.8374, 22.8260, 24.8214, 26.8081, 28.7996, 30.8073,\n",
      "        32.8303, 34.8779, 36.9502, 39.0509, 41.1707, 43.3070, 45.4513, 47.6064,\n",
      "        49.7790, 51.9657, 54.1692, 56.3764, 58.5904, 60.8033, 63.0245, 65.2473,\n",
      "        67.4762, 69.7069, 71.9439, 74.1805, 76.4181, 78.6532, 80.8883, 83.1159],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:90,loss:0.5081087946891785,time:6.687519311904907,lr:1.0000000000000002e-06\n",
      "val_loss: tensor(0.5081, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3482, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(28.8429, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 91\n",
      "tensor([ 2.4381,  4.0032,  5.6604,  7.3694,  9.1453, 11.0320, 12.9729, 14.9095,\n",
      "        16.8697, 18.8525, 20.8382, 22.8269, 24.8224, 26.8091, 28.8003, 30.8075,\n",
      "        32.8306, 34.8786, 36.9507, 39.0515, 41.1715, 43.3073, 45.4515, 47.6069,\n",
      "        49.7794, 51.9661, 54.1698, 56.3768, 58.5906, 60.8034, 63.0245, 65.2474,\n",
      "        67.4758, 69.7067, 71.9437, 74.1808, 76.4179, 78.6530, 80.8885, 83.1162],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 92\n",
      "tensor([ 2.4370,  4.0027,  5.6603,  7.3693,  9.1445, 11.0317, 12.9719, 14.9092,\n",
      "        16.8690, 18.8519, 20.8374, 22.8262, 24.8219, 26.8079, 28.7994, 30.8071,\n",
      "        32.8297, 34.8777, 36.9499, 39.0508, 41.1710, 43.3066, 45.4511, 47.6063,\n",
      "        49.7785, 51.9654, 54.1688, 56.3757, 58.5901, 60.8027, 63.0237, 65.2462,\n",
      "        67.4748, 69.7062, 71.9431, 74.1795, 76.4171, 78.6524, 80.8876, 83.1153],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 93\n",
      "tensor([ 2.4370,  4.0025,  5.6597,  7.3686,  9.1443, 11.0318, 12.9718, 14.9090,\n",
      "        16.8687, 18.8513, 20.8369, 22.8254, 24.8208, 26.8075, 28.7990, 30.8062,\n",
      "        32.8291, 34.8769, 36.9492, 39.0504, 41.1706, 43.3064, 45.4508, 47.6059,\n",
      "        49.7784, 51.9652, 54.1688, 56.3754, 58.5895, 60.8026, 63.0236, 65.2460,\n",
      "        67.4744, 69.7057, 71.9427, 74.1793, 76.4167, 78.6519, 80.8870, 83.1147],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 94\n",
      "tensor([ 2.4382,  4.0044,  5.6613,  7.3696,  9.1448, 11.0320, 12.9720, 14.9092,\n",
      "        16.8696, 18.8520, 20.8377, 22.8262, 24.8217, 26.8080, 28.7996, 30.8063,\n",
      "        32.8292, 34.8775, 36.9496, 39.0505, 41.1709, 43.3069, 45.4509, 47.6061,\n",
      "        49.7783, 51.9658, 54.1686, 56.3760, 58.5897, 60.8024, 63.0238, 65.2462,\n",
      "        67.4750, 69.7055, 71.9429, 74.1795, 76.4172, 78.6522, 80.8877, 83.1149],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 95\n",
      "tensor([ 2.4370,  4.0037,  5.6604,  7.3690,  9.1445, 11.0310, 12.9716, 14.9083,\n",
      "        16.8688, 18.8511, 20.8365, 22.8254, 24.8207, 26.8073, 28.7989, 30.8060,\n",
      "        32.8288, 34.8769, 36.9490, 39.0497, 41.1697, 43.3056, 45.4499, 47.6051,\n",
      "        49.7775, 51.9643, 54.1677, 56.3748, 58.5889, 60.8016, 63.0224, 65.2449,\n",
      "        67.4738, 69.7047, 71.9414, 74.1777, 76.4154, 78.6507, 80.8860, 83.1138],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 96\n",
      "tensor([ 2.4366,  4.0033,  5.6600,  7.3682,  9.1439, 11.0307, 12.9714, 14.9079,\n",
      "        16.8681, 18.8506, 20.8359, 22.8247, 24.8201, 26.8068, 28.7986, 30.8057,\n",
      "        32.8284, 34.8764, 36.9487, 39.0495, 41.1691, 43.3045, 45.4490, 47.6043,\n",
      "        49.7769, 51.9637, 54.1672, 56.3743, 58.5883, 60.8013, 63.0221, 65.2446,\n",
      "        67.4732, 69.7038, 71.9409, 74.1774, 76.4149, 78.6501, 80.8853, 83.1132],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 97\n",
      "tensor([ 2.4380,  4.0040,  5.6610,  7.3696,  9.1457, 11.0320, 12.9722, 14.9090,\n",
      "        16.8691, 18.8515, 20.8370, 22.8257, 24.8210, 26.8075, 28.7989, 30.8065,\n",
      "        32.8290, 34.8772, 36.9498, 39.0503, 41.1701, 43.3057, 45.4503, 47.6054,\n",
      "        49.7778, 51.9646, 54.1680, 56.3747, 58.5886, 60.8016, 63.0227, 65.2456,\n",
      "        67.4739, 69.7050, 71.9419, 74.1782, 76.4160, 78.6510, 80.8861, 83.1139],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 98\n",
      "tensor([ 2.4381,  4.0042,  5.6616,  7.3700,  9.1456, 11.0326, 12.9731, 14.9099,\n",
      "        16.8696, 18.8522, 20.8376, 22.8263, 24.8211, 26.8077, 28.7998, 30.8067,\n",
      "        32.8296, 34.8774, 36.9498, 39.0507, 41.1704, 43.3064, 45.4507, 47.6061,\n",
      "        49.7782, 51.9649, 54.1688, 56.3753, 58.5892, 60.8021, 63.0227, 65.2454,\n",
      "        67.4741, 69.7049, 71.9418, 74.1783, 76.4159, 78.6511, 80.8864, 83.1140],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 99\n",
      "tensor([ 2.4388,  4.0043,  5.6617,  7.3703,  9.1455, 11.0328, 12.9732, 14.9098,\n",
      "        16.8693, 18.8516, 20.8369, 22.8260, 24.8210, 26.8080, 28.7998, 30.8066,\n",
      "        32.8295, 34.8775, 36.9497, 39.0503, 41.1704, 43.3063, 45.4505, 47.6056,\n",
      "        49.7780, 51.9644, 54.1683, 56.3747, 58.5887, 60.8018, 63.0226, 65.2455,\n",
      "        67.4739, 69.7046, 71.9416, 74.1781, 76.4155, 78.6507, 80.8861, 83.1138],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 100\n",
      "tensor([ 2.4374,  4.0036,  5.6613,  7.3694,  9.1443, 11.0314, 12.9713, 14.9083,\n",
      "        16.8685, 18.8509, 20.8366, 22.8251, 24.8204, 26.8068, 28.7982, 30.8054,\n",
      "        32.8278, 34.8759, 36.9482, 39.0488, 41.1691, 43.3050, 45.4491, 47.6044,\n",
      "        49.7768, 51.9631, 54.1670, 56.3739, 58.5878, 60.8007, 63.0216, 65.2439,\n",
      "        67.4724, 69.7032, 71.9400, 74.1768, 76.4141, 78.6495, 80.8848, 83.1124],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:100,loss:0.5068622827529907,time:7.448193073272705,lr:1.0000000000000002e-06\n",
      "val_loss: tensor(0.5069, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3482, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(28.8378, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 101\n",
      "tensor([ 2.4371,  4.0035,  5.6609,  7.3691,  9.1438, 11.0309, 12.9714, 14.9083,\n",
      "        16.8685, 18.8510, 20.8366, 22.8256, 24.8208, 26.8072, 28.7989, 30.8058,\n",
      "        32.8284, 34.8762, 36.9483, 39.0493, 41.1693, 43.3052, 45.4494, 47.6042,\n",
      "        49.7769, 51.9633, 54.1666, 56.3738, 58.5877, 60.8005, 63.0216, 65.2440,\n",
      "        67.4722, 69.7032, 71.9403, 74.1771, 76.4141, 78.6493, 80.8847, 83.1122],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 102\n",
      "tensor([ 2.4372,  4.0033,  5.6603,  7.3684,  9.1436, 11.0308, 12.9712, 14.9082,\n",
      "        16.8684, 18.8510, 20.8365, 22.8254, 24.8207, 26.8072, 28.7990, 30.8058,\n",
      "        32.8283, 34.8760, 36.9480, 39.0491, 41.1689, 43.3050, 45.4494, 47.6042,\n",
      "        49.7765, 51.9627, 54.1662, 56.3733, 58.5869, 60.7999, 63.0210, 65.2438,\n",
      "        67.4720, 69.7030, 71.9398, 74.1762, 76.4136, 78.6489, 80.8840, 83.1117],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 103\n",
      "tensor([ 2.4372,  4.0022,  5.6594,  7.3685,  9.1437, 11.0300, 12.9707, 14.9080,\n",
      "        16.8678, 18.8503, 20.8356, 22.8239, 24.8193, 26.8061, 28.7972, 30.8044,\n",
      "        32.8272, 34.8748, 36.9469, 39.0481, 41.1680, 43.3040, 45.4484, 47.6032,\n",
      "        49.7755, 51.9621, 54.1658, 56.3724, 58.5862, 60.7992, 63.0203, 65.2428,\n",
      "        67.4711, 69.7022, 71.9390, 74.1754, 76.4129, 78.6480, 80.8832, 83.1109],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 104\n",
      "tensor([ 2.4374,  4.0035,  5.6607,  7.3692,  9.1442, 11.0305, 12.9712, 14.9083,\n",
      "        16.8682, 18.8510, 20.8357, 22.8245, 24.8202, 26.8065, 28.7980, 30.8056,\n",
      "        32.8278, 34.8755, 36.9476, 39.0483, 41.1687, 43.3041, 45.4481, 47.6034,\n",
      "        49.7761, 51.9624, 54.1658, 56.3724, 58.5862, 60.7991, 63.0200, 65.2425,\n",
      "        67.4710, 69.7020, 71.9386, 74.1751, 76.4125, 78.6478, 80.8831, 83.1106],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 105\n",
      "tensor([ 2.4374,  4.0036,  5.6607,  7.3691,  9.1438, 11.0301, 12.9709, 14.9080,\n",
      "        16.8680, 18.8508, 20.8357, 22.8244, 24.8200, 26.8063, 28.7975, 30.8050,\n",
      "        32.8276, 34.8753, 36.9475, 39.0485, 41.1686, 43.3038, 45.4479, 47.6033,\n",
      "        49.7759, 51.9621, 54.1656, 56.3724, 58.5866, 60.7994, 63.0201, 65.2424,\n",
      "        67.4708, 69.7015, 71.9382, 74.1751, 76.4124, 78.6475, 80.8829, 83.1105],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 106\n",
      "tensor([ 2.4377,  4.0027,  5.6601,  7.3681,  9.1437, 11.0302, 12.9708, 14.9077,\n",
      "        16.8675, 18.8495, 20.8354, 22.8238, 24.8187, 26.8059, 28.7971, 30.8041,\n",
      "        32.8268, 34.8745, 36.9470, 39.0475, 41.1678, 43.3037, 45.4478, 47.6031,\n",
      "        49.7752, 51.9612, 54.1645, 56.3718, 58.5852, 60.7980, 63.0186, 65.2411,\n",
      "        67.4697, 69.7007, 71.9375, 74.1740, 76.4113, 78.6467, 80.8820, 83.1092],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 107\n",
      "tensor([ 2.4377,  4.0027,  5.6601,  7.3681,  9.1437, 11.0302, 12.9708, 14.9077,\n",
      "        16.8676, 18.8494, 20.8353, 22.8236, 24.8185, 26.8055, 28.7967, 30.8039,\n",
      "        32.8267, 34.8744, 36.9467, 39.0471, 41.1674, 43.3035, 45.4476, 47.6028,\n",
      "        49.7750, 51.9611, 54.1644, 56.3715, 58.5849, 60.7979, 63.0186, 65.2409,\n",
      "        67.4694, 69.7005, 71.9374, 74.1739, 76.4111, 78.6467, 80.8819, 83.1092],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 108\n",
      "tensor([ 2.4378,  4.0029,  5.6601,  7.3681,  9.1438, 11.0303, 12.9709, 14.9078,\n",
      "        16.8677, 18.8495, 20.8353, 22.8237, 24.8186, 26.8056, 28.7968, 30.8040,\n",
      "        32.8268, 34.8744, 36.9468, 39.0471, 41.1675, 43.3035, 45.4476, 47.6028,\n",
      "        49.7749, 51.9611, 54.1644, 56.3715, 58.5850, 60.7981, 63.0187, 65.2410,\n",
      "        67.4694, 69.7005, 71.9373, 74.1739, 76.4111, 78.6466, 80.8818, 83.1091],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 109\n",
      "tensor([ 2.4378,  4.0029,  5.6602,  7.3683,  9.1438, 11.0303, 12.9710, 14.9079,\n",
      "        16.8677, 18.8496, 20.8353, 22.8237, 24.8186, 26.8056, 28.7968, 30.8040,\n",
      "        32.8268, 34.8744, 36.9468, 39.0472, 41.1677, 43.3036, 45.4476, 47.6028,\n",
      "        49.7750, 51.9614, 54.1648, 56.3718, 58.5850, 60.7978, 63.0184, 65.2408,\n",
      "        67.4694, 69.7007, 71.9376, 74.1741, 76.4110, 78.6465, 80.8819, 83.1092],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 110\n",
      "tensor([ 2.4377,  4.0028,  5.6601,  7.3682,  9.1437, 11.0302, 12.9710, 14.9078,\n",
      "        16.8677, 18.8495, 20.8352, 22.8235, 24.8184, 26.8053, 28.7964, 30.8037,\n",
      "        32.8264, 34.8740, 36.9464, 39.0470, 41.1676, 43.3034, 45.4474, 47.6025,\n",
      "        49.7747, 51.9612, 54.1645, 56.3715, 58.5848, 60.7976, 63.0181, 65.2406,\n",
      "        67.4694, 69.7007, 71.9375, 74.1740, 76.4110, 78.6465, 80.8818, 83.1092],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:110,loss:0.5054221153259277,time:8.185897588729858,lr:1.0000000000000002e-07\n",
      "val_loss: tensor(0.5054, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3482, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(28.8534, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 111\n",
      "tensor([ 2.4376,  4.0026,  5.6600,  7.3683,  9.1439, 11.0304, 12.9711, 14.9078,\n",
      "        16.8677, 18.8497, 20.8353, 22.8236, 24.8185, 26.8053, 28.7962, 30.8035,\n",
      "        32.8263, 34.8740, 36.9466, 39.0471, 41.1675, 43.3035, 45.4473, 47.6022,\n",
      "        49.7746, 51.9610, 54.1644, 56.3714, 58.5848, 60.7977, 63.0182, 65.2406,\n",
      "        67.4696, 69.7009, 71.9376, 74.1742, 76.4111, 78.6463, 80.8818, 83.1091],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 112\n",
      "tensor([ 2.4376,  4.0025,  5.6600,  7.3684,  9.1438, 11.0301, 12.9709, 14.9076,\n",
      "        16.8678, 18.8498, 20.8353, 22.8235, 24.8185, 26.8054, 28.7963, 30.8036,\n",
      "        32.8264, 34.8739, 36.9465, 39.0471, 41.1674, 43.3035, 45.4475, 47.6024,\n",
      "        49.7746, 51.9611, 54.1643, 56.3713, 58.5850, 60.7978, 63.0182, 65.2407,\n",
      "        67.4697, 69.7010, 71.9376, 74.1742, 76.4111, 78.6463, 80.8817, 83.1090],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 113\n",
      "tensor([ 2.4376,  4.0025,  5.6600,  7.3684,  9.1439, 11.0302, 12.9710, 14.9076,\n",
      "        16.8678, 18.8500, 20.8354, 22.8237, 24.8187, 26.8054, 28.7961, 30.8034,\n",
      "        32.8263, 34.8739, 36.9466, 39.0472, 41.1674, 43.3035, 45.4476, 47.6025,\n",
      "        49.7747, 51.9611, 54.1643, 56.3713, 58.5849, 60.7977, 63.0183, 65.2408,\n",
      "        67.4696, 69.7009, 71.9374, 74.1741, 76.4110, 78.6462, 80.8815, 83.1089],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 114\n",
      "tensor([ 2.4371,  4.0024,  5.6597,  7.3682,  9.1436, 11.0301, 12.9709, 14.9075,\n",
      "        16.8677, 18.8499, 20.8353, 22.8236, 24.8186, 26.8053, 28.7961, 30.8033,\n",
      "        32.8263, 34.8739, 36.9466, 39.0471, 41.1674, 43.3035, 45.4476, 47.6025,\n",
      "        49.7746, 51.9611, 54.1642, 56.3712, 58.5849, 60.7977, 63.0183, 65.2407,\n",
      "        67.4697, 69.7009, 71.9375, 74.1742, 76.4111, 78.6462, 80.8816, 83.1090],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 115\n",
      "tensor([ 2.4371,  4.0024,  5.6597,  7.3682,  9.1436, 11.0302, 12.9709, 14.9075,\n",
      "        16.8677, 18.8499, 20.8353, 22.8236, 24.8187, 26.8054, 28.7962, 30.8034,\n",
      "        32.8263, 34.8739, 36.9466, 39.0471, 41.1673, 43.3033, 45.4474, 47.6023,\n",
      "        49.7745, 51.9610, 54.1640, 56.3710, 58.5848, 60.7976, 63.0183, 65.2408,\n",
      "        67.4695, 69.7008, 71.9374, 74.1742, 76.4110, 78.6461, 80.8815, 83.1088],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 116\n",
      "tensor([ 2.4383,  4.0033,  5.6605,  7.3690,  9.1437, 11.0304, 12.9708, 14.9077,\n",
      "        16.8679, 18.8502, 20.8354, 22.8240, 24.8194, 26.8057, 28.7974, 30.8047,\n",
      "        32.8267, 34.8746, 36.9466, 39.0474, 41.1676, 43.3033, 45.4474, 47.6028,\n",
      "        49.7748, 51.9612, 54.1650, 56.3714, 58.5857, 60.7981, 63.0189, 65.2416,\n",
      "        67.4700, 69.7008, 71.9375, 74.1740, 76.4115, 78.6467, 80.8820, 83.1095],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 117\n",
      "tensor([ 2.4383,  4.0033,  5.6604,  7.3688,  9.1434, 11.0303, 12.9708, 14.9077,\n",
      "        16.8677, 18.8502, 20.8354, 22.8239, 24.8194, 26.8057, 28.7972, 30.8046,\n",
      "        32.8268, 34.8747, 36.9468, 39.0475, 41.1675, 43.3032, 45.4474, 47.6027,\n",
      "        49.7748, 51.9611, 54.1648, 56.3713, 58.5855, 60.7980, 63.0188, 65.2415,\n",
      "        67.4700, 69.7008, 71.9376, 74.1742, 76.4114, 78.6465, 80.8818, 83.1092],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 118\n",
      "tensor([ 2.4383,  4.0033,  5.6604,  7.3688,  9.1434, 11.0303, 12.9708, 14.9077,\n",
      "        16.8677, 18.8502, 20.8354, 22.8239, 24.8194, 26.8057, 28.7972, 30.8046,\n",
      "        32.8269, 34.8748, 36.9469, 39.0476, 41.1675, 43.3032, 45.4473, 47.6025,\n",
      "        49.7746, 51.9610, 54.1647, 56.3712, 58.5854, 60.7979, 63.0187, 65.2413,\n",
      "        67.4699, 69.7007, 71.9375, 74.1742, 76.4114, 78.6464, 80.8816, 83.1091],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 119\n",
      "tensor([ 2.4383,  4.0034,  5.6604,  7.3688,  9.1435, 11.0303, 12.9708, 14.9078,\n",
      "        16.8678, 18.8503, 20.8356, 22.8240, 24.8195, 26.8057, 28.7972, 30.8047,\n",
      "        32.8270, 34.8749, 36.9471, 39.0479, 41.1677, 43.3033, 45.4472, 47.6025,\n",
      "        49.7746, 51.9611, 54.1650, 56.3715, 58.5856, 60.7981, 63.0189, 65.2415,\n",
      "        67.4698, 69.7005, 71.9373, 74.1740, 76.4111, 78.6461, 80.8814, 83.1090],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 120\n",
      "tensor([ 2.4383,  4.0033,  5.6603,  7.3686,  9.1435, 11.0303, 12.9708, 14.9077,\n",
      "        16.8677, 18.8502, 20.8354, 22.8238, 24.8193, 26.8058, 28.7975, 30.8048,\n",
      "        32.8270, 34.8748, 36.9471, 39.0480, 41.1678, 43.3033, 45.4473, 47.6025,\n",
      "        49.7747, 51.9612, 54.1651, 56.3716, 58.5857, 60.7982, 63.0189, 65.2414,\n",
      "        67.4698, 69.7006, 71.9374, 74.1740, 76.4111, 78.6461, 80.8814, 83.1091],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "epoch:120,loss:0.5056668519973755,time:8.938225269317627,lr:1.0000000000000002e-07\n",
      "val_loss: tensor(0.5057, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "re_loss: tensor(0.3482, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "d2_loss: tensor(28.8531, device='cuda:2', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "epoch: 121\n",
      "tensor([ 2.4383,  4.0034,  5.6602,  7.3686,  9.1434, 11.0302, 12.9707, 14.9076,\n",
      "        16.8676, 18.8499, 20.8352, 22.8237, 24.8193, 26.8057, 28.7974, 30.8048,\n",
      "        32.8271, 34.8749, 36.9471, 39.0480, 41.1677, 43.3033, 45.4472, 47.6025,\n",
      "        49.7749, 51.9613, 54.1652, 56.3718, 58.5859, 60.7985, 63.0193, 65.2418,\n",
      "        67.4701, 69.7009, 71.9376, 74.1740, 76.4114, 78.6464, 80.8816, 83.1093],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 122\n",
      "tensor([ 2.4383,  4.0034,  5.6602,  7.3686,  9.1434, 11.0302, 12.9706, 14.9074,\n",
      "        16.8674, 18.8499, 20.8352, 22.8239, 24.8193, 26.8057, 28.7973, 30.8049,\n",
      "        32.8272, 34.8749, 36.9471, 39.0479, 41.1677, 43.3032, 45.4471, 47.6025,\n",
      "        49.7749, 51.9613, 54.1652, 56.3717, 58.5858, 60.7983, 63.0192, 65.2418,\n",
      "        67.4701, 69.7007, 71.9374, 74.1738, 76.4112, 78.6465, 80.8817, 83.1093],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 123\n",
      "tensor([ 2.4383,  4.0032,  5.6599,  7.3680,  9.1429, 11.0299, 12.9703, 14.9072,\n",
      "        16.8674, 18.8498, 20.8350, 22.8237, 24.8191, 26.8055, 28.7973, 30.8045,\n",
      "        32.8269, 34.8748, 36.9466, 39.0476, 41.1675, 43.3028, 45.4470, 47.6023,\n",
      "        49.7746, 51.9611, 54.1648, 56.3715, 58.5857, 60.7982, 63.0191, 65.2416,\n",
      "        67.4700, 69.7008, 71.9374, 74.1739, 76.4111, 78.6464, 80.8816, 83.1092],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 124\n",
      "tensor([ 2.4383,  4.0033,  5.6600,  7.3682,  9.1430, 11.0300, 12.9704, 14.9074,\n",
      "        16.8675, 18.8500, 20.8352, 22.8238, 24.8192, 26.8057, 28.7974, 30.8046,\n",
      "        32.8270, 34.8749, 36.9468, 39.0477, 41.1676, 43.3030, 45.4471, 47.6024,\n",
      "        49.7748, 51.9612, 54.1647, 56.3713, 58.5853, 60.7977, 63.0186, 65.2412,\n",
      "        67.4696, 69.7005, 71.9374, 74.1739, 76.4111, 78.6463, 80.8814, 83.1092],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 125\n",
      "tensor([ 2.4383,  4.0032,  5.6598,  7.3680,  9.1430, 11.0300, 12.9704, 14.9073,\n",
      "        16.8676, 18.8500, 20.8352, 22.8238, 24.8192, 26.8058, 28.7976, 30.8047,\n",
      "        32.8271, 34.8750, 36.9468, 39.0478, 41.1676, 43.3031, 45.4473, 47.6026,\n",
      "        49.7749, 51.9613, 54.1648, 56.3713, 58.5853, 60.7978, 63.0188, 65.2413,\n",
      "        67.4698, 69.7006, 71.9375, 74.1740, 76.4112, 78.6464, 80.8815, 83.1092],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "\n",
      "epoch: 126\n",
      "tensor([ 2.4386,  4.0036,  5.6599,  7.3682,  9.1431, 11.0299, 12.9706, 14.9072,\n",
      "        16.8677, 18.8501, 20.8352, 22.8240, 24.8191, 26.8060, 28.7975, 30.8048,\n",
      "        32.8270, 34.8748, 36.9467, 39.0475, 41.1677, 43.3030, 45.4475, 47.6025,\n",
      "        49.7749, 51.9611, 54.1647, 56.3714, 58.5852, 60.7979, 63.0185, 65.2415,\n",
      "        67.4696, 69.7008, 71.9374, 74.1740, 76.4114, 78.6464, 80.8816, 83.1090],\n",
      "       device='cuda:2', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21., 23., 25., 27.,\n",
      "        29., 31., 33., 35., 37., 39., 41., 43., 45., 47., 49., 51., 53., 55.,\n",
      "        57., 59.], device='cuda:2')\n",
      "terminal epoch:  127\n"
     ]
    }
   ],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=20,threshold=1e-4)\n",
    "loss_list=[]\n",
    "init_time=time.time()\n",
    "for i in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    V_diag=model(grid)\n",
    "    A = torch.diag(diag) + torch.diag(off_diag,diagonal=1) + torch.diag(off_diag, diagonal=-1)+torch.diag(V_diag.flatten())\n",
    "    eigenvalues= torch.linalg.eigvalsh(A)\n",
    "    '''output=NN(input)\n",
    "    diag=torch.diag(output.flatten())'''\n",
    "    \n",
    "    print('\\nepoch:',i)\n",
    "    print(eigenvalues[:en_num+10])\n",
    "    print(real_en)\n",
    "    \n",
    "    output=eigenvalues[:en_num]\n",
    "    \n",
    "    re_loss=regular_loss(model,grid)\n",
    "    val_loss=loss_fn(output,real_en)\n",
    "    d2_loss=dd_loss(model,grid)\n",
    "    \n",
    "    loss=val_loss+re_loss*l2_reg+d2_loss*d2_reg\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_list.append(loss.item())\n",
    "    \n",
    "    os.makedirs(f'./fun_images/V_{La}_{Lb}_{N}_{en_num}', exist_ok=True)\n",
    "    torch.save(V_diag,f'./fun_images/V_{La}_{Lb}_{N}_{en_num}/V_diag_{i}.pth')\n",
    "    \n",
    "    if i%10==0:\n",
    "        # clear_output(wait=True)\n",
    "        print(f'epoch:{i},loss:{loss},time:{time.time()-init_time},lr:{optimizer.param_groups[0][\"lr\"]}')\n",
    "        print('val_loss:',val_loss)\n",
    "        print('re_loss:',re_loss)\n",
    "        print('d2_loss:',d2_loss)\n",
    "    if (i+1)%100==0:torch.save(model.state_dict(),f'./model_para_{sym}_{layer_num}_{hidden_num}_{dtype}/model_para_use_eigvalues_{en_num}_La{La}_Lb{Lb}_N{N}.pth')\n",
    "    \n",
    "    scheduler.step(loss)\n",
    "    if optimizer.param_groups[0][\"lr\"] <= 1.1e-8:break\n",
    "print('terminal epoch: ',i+1)\n",
    "torch.save(model.state_dict(),f'./model_para_{sym}_{layer_num}_{hidden_num}_{dtype}/model_para_use_eigvalues_{en_num}_La{La}_Lb{Lb}_N{N}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "La=-10\n",
    "Lb =10\n",
    "L=Lb-La  # domain length\n",
    "N = 2000   # number of interior points # 对时间成本来说几乎是平方量级\n",
    "h :float= L / (N+1)\n",
    "grid=torch.linspace(La,Lb,N+2,dtype=dtype,device=device)\n",
    "grid=grid[1:-1].unsqueeze(-1)\n",
    "V_NN=model(grid)\n",
    "\n",
    "# V_for_contrast=torch.load(f'./V_NN_30_La-10_Lb10_N2000.pth',map_location=device)\n",
    "\n",
    "V_NN=V_NN.cpu().detach().numpy()\n",
    "# V_for_contrast=V_for_contrast.cpu().detach().numpy()\n",
    "\n",
    "real_poten=potential(grid.cpu().detach().numpy(),k)\n",
    "plt.plot(grid.cpu().detach().numpy(),real_poten,label='real')\n",
    "plt.plot(grid.cpu().detach().numpy(),V_NN,label='NN')\n",
    "# plt.plot(grid.cpu().detach().numpy(),V_for_contrast,label='contrast')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(f'./V_NN_value_{sym}_{layer_num}_{hidden_num}_{dtype}', exist_ok=True)\n",
    "# torch.save(V_NN,f'./V_NN_value_{sym}_{layer_num}_{hidden_num}_{dtype}/V_NN_use_eigvalues_{en_num}_La{La}_Lb{Lb}_N{N}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_error: 5.9444942\n",
      "max_error: 15.051071\n"
     ]
    }
   ],
   "source": [
    "error=V_NN-real_poten\n",
    "mean_error=np.mean(np.abs(error))\n",
    "max_error=np.max(np.abs(error))\n",
    "print('mean_error:',mean_error)\n",
    "print('max_error:',max_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
